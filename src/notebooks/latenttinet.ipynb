{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd297a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "basedir = \"../..\"\n",
    "\n",
    "from common.config import create_object, load_config\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.disable()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# dconfig = load_config(\"../autoencoder/configs/data/burgersshift.yaml\")\n",
    "# dconfig.datasize.spacedim = 1\n",
    "# dset = create_object(dconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import itertools\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from copy import deepcopy\n",
    "\n",
    "import utils\n",
    "\n",
    "class LTINetHelper():\n",
    "  def __init__(self, config):\n",
    "    self.update_config(config)\n",
    "\n",
    "  def update_config(self, config):\n",
    "    self.config = deepcopy(config)\n",
    "\n",
    "  def create_ltinet(self, dataset, config=None, **args):\n",
    "    if config is None:\n",
    "      config = self.config\n",
    "\n",
    "    assert(len(dataset.data.shape) < 4)\n",
    "    if len(dataset.data.shape) == 3:\n",
    "      din = dataset.params.shape[-1]\n",
    "      dout = dataset.data.shape[-1]\n",
    "\n",
    "    td = args.get(\"td\", None)\n",
    "    seed = args.get(\"seed\", 0)\n",
    "    device = args.get(\"device\", 0)\n",
    "\n",
    "    recclass = globals()[args.get(\"recclass\", config.recclass)]\n",
    "    recparams = copy.deepcopy(dict(args.get(\"recparams\", config.recparams)))\n",
    "\n",
    "    recparams[\"seq\"][0] = din + 1\n",
    "    recparams[\"seq\"][-1] = dout\n",
    "\n",
    "    return LTINet(dataset, recclass, recparams, td=td, seed=seed, device=device)\n",
    "\n",
    "  @staticmethod\n",
    "  def get_operrs(ltinet, times=None, testonly=False):\n",
    "    if testonly:\n",
    "      data = ltinet.dataset.data[ltinet.numtrain:,]\n",
    "    else:\n",
    "      data = ltinet.dataset.data\n",
    "\n",
    "    errors = ltinet.get_errors(data, times=times, aggregate=False)\n",
    "\n",
    "    return errors\n",
    "  \n",
    "  @staticmethod\n",
    "  def plot_op_predicts(ltinet, testonly=False, xs=None, cmap=\"viridis\"):\n",
    "    if testonly:\n",
    "      data = ltinet.dataset.data[ltinet.numtrain:,]\n",
    "      params = ltinet.dataset.params[ltinet.numtrain:,]\n",
    "    else:\n",
    "      data = ltinet.dataset.data\n",
    "      params = ltinet.dataset.params\n",
    "\n",
    "    if xs == None:\n",
    "      xs = np.linspace(0, 1, len(data[0, 0]))\n",
    "\n",
    "    params = torch.tensor(np.float32(params)).to(ltinet.device)\n",
    "\n",
    "    predicts = ltinet.propagate(params).cpu().detach()\n",
    "\n",
    "    errors = []\n",
    "    n = predicts.shape[0]\n",
    "    for s in range(data.shape[1]):\n",
    "      currpredict = predicts[:, s-1].reshape((n, -1))\n",
    "      currreference = data[:, s].reshape((n, -1))\n",
    "      errors.append(np.mean(np.linalg.norm(currpredict - currreference, axis=1) / np.linalg.norm(currreference, axis=1)))\n",
    "        \n",
    "    print(f\"Average Relative L2 Error over all times: {np.mean(errors):.4f}\")\n",
    "\n",
    "    if len(data.shape) == 3:\n",
    "      fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "    @widgets.interact(i=(0, n-1), s=(1, ltinet.T-1))\n",
    "    def plot_interact(i=0, s=1):\n",
    "      print(f\"Avg Relative L2 Error for t0 to t{s}: {errors[s-1]:.4f}\")\n",
    "\n",
    "      if len(data.shape) == 3:\n",
    "        ax.clear()\n",
    "        ax.set_title(f\"RelL2 {np.linalg.norm(predicts[i, s-1] - data[i, s]) / np.linalg.norm(data[i, s])}\")\n",
    "        ax.plot(xs, data[i, 0], label=\"Input\", linewidth=1)\n",
    "        ax.plot(xs, predicts[i, s-1], label=\"Predicted\", linewidth=1)\n",
    "        ax.plot(xs, data[i, s], label=\"Exact\", linewidth=1)\n",
    "        ax.legend()\n",
    "        \n",
    "  @staticmethod\n",
    "  def plot_errorparams(ltinet, param=-1):\n",
    "    if param == -1:\n",
    "        # Auto-detect one varying parameter\n",
    "        param = 0\n",
    "        P = ltinet.dataset.params.shape[1]\n",
    "        for p in range(P):\n",
    "            if np.abs(ltinet.dataset.params[0, p] - ltinet.dataset.params[1, p]) > 0:\n",
    "                param = p\n",
    "                break\n",
    "\n",
    "    l2error = np.asarray(LTINetHelper.get_operrs(ltinet, times=[ltinet.T - 1]))\n",
    "    params = ltinet.dataset.params\n",
    "\n",
    "    print(params.shape, l2error.shape)\n",
    "\n",
    "    if isinstance(param, (list, tuple, np.ndarray)) and len(param) == 2:\n",
    "        # 3D scatter plot for 2 varying parameters\n",
    "        x = params[:, param[0]]\n",
    "        y = params[:, param[1]]\n",
    "        z = l2error\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        sc = ax.scatter(x, y, z, c=z, cmap='viridis', s=10)\n",
    "\n",
    "        ax.set_xlabel(f\"Param {param[0]}\")\n",
    "        ax.set_ylabel(f\"Param {param[1]}\")\n",
    "        ax.set_zlabel(\"Operator Error\")\n",
    "        fig.colorbar(sc, ax=ax, label=\"Operator Error\")\n",
    "\n",
    "    else:\n",
    "        # Fallback to 2D scatter if param is 1D\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(params[:, param], l2error, s=2)\n",
    "        ax.set_xlabel(f\"Parameter {param}\")\n",
    "        ax.set_ylabel(\"Operator Error\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "class LTINet():\n",
    "  def __init__(self, dataset, recclass, recparams, td, seed, device):\n",
    "    self.dataset = dataset\n",
    "    self.device = device\n",
    "    self.td = td\n",
    "    self.f = self.dataset.params.shape[1]\n",
    "  \n",
    "    if self.td is None:\n",
    "      self.prefix = f\"{self.dataset.name}{str(recclass.__name__)}LTINet\"\n",
    "    else:\n",
    "      self.prefix = self.td\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    self.seed = seed\n",
    "\n",
    "    datacopy = self.dataset.data.copy()\n",
    "    self.numtrain = int(datacopy.shape[0] * 0.8)\n",
    "    \n",
    "    self.T = self.dataset.data.shape[1]\n",
    "    self.trainarr = datacopy[:self.numtrain]\n",
    "    self.testarr = datacopy[self.numtrain:]\n",
    "    self.trainparams = self.dataset.params[:self.numtrain]\n",
    "    self.testparams = self.dataset.params[self.numtrain:]\n",
    "    self.optparams = None\n",
    "\n",
    "    self.datadim = len(self.dataset.data.shape) - 2\n",
    "\n",
    "    self.recclass = recclass\n",
    "    self.recparams = copy.deepcopy(recparams)\n",
    "\n",
    "    recparams[\"seq\"][0] = self.f + 1\n",
    "    recparams[\"seq\"][-1] = self.dataset.data.shape[-1]\n",
    "\n",
    "    self.recnet = recclass(**recparams).float().to(device)\n",
    "\n",
    "    self.metadata = {\n",
    "      \"recclass\": recclass.__name__,\n",
    "      \"recparams\": recparams,\n",
    "      \"dataset_name\": dataset.name,\n",
    "      \"data_shape\": list(dataset.data.shape),\n",
    "      \"data_checksum\": float(np.sum(dataset.data)),\n",
    "      \"seed\": seed,\n",
    "      \"epochs\": []\n",
    "    }\n",
    "\n",
    "  def propagate(self, code, start=0, end=-1):\n",
    "    fullts = torch.linspace(0, 1, self.T).float().to(self.device)\n",
    "    \n",
    "    if end > 0:\n",
    "      ts = fullts[start:end+1]\n",
    "    else:\n",
    "      ts = fullts[start:]\n",
    "\n",
    "    out = self.forward(code, ts)\n",
    "    return out\n",
    "\n",
    "  def get_errors(self, testarr, testparams, ords=(2,), times=None, aggregate=True):\n",
    "    assert(aggregate or len(ords) == 1)\n",
    "    \n",
    "    if isinstance(testarr, np.ndarray):\n",
    "      testarr = torch.tensor(testarr, dtype=torch.float32)\n",
    "\n",
    "    if isinstance(testparams, np.ndarray):\n",
    "      testparams = torch.tensor(testparams, dtype=torch.float32)\n",
    "\n",
    "    if times is None:\n",
    "      times = range(self.T-1)\n",
    "  \n",
    "    out = self.propagate(testparams)\n",
    "\n",
    "    n = testarr.shape[0]\n",
    "    orig = testarr.cpu().detach().numpy()\n",
    "    out = out.cpu().detach().numpy()\n",
    "\n",
    "    if aggregate:\n",
    "      orig = orig.reshape([n, -1])\n",
    "      out = out.reshape([n, -1])\n",
    "      testerrs = []\n",
    "      for o in ords:\n",
    "        testerrs.append(np.mean(np.linalg.norm(orig - out, axis=1, ord=o) / np.linalg.norm(orig, axis=1, ord=o)))\n",
    "\n",
    "      return tuple(testerrs)\n",
    "    \n",
    "    else:\n",
    "      o = ords[0]\n",
    "      testerrs = []\n",
    "\n",
    "      if len(times) == 1:\n",
    "        t = times[0]\n",
    "        origslice = orig[:, t].reshape([n, -1])\n",
    "        outslice = out[:, t].reshape([n, -1])\n",
    "        return np.linalg.norm(origslice - outslice, axis=1, ord=o) / np.linalg.norm(origslice, axis=1, ord=o)\n",
    "      else:\n",
    "        for t in range(orig.shape[1]):\n",
    "          origslice = orig[:, t].reshape([n, -1])\n",
    "          outslice = out[:, t].reshape([n, -1])\n",
    "          testerrs.append(np.mean(np.linalg.norm(origslice - outslice, axis=1, ord=o) / np.linalg.norm(origslice, axis=1, ord=o)))\n",
    "\n",
    "        return testerrs\n",
    "\n",
    "  def forward(self, z, ts):\n",
    "    z_shape = z.shape\n",
    "    *leading_dims, N = z_shape\n",
    "    T = ts.shape[0]\n",
    "\n",
    "    z_expanded = z.unsqueeze(-2).expand(*leading_dims, T, N)\n",
    "\n",
    "    t_shape = [1] * len(leading_dims) + [T, 1]\n",
    "    t_expanded = ts.view(*t_shape).expand(*leading_dims, T, 1)\n",
    "\n",
    "    result = torch.cat([z_expanded, t_expanded], dim=-1)\n",
    "\n",
    "    decoded = self.recnet(result)\n",
    "    return decoded\n",
    "\n",
    "  def load_model(self, filename_prefix, verbose=False, min_epochs=0):\n",
    "    search_path = f\"savedmodels/ltinet/{filename_prefix}*.pickle\"\n",
    "    matching_files = glob.glob(search_path)\n",
    "\n",
    "    print(\"Searching for model files matching prefix:\", filename_prefix)\n",
    "    if not hasattr(self, \"metadata\"):\n",
    "        raise ValueError(\"Missing self.metadata. Cannot match models without metadata. Ensure model has been initialized with same config.\")\n",
    "\n",
    "    for addr in matching_files:\n",
    "      try:\n",
    "          with open(addr, \"rb\") as handle:\n",
    "              dic = pickle.load(handle)\n",
    "      except Exception as e:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to read error: {e}\")\n",
    "          continue\n",
    "\n",
    "      meta = dic.get(\"metadata\", {})\n",
    "      is_match = all(\n",
    "          meta.get(k) == self.metadata.get(k)\n",
    "          for k in self.metadata.keys()\n",
    "      )\n",
    "\n",
    "      # Check if model meets the minimum epoch requirement\n",
    "      model_epochs = meta.get(\"epochs\")\n",
    "      if model_epochs is None:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to missing epoch metadata.\")\n",
    "          continue\n",
    "      elif isinstance(model_epochs, list):  # handle legacy or list format\n",
    "          if sum(model_epochs) < min_epochs:\n",
    "              if verbose:\n",
    "                  print(f\"Skipping {addr} due to insufficient epochs ({sum(model_epochs)} < {min_epochs})\")\n",
    "              continue\n",
    "      elif model_epochs < min_epochs:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to insufficient epochs ({model_epochs} < {min_epochs})\")\n",
    "          continue\n",
    "\n",
    "      if is_match:\n",
    "          print(\"Model match found. Loading from:\", addr)\n",
    "          self.recnet.load_state_dict(dic[\"recnet\"])\n",
    "          self.metadata[\"epochs\"] = meta.get(\"epochs\")\n",
    "          if \"opt\" in dic:     \n",
    "            self.optparams = dic[\"opt\"]\n",
    "\n",
    "          return True\n",
    "      elif verbose:\n",
    "          print(\"Metadata mismatch in file:\", addr)\n",
    "          for k in self.metadata:\n",
    "              print(f\"{k}: saved={meta.get(k)} vs current={self.metadata.get(k)}\")\n",
    "\n",
    "    print(\"Load failed. No matching models found.\")\n",
    "    print(\"Searched:\", matching_files)\n",
    "    return False\n",
    "\n",
    "  def train_model(self, epochs, save=True, optim=torch.optim.AdamW, lr=1e-4, printinterval=10, batch=32, ridge=0, loss=None, best=True, verbose=False):\n",
    "    def train_epoch(dataloader, writer=None, optimizer=None, scheduler=None, ep=0, printinterval=10, loss=None, testarr=None, testparams=None):\n",
    "      losses = []\n",
    "      testerrors1 = []\n",
    "      testerrors2 = []\n",
    "      testerrorsinf = []\n",
    "\n",
    "      def closure(values, params):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = self.propagate(params)\n",
    "        target = values\n",
    "        \n",
    "        res = loss(out, target)\n",
    "        res.backward()\n",
    "        \n",
    "        if writer is not None and self.trainstep % 5 == 0:\n",
    "          writer.add_scalar(\"main/loss\", res, global_step=self.trainstep)\n",
    "\n",
    "        return res\n",
    "\n",
    "      for values, params in dataloader:\n",
    "        self.trainstep += 1\n",
    "        error = optimizer.step(lambda: closure(values, params))\n",
    "        losses.append(float(error.cpu().detach()))\n",
    "\n",
    "      if scheduler is not None:\n",
    "        scheduler.step(np.mean(losses))\n",
    "\n",
    "      # print test\n",
    "      if printinterval > 0 and (ep % printinterval == 0):\n",
    "        testerr1, testerr2, testerrinf = self.get_errors(testarr, testparams, ords=(1, 2, np.inf))\n",
    "        if scheduler is not None:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, LR {scheduler.get_last_lr()[-1]:.3e}, Relative LTINet Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "        else:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, Relative LTINet Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"misc/relativeL1error\", testerr1, global_step=ep)\n",
    "            writer.add_scalar(\"main/relativeL2error\", testerr2, global_step=ep)\n",
    "            writer.add_scalar(\"misc/relativeLInferror\", testerrinf, global_step=ep)\n",
    "\n",
    "      return losses, testerrors1, testerrors2, testerrorsinf\n",
    "  \n",
    "    loss = nn.MSELoss() if loss is None else loss()\n",
    "\n",
    "    losses, testerrors1, testerrors2, testerrorsinf = [], [], [], []\n",
    "    self.trainstep = 0\n",
    "\n",
    "    train = torch.tensor(self.trainarr, dtype=torch.float32).to(self.device)\n",
    "    params = torch.tensor(self.trainparams, dtype=torch.float32).to(self.device)\n",
    "    test = self.testarr\n",
    "\n",
    "    opt = optim(self.recnet.parameters(), lr=lr, weight_decay=ridge)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(opt, patience=30)\n",
    "    dataloader = DataLoader(torch.utils.data.TensorDataset(train, params), shuffle=False, batch_size=batch)\n",
    "\n",
    "    if self.optparams is not None:\n",
    "      opt.load_state_dict(self.optparams)\n",
    "\n",
    "    writer = None\n",
    "    if self.td is not None:\n",
    "      name = f\"./tensorboard/{datetime.datetime.now().strftime('%d-%B-%Y')}/{self.td}/{datetime.datetime.now().strftime('%H.%M.%S')}/\"\n",
    "      writer = torch.utils.tensorboard.SummaryWriter(name)\n",
    "      print(\"Tensorboard writer location is \" + name)\n",
    "\n",
    "    print(\"Number of NN trainable parameters\", utils.num_params(self.recnet))\n",
    "    print(f\"Starting training LTINet model at {time.asctime()}...\")\n",
    "    print(\"train\", train.shape, \"test\", test.shape)\n",
    "      \n",
    "    bestdict = { \"loss\": float(np.inf), \"ep\": 0 }\n",
    "    for ep in range(epochs):\n",
    "      lossesN, testerrors1N, testerrors2N, testerrorsinfN = train_epoch(dataloader, optimizer=opt, scheduler=scheduler, writer=writer, ep=ep, printinterval=printinterval, loss=loss, testarr=test, testparams=self.testparams)\n",
    "      losses += lossesN; testerrors1 += testerrors1N; testerrors2 += testerrors2N; testerrorsinf += testerrorsinfN\n",
    "\n",
    "      if best and ep > epochs // 2:\n",
    "        avgloss = np.mean(lossesN)\n",
    "        if avgloss < bestdict[\"loss\"]:\n",
    "          bestdict[\"recnet\"] = self.recnet.state_dict()\n",
    "          bestdict[\"opt\"] = opt.state_dict()\n",
    "          bestdict[\"loss\"] = avgloss\n",
    "          bestdict[\"ep\"] = ep\n",
    "        elif verbose:\n",
    "          print(f\"Loss not improved at epoch {ep} (Ratio: {avgloss/bestdict['loss']:.2f}) from {bestdict['ep']} (Loss: {bestdict['loss']:.2e})\")\n",
    "      \n",
    "    print(f\"Finished training LTINet model at {time.asctime()}...\")\n",
    "\n",
    "    if best:\n",
    "      self.recnet.load_state_dict(bestdict[\"recnet\"])\n",
    "      opt.load_state_dict(bestdict[\"opt\"])\n",
    "\n",
    "    self.optparams = opt.state_dict()\n",
    "    self.metadata[\"epochs\"].append(epochs)\n",
    "\n",
    "    if save:\n",
    "      now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "      # Compute total training epochs\n",
    "      total_epochs = sum(self.metadata[\"epochs\"]) if isinstance(self.metadata[\"epochs\"], list) else self.metadata[\"epochs\"]\n",
    "\n",
    "      filename = (\n",
    "          f\"{self.dataset.name}_\"\n",
    "          f\"{self.recclass.__name__}_\"\n",
    "          f\"{self.recparams['seq']}_\"\n",
    "          f\"{self.seed}_\"\n",
    "          f\"{total_epochs}ep_\"\n",
    "          f\"{now}.pickle\"\n",
    "      )\n",
    "\n",
    "      dire = \"savedmodels/ltinet\"\n",
    "      addr = os.path.join(dire, filename)\n",
    "\n",
    "      if not os.path.exists(dire):\n",
    "          os.makedirs(dire)\n",
    "\n",
    "      with open(addr, \"wb\") as handle:\n",
    "          pickle.dump({\n",
    "              \"recnet\": self.recnet.state_dict(),\n",
    "              \"metadata\": self.metadata,\n",
    "              \"opt\": self.optparams\n",
    "          }, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "      print(\"Model saved at\", addr)\n",
    "\n",
    "    return { \"losses\": losses, \"testerrors1\": testerrors1, \"testerrors2\": testerrors2, \"testerrorsinf\": testerrorsinf }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd070be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dconfig = load_config(\"../autoencoder/configs/data/burgersshift.yaml\")\n",
    "dconfig.datasize.spacedim = 1\n",
    "dset = create_object(dconfig)\n",
    "\n",
    "#dset.downsample_time(10)\n",
    "dset.downsample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3860e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NN trainable parameters 1581700\n",
      "Starting training TI model DeepONet at Fri Jul  4 22:40:59 2025...\n",
      "train torch.Size([400, 301, 128]) test (100, 301, 128)\n",
      "1: Train Loss 1.228e-01, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.871023, 0.885350, 0.974652\n",
      "11: Train Loss 2.543e-02, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.348901, 0.429166, 0.802944\n",
      "21: Train Loss 1.710e-02, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.257463, 0.338546, 0.743950\n",
      "31: Train Loss 1.596e-02, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.252372, 0.333476, 0.744745\n",
      "41: Train Loss 1.260e-02, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.215464, 0.305711, 0.755190\n",
      "51: Train Loss 1.176e-02, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.199164, 0.276510, 0.707517\n",
      "61: Train Loss 9.608e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.195823, 0.268300, 0.687340\n",
      "71: Train Loss 9.289e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.199168, 0.268056, 0.718723\n",
      "81: Train Loss 9.818e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.181441, 0.255563, 0.705440\n",
      "91: Train Loss 7.224e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.164721, 0.235115, 0.675810\n",
      "101: Train Loss 7.684e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.150756, 0.222192, 0.640988\n",
      "111: Train Loss 6.946e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.159268, 0.225822, 0.647256\n",
      "121: Train Loss 5.762e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.157205, 0.220309, 0.639458\n",
      "131: Train Loss 7.103e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.153283, 0.231690, 0.701378\n",
      "141: Train Loss 7.398e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.159988, 0.226838, 0.675443\n",
      "151: Train Loss 5.980e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.130287, 0.199304, 0.628194\n",
      "161: Train Loss 6.139e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.150347, 0.212507, 0.663203\n",
      "171: Train Loss 5.123e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.123622, 0.185668, 0.584739\n",
      "181: Train Loss 5.807e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.129326, 0.193522, 0.612215\n",
      "191: Train Loss 5.401e-03, LR 1.000e-03, Relative TI Error (1, 2, inf): 0.130407, 0.195776, 0.628226\n",
      "201: Train Loss 3.788e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.095191, 0.164795, 0.557487\n",
      "211: Train Loss 3.654e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.094403, 0.162275, 0.543615\n",
      "221: Train Loss 3.611e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.094326, 0.161469, 0.541727\n",
      "231: Train Loss 3.581e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.094121, 0.160827, 0.540673\n",
      "241: Train Loss 3.557e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.093927, 0.160275, 0.539808\n",
      "251: Train Loss 3.534e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.093796, 0.159756, 0.539050\n",
      "261: Train Loss 3.513e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.093706, 0.159242, 0.538130\n",
      "271: Train Loss 3.492e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.093576, 0.158706, 0.536845\n",
      "281: Train Loss 3.472e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.093406, 0.158148, 0.535349\n",
      "291: Train Loss 3.449e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.093247, 0.157541, 0.533515\n",
      "301: Train Loss 3.423e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.093076, 0.156882, 0.531294\n",
      "311: Train Loss 3.395e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.092877, 0.156180, 0.528843\n",
      "321: Train Loss 3.367e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.092637, 0.155420, 0.526702\n",
      "331: Train Loss 3.338e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.092307, 0.154596, 0.524933\n",
      "341: Train Loss 3.309e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.091798, 0.153685, 0.523578\n",
      "351: Train Loss 3.278e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.091047, 0.152680, 0.522732\n",
      "361: Train Loss 3.239e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.089995, 0.151565, 0.522394\n",
      "371: Train Loss 3.192e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.088657, 0.150358, 0.521991\n",
      "381: Train Loss 3.138e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.087093, 0.149107, 0.521541\n",
      "391: Train Loss 3.081e-03, LR 1.000e-04, Relative TI Error (1, 2, inf): 0.085412, 0.147836, 0.521340\n",
      "Finished training TI model DeepONet at Fri Jul  4 22:45:07 2025...\n",
      "Model saved at savedmodels/timeinput\\bshift_DeepONet_ReLU()_[128, 600, 600, 600, 50]x[2, 600, 600, 600, 50]_0_400ep_2025-07-04_22-45-07.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'losses': [0.15397650003433228,\n",
       "  0.16766208410263062,\n",
       "  0.15065503120422363,\n",
       "  0.14493978023529053,\n",
       "  0.13401150703430176,\n",
       "  0.13720950484275818,\n",
       "  0.15102946758270264,\n",
       "  0.11988787353038788,\n",
       "  0.13123498857021332,\n",
       "  0.13423579931259155,\n",
       "  0.13364563882350922,\n",
       "  0.12886355817317963,\n",
       "  0.12280315905809402,\n",
       "  0.1250154972076416,\n",
       "  0.12196734547615051,\n",
       "  0.11190628260374069,\n",
       "  0.11065759509801865,\n",
       "  0.10506986081600189,\n",
       "  0.09584058076143265,\n",
       "  0.0888606384396553,\n",
       "  0.08075150102376938,\n",
       "  0.08250419795513153,\n",
       "  0.08053058385848999,\n",
       "  0.0747310221195221,\n",
       "  0.07164130359888077,\n",
       "  0.0783054530620575,\n",
       "  0.07394121587276459,\n",
       "  0.07109012454748154,\n",
       "  0.06953132152557373,\n",
       "  0.07250446826219559,\n",
       "  0.07464279979467392,\n",
       "  0.06476087123155594,\n",
       "  0.06525207310914993,\n",
       "  0.061666227877140045,\n",
       "  0.06442311406135559,\n",
       "  0.06318215280771255,\n",
       "  0.06095825880765915,\n",
       "  0.05885211378335953,\n",
       "  0.06190148741006851,\n",
       "  0.058058105409145355,\n",
       "  0.05872269347310066,\n",
       "  0.056085728108882904,\n",
       "  0.05460492521524429,\n",
       "  0.05307523161172867,\n",
       "  0.055178191512823105,\n",
       "  0.05564038082957268,\n",
       "  0.055233605206012726,\n",
       "  0.0547105148434639,\n",
       "  0.05116948485374451,\n",
       "  0.05335681512951851,\n",
       "  0.04987850785255432,\n",
       "  0.05293476954102516,\n",
       "  0.04921351745724678,\n",
       "  0.05070117861032486,\n",
       "  0.049165599048137665,\n",
       "  0.04782578721642494,\n",
       "  0.04688383266329765,\n",
       "  0.045204680413007736,\n",
       "  0.04838288202881813,\n",
       "  0.0452253483235836,\n",
       "  0.047912005335092545,\n",
       "  0.045821208506822586,\n",
       "  0.04933140054345131,\n",
       "  0.044279128313064575,\n",
       "  0.04562509059906006,\n",
       "  0.04421350732445717,\n",
       "  0.04672597348690033,\n",
       "  0.04491335526108742,\n",
       "  0.04428693652153015,\n",
       "  0.042393144220113754,\n",
       "  0.04039464890956879,\n",
       "  0.04208798334002495,\n",
       "  0.0391073077917099,\n",
       "  0.0417519249022007,\n",
       "  0.03808749467134476,\n",
       "  0.04016963392496109,\n",
       "  0.03651133552193642,\n",
       "  0.034494563937187195,\n",
       "  0.03507368266582489,\n",
       "  0.03284178674221039,\n",
       "  0.029276913031935692,\n",
       "  0.03234498202800751,\n",
       "  0.03144794702529907,\n",
       "  0.032404396682977676,\n",
       "  0.03787212818861008,\n",
       "  0.0355239063501358,\n",
       "  0.04106033593416214,\n",
       "  0.04006374999880791,\n",
       "  0.0417657233774662,\n",
       "  0.03303145989775658,\n",
       "  0.039830513298511505,\n",
       "  0.0381067618727684,\n",
       "  0.03883164003491402,\n",
       "  0.03373396024107933,\n",
       "  0.0369771271944046,\n",
       "  0.03613823652267456,\n",
       "  0.0303801316767931,\n",
       "  0.033115826547145844,\n",
       "  0.03175710141658783,\n",
       "  0.034028828144073486,\n",
       "  0.03318153694272041,\n",
       "  0.03196670860052109,\n",
       "  0.03410748764872551,\n",
       "  0.0299052856862545,\n",
       "  0.03362440690398216,\n",
       "  0.031050104647874832,\n",
       "  0.028935328125953674,\n",
       "  0.029701774939894676,\n",
       "  0.02795756794512272,\n",
       "  0.028504671528935432,\n",
       "  0.0302375927567482,\n",
       "  0.02786005102097988,\n",
       "  0.03102133236825466,\n",
       "  0.028334230184555054,\n",
       "  0.029721852391958237,\n",
       "  0.027784230187535286,\n",
       "  0.026800967752933502,\n",
       "  0.029498489573597908,\n",
       "  0.026656975969672203,\n",
       "  0.027262218296527863,\n",
       "  0.025945840403437614,\n",
       "  0.026624688878655434,\n",
       "  0.02642326056957245,\n",
       "  0.026948150247335434,\n",
       "  0.027450181543827057,\n",
       "  0.026758108288049698,\n",
       "  0.028058847412467003,\n",
       "  0.025760086253285408,\n",
       "  0.026570724323391914,\n",
       "  0.026263298466801643,\n",
       "  0.028009403496980667,\n",
       "  0.025828586891293526,\n",
       "  0.02618684060871601,\n",
       "  0.025061186403036118,\n",
       "  0.026273002848029137,\n",
       "  0.026172108948230743,\n",
       "  0.027605878189206123,\n",
       "  0.02766173519194126,\n",
       "  0.027191651985049248,\n",
       "  0.02804371528327465,\n",
       "  0.027202129364013672,\n",
       "  0.027877414599061012,\n",
       "  0.025429466739296913,\n",
       "  0.028675531968474388,\n",
       "  0.02617158740758896,\n",
       "  0.02535233274102211,\n",
       "  0.025549694895744324,\n",
       "  0.02482932060956955,\n",
       "  0.026982273906469345,\n",
       "  0.025918155908584595,\n",
       "  0.02636909671127796,\n",
       "  0.026553543284535408,\n",
       "  0.02660674788057804,\n",
       "  0.02429860830307007,\n",
       "  0.024568472057580948,\n",
       "  0.02576063759624958,\n",
       "  0.02584834210574627,\n",
       "  0.026508549228310585,\n",
       "  0.023326536640524864,\n",
       "  0.024837208911776543,\n",
       "  0.023448621854186058,\n",
       "  0.02410385012626648,\n",
       "  0.026115436106920242,\n",
       "  0.026093078777194023,\n",
       "  0.024450702592730522,\n",
       "  0.02534923516213894,\n",
       "  0.023995041847229004,\n",
       "  0.024998245760798454,\n",
       "  0.02378399297595024,\n",
       "  0.02642625942826271,\n",
       "  0.02445327118039131,\n",
       "  0.022797001525759697,\n",
       "  0.02336874045431614,\n",
       "  0.023827217519283295,\n",
       "  0.02299128659069538,\n",
       "  0.024884864687919617,\n",
       "  0.024942275136709213,\n",
       "  0.024668388068675995,\n",
       "  0.02358167991042137,\n",
       "  0.023153888061642647,\n",
       "  0.024379195645451546,\n",
       "  0.023534607142210007,\n",
       "  0.023530010133981705,\n",
       "  0.024573642760515213,\n",
       "  0.022347867488861084,\n",
       "  0.02204832434654236,\n",
       "  0.022888096049427986,\n",
       "  0.02271082252264023,\n",
       "  0.02372976392507553,\n",
       "  0.023614870384335518,\n",
       "  0.02332276478409767,\n",
       "  0.02221020497381687,\n",
       "  0.02216407284140587,\n",
       "  0.023748109117150307,\n",
       "  0.02190415747463703,\n",
       "  0.023601045832037926,\n",
       "  0.023418838158249855,\n",
       "  0.021246135234832764,\n",
       "  0.0210507120937109,\n",
       "  0.021847687661647797,\n",
       "  0.02203483320772648,\n",
       "  0.022341206669807434,\n",
       "  0.02266678772866726,\n",
       "  0.023075567558407784,\n",
       "  0.02157491073012352,\n",
       "  0.02140989899635315,\n",
       "  0.022459734231233597,\n",
       "  0.021025091409683228,\n",
       "  0.022237444296479225,\n",
       "  0.021679729223251343,\n",
       "  0.020489903166890144,\n",
       "  0.020311497151851654,\n",
       "  0.020914653316140175,\n",
       "  0.020995575934648514,\n",
       "  0.021436398848891258,\n",
       "  0.02180425636470318,\n",
       "  0.02194627746939659,\n",
       "  0.020813582465052605,\n",
       "  0.020347831770777702,\n",
       "  0.022026723250746727,\n",
       "  0.020466187968850136,\n",
       "  0.02229130268096924,\n",
       "  0.020950566977262497,\n",
       "  0.019783375784754753,\n",
       "  0.01962685026228428,\n",
       "  0.02015911601483822,\n",
       "  0.019664037972688675,\n",
       "  0.020561134442687035,\n",
       "  0.02059067226946354,\n",
       "  0.020961018279194832,\n",
       "  0.019727133214473724,\n",
       "  0.019763337448239326,\n",
       "  0.02012890949845314,\n",
       "  0.01950833387672901,\n",
       "  0.02043265849351883,\n",
       "  0.01955365389585495,\n",
       "  0.01864430494606495,\n",
       "  0.018741222098469734,\n",
       "  0.01916421577334404,\n",
       "  0.018585680052638054,\n",
       "  0.018983295187354088,\n",
       "  0.019255930557847023,\n",
       "  0.01943165622651577,\n",
       "  0.018461262807250023,\n",
       "  0.018028615042567253,\n",
       "  0.018796775490045547,\n",
       "  0.01767847128212452,\n",
       "  0.020176121965050697,\n",
       "  0.018175281584262848,\n",
       "  0.01747753471136093,\n",
       "  0.017685074359178543,\n",
       "  0.018062658607959747,\n",
       "  0.017254699021577835,\n",
       "  0.01791195757687092,\n",
       "  0.01777425967156887,\n",
       "  0.018303457647562027,\n",
       "  0.0173035841435194,\n",
       "  0.017243534326553345,\n",
       "  0.017081350088119507,\n",
       "  0.016862237825989723,\n",
       "  0.018278639763593674,\n",
       "  0.017103999853134155,\n",
       "  0.016748296096920967,\n",
       "  0.016355328261852264,\n",
       "  0.016739146783947945,\n",
       "  0.01747581921517849,\n",
       "  0.01665409281849861,\n",
       "  0.01809776946902275,\n",
       "  0.01725667528808117,\n",
       "  0.018090108409523964,\n",
       "  0.016852732747793198,\n",
       "  0.017041433602571487,\n",
       "  0.017098087817430496,\n",
       "  0.017944978550076485,\n",
       "  0.018382206559181213,\n",
       "  0.016191313043236732,\n",
       "  0.016895832493901253,\n",
       "  0.01677563413977623,\n",
       "  0.016947928816080093,\n",
       "  0.016432449221611023,\n",
       "  0.01710088737308979,\n",
       "  0.017248518764972687,\n",
       "  0.017341554164886475,\n",
       "  0.01621747948229313,\n",
       "  0.019185706973075867,\n",
       "  0.016306782141327858,\n",
       "  0.019016597419977188,\n",
       "  0.017423784360289574,\n",
       "  0.016465432941913605,\n",
       "  0.016277369111776352,\n",
       "  0.016681700944900513,\n",
       "  0.016584238037467003,\n",
       "  0.016936417669057846,\n",
       "  0.017328396439552307,\n",
       "  0.0169688630849123,\n",
       "  0.01654824987053871,\n",
       "  0.01606786623597145,\n",
       "  0.016475258395075798,\n",
       "  0.01583842933177948,\n",
       "  0.01698428764939308,\n",
       "  0.017481517046689987,\n",
       "  0.016229785978794098,\n",
       "  0.01575244776904583,\n",
       "  0.017161158844828606,\n",
       "  0.016114383935928345,\n",
       "  0.016593778505921364,\n",
       "  0.016523810103535652,\n",
       "  0.01688867062330246,\n",
       "  0.016464801505208015,\n",
       "  0.016154786571860313,\n",
       "  0.016255605965852737,\n",
       "  0.015778550878167152,\n",
       "  0.017881879583001137,\n",
       "  0.01575918309390545,\n",
       "  0.016657007858157158,\n",
       "  0.015260430984199047,\n",
       "  0.016962014138698578,\n",
       "  0.016894757747650146,\n",
       "  0.017777498811483383,\n",
       "  0.016795966774225235,\n",
       "  0.017717774957418442,\n",
       "  0.01728561706840992,\n",
       "  0.017689215019345284,\n",
       "  0.017043283209204674,\n",
       "  0.017685893923044205,\n",
       "  0.017692672088742256,\n",
       "  0.01828954555094242,\n",
       "  0.016213707625865936,\n",
       "  0.016202282160520554,\n",
       "  0.015916546806693077,\n",
       "  0.017929313704371452,\n",
       "  0.017374731600284576,\n",
       "  0.015902938321232796,\n",
       "  0.017265688627958298,\n",
       "  0.01803777739405632,\n",
       "  0.016729852184653282,\n",
       "  0.01622427999973297,\n",
       "  0.016569221392273903,\n",
       "  0.017851857468485832,\n",
       "  0.01693948730826378,\n",
       "  0.016232643276453018,\n",
       "  0.015202960930764675,\n",
       "  0.01671808958053589,\n",
       "  0.017279336228966713,\n",
       "  0.016471462324261665,\n",
       "  0.01664717309176922,\n",
       "  0.018059123307466507,\n",
       "  0.016907326877117157,\n",
       "  0.019567880779504776,\n",
       "  0.019199155271053314,\n",
       "  0.01687343418598175,\n",
       "  0.02229848876595497,\n",
       "  0.018694642931222916,\n",
       "  0.018204012885689735,\n",
       "  0.01831570640206337,\n",
       "  0.01730160042643547,\n",
       "  0.021168095991015434,\n",
       "  0.017441987991333008,\n",
       "  0.019713059067726135,\n",
       "  0.01712632179260254,\n",
       "  0.020543254911899567,\n",
       "  0.016807373613119125,\n",
       "  0.019103508442640305,\n",
       "  0.017486413940787315,\n",
       "  0.02027452178299427,\n",
       "  0.018267877399921417,\n",
       "  0.016970660537481308,\n",
       "  0.017085490748286247,\n",
       "  0.016512038186192513,\n",
       "  0.019710009917616844,\n",
       "  0.017449544742703438,\n",
       "  0.017057394608855247,\n",
       "  0.018254531547427177,\n",
       "  0.017603306099772453,\n",
       "  0.016918843612074852,\n",
       "  0.01667567901313305,\n",
       "  0.01652214489877224,\n",
       "  0.019018135964870453,\n",
       "  0.016832169145345688,\n",
       "  0.01667783223092556,\n",
       "  0.016349926590919495,\n",
       "  0.019005516543984413,\n",
       "  0.016044484451413155,\n",
       "  0.017350560054183006,\n",
       "  0.017977062612771988,\n",
       "  0.017454376444220543,\n",
       "  0.017007675021886826,\n",
       "  0.015795225277543068,\n",
       "  0.0167187862098217,\n",
       "  0.01599314622581005,\n",
       "  0.01893560215830803,\n",
       "  0.015174481086432934,\n",
       "  0.015446607954800129,\n",
       "  0.015057160519063473,\n",
       "  0.016036324203014374,\n",
       "  0.01582479290664196,\n",
       "  0.015375887975096703,\n",
       "  0.016146818175911903,\n",
       "  0.01615154556930065,\n",
       "  0.01620406284928322,\n",
       "  0.014786913059651852,\n",
       "  0.015190022997558117,\n",
       "  0.01596098579466343,\n",
       "  0.01699669100344181,\n",
       "  0.016492905095219612,\n",
       "  0.01425100676715374,\n",
       "  0.014505765400826931,\n",
       "  0.015675123780965805,\n",
       "  0.01644640415906906,\n",
       "  0.015186120755970478,\n",
       "  0.015532223507761955,\n",
       "  0.017794786021113396,\n",
       "  0.015561613254249096,\n",
       "  0.015146713703870773,\n",
       "  0.015921281650662422,\n",
       "  0.016044624149799347,\n",
       "  0.015561589039862156,\n",
       "  0.015540124848484993,\n",
       "  0.014197740703821182,\n",
       "  0.013337747193872929,\n",
       "  0.014550816267728806,\n",
       "  0.01419562567025423,\n",
       "  0.014449350535869598,\n",
       "  0.014175805263221264,\n",
       "  0.015124586410820484,\n",
       "  0.014342454262077808,\n",
       "  0.015248113311827183,\n",
       "  0.014184647239744663,\n",
       "  0.013636350631713867,\n",
       "  0.014851542189717293,\n",
       "  0.014237513765692711,\n",
       "  0.013262085616588593,\n",
       "  0.01262686587870121,\n",
       "  0.01375201903283596,\n",
       "  0.014172092080116272,\n",
       "  0.013658695854246616,\n",
       "  0.013134944252669811,\n",
       "  0.014490965753793716,\n",
       "  0.013184875249862671,\n",
       "  0.012620540335774422,\n",
       "  0.013044271618127823,\n",
       "  0.01281808316707611,\n",
       "  0.01365527231246233,\n",
       "  0.012390892952680588,\n",
       "  0.012299153953790665,\n",
       "  0.011890847235918045,\n",
       "  0.012476492673158646,\n",
       "  0.012863622047007084,\n",
       "  0.012372379191219807,\n",
       "  0.012561262585222721,\n",
       "  0.0132902255281806,\n",
       "  0.012999280355870724,\n",
       "  0.012101254425942898,\n",
       "  0.012387110851705074,\n",
       "  0.012367188930511475,\n",
       "  0.01387022715061903,\n",
       "  0.012046035379171371,\n",
       "  0.011822052299976349,\n",
       "  0.011857214383780956,\n",
       "  0.01222888845950365,\n",
       "  0.012141112238168716,\n",
       "  0.012535116635262966,\n",
       "  0.012239526025950909,\n",
       "  0.013318114914000034,\n",
       "  0.012778365053236485,\n",
       "  0.012302408926188946,\n",
       "  0.012357020750641823,\n",
       "  0.012176241725683212,\n",
       "  0.013563248328864574,\n",
       "  0.011991364881396294,\n",
       "  0.011918815784156322,\n",
       "  0.011406329460442066,\n",
       "  0.012081020511686802,\n",
       "  0.01207432895898819,\n",
       "  0.012254156172275543,\n",
       "  0.011937836185097694,\n",
       "  0.013232957571744919,\n",
       "  0.012644657865166664,\n",
       "  0.011764345690608025,\n",
       "  0.01239081285893917,\n",
       "  0.011922073550522327,\n",
       "  0.013444272801280022,\n",
       "  0.011970588006079197,\n",
       "  0.011771534569561481,\n",
       "  0.011772850528359413,\n",
       "  0.011911056935787201,\n",
       "  0.01183642540127039,\n",
       "  0.012142733670771122,\n",
       "  0.011970318853855133,\n",
       "  0.012827624566853046,\n",
       "  0.013291659764945507,\n",
       "  0.011566362343728542,\n",
       "  0.01244147215038538,\n",
       "  0.012034395709633827,\n",
       "  0.013891779817640781,\n",
       "  0.012241972610354424,\n",
       "  0.011351530440151691,\n",
       "  0.01185722928494215,\n",
       "  0.012240870855748653,\n",
       "  0.011396857909858227,\n",
       "  0.012188166379928589,\n",
       "  0.01207758765667677,\n",
       "  0.013168069534003735,\n",
       "  0.01201628614217043,\n",
       "  0.011778074316680431,\n",
       "  0.012385297566652298,\n",
       "  0.011612266302108765,\n",
       "  0.012928176671266556,\n",
       "  0.012990206480026245,\n",
       "  0.011335317976772785,\n",
       "  0.011846665292978287,\n",
       "  0.012194503098726273,\n",
       "  0.011534099467098713,\n",
       "  0.011478088796138763,\n",
       "  0.011836419813334942,\n",
       "  0.01222373265773058,\n",
       "  0.011911722831428051,\n",
       "  0.011208958923816681,\n",
       "  0.012311144731938839,\n",
       "  0.011579381301999092,\n",
       "  0.013494790531694889,\n",
       "  0.012826177291572094,\n",
       "  0.01160811260342598,\n",
       "  0.011774323880672455,\n",
       "  0.011957588605582714,\n",
       "  0.012004266493022442,\n",
       "  0.011769572272896767,\n",
       "  0.011761131696403027,\n",
       "  0.012563546188175678,\n",
       "  0.012009025551378727,\n",
       "  0.012994666583836079,\n",
       "  0.01223678421229124,\n",
       "  0.012595901265740395,\n",
       "  0.015165415592491627,\n",
       "  0.013947485946118832,\n",
       "  0.011882754974067211,\n",
       "  0.012144675478339195,\n",
       "  0.01257142424583435,\n",
       "  0.013482529670000076,\n",
       "  0.011862226761877537,\n",
       "  0.012624348513782024,\n",
       "  0.013729353435337543,\n",
       "  0.014488539658486843,\n",
       "  0.012847726233303547,\n",
       "  0.012926432304084301,\n",
       "  0.013958673924207687,\n",
       "  0.014537587761878967,\n",
       "  0.013561190105974674,\n",
       "  0.013077872805297375,\n",
       "  0.012142358347773552,\n",
       "  0.013334482908248901,\n",
       "  0.013485575094819069,\n",
       "  0.015287913382053375,\n",
       "  0.013577161356806755,\n",
       "  0.014684529975056648,\n",
       "  0.01473325490951538,\n",
       "  0.015401097945868969,\n",
       "  0.01408841460943222,\n",
       "  0.01368734985589981,\n",
       "  0.015053879469633102,\n",
       "  0.014933997765183449,\n",
       "  0.012587131932377815,\n",
       "  0.013141900300979614,\n",
       "  0.012720839120447636,\n",
       "  0.013982035219669342,\n",
       "  0.013338079676032066,\n",
       "  0.013820130378007889,\n",
       "  0.01551358588039875,\n",
       "  0.01294643059372902,\n",
       "  0.013162348419427872,\n",
       "  0.013962650671601295,\n",
       "  0.013930381275713444,\n",
       "  0.014285916462540627,\n",
       "  0.014529875479638577,\n",
       "  0.013055377639830112,\n",
       "  0.011057733558118343,\n",
       "  0.013663123361766338,\n",
       "  0.014357166364789009,\n",
       "  0.011940713040530682,\n",
       "  0.012602424249053001,\n",
       "  0.013715199194848537,\n",
       "  0.013260463252663612,\n",
       "  0.012735382653772831,\n",
       "  0.012135055847465992,\n",
       "  0.012822789140045643,\n",
       "  0.013577449135482311,\n",
       "  0.01409420371055603,\n",
       "  0.01210056059062481,\n",
       "  0.01310267485678196,\n",
       "  0.012431679293513298,\n",
       "  0.013390175998210907,\n",
       "  0.01222805492579937,\n",
       "  0.01319315005093813,\n",
       "  0.013372491113841534,\n",
       "  0.013718975707888603,\n",
       "  0.012505988590419292,\n",
       "  0.012345249764621258,\n",
       "  0.012921014800667763,\n",
       "  0.013687334023416042,\n",
       "  0.013472959399223328,\n",
       "  0.012033277191221714,\n",
       "  0.011444952338933945,\n",
       "  0.012508442625403404,\n",
       "  0.012682003900408745,\n",
       "  0.012135799042880535,\n",
       "  0.011728144250810146,\n",
       "  0.012888994067907333,\n",
       "  0.012186412699520588,\n",
       "  0.012961739674210548,\n",
       "  0.011924788355827332,\n",
       "  0.011370953172445297,\n",
       "  0.013036368414759636,\n",
       "  0.01235504075884819,\n",
       "  0.011226973496377468,\n",
       "  0.01056370884180069,\n",
       "  0.01155195664614439,\n",
       "  0.012085415422916412,\n",
       "  0.011345495469868183,\n",
       "  0.012141607701778412,\n",
       "  0.01252396497875452,\n",
       "  0.010966167785227299,\n",
       "  0.011379405856132507,\n",
       "  0.01173288095742464,\n",
       "  0.010679707862436771,\n",
       "  0.012432972900569439,\n",
       "  0.011389889754354954,\n",
       "  0.010898054577410221,\n",
       "  0.010302048176527023,\n",
       "  0.010990592651069164,\n",
       "  0.011286676861345768,\n",
       "  0.010924197733402252,\n",
       "  0.011089908890426159,\n",
       "  0.01237430889159441,\n",
       "  0.011067051440477371,\n",
       "  0.011038823053240776,\n",
       "  0.011937825009226799,\n",
       "  0.012141312472522259,\n",
       "  0.011541533283889294,\n",
       "  0.012336130253970623,\n",
       "  0.011568188667297363,\n",
       "  0.010511895641684532,\n",
       "  0.011049579828977585,\n",
       "  0.012346362695097923,\n",
       "  0.012035222724080086,\n",
       "  0.010961533524096012,\n",
       "  0.012386283837258816,\n",
       "  0.012188387103378773,\n",
       "  0.011894412338733673,\n",
       "  0.011353882029652596,\n",
       "  0.012344500981271267,\n",
       "  0.013065931387245655,\n",
       "  0.011103861965239048,\n",
       "  0.010725785978138447,\n",
       "  0.011131019331514835,\n",
       "  0.01144629716873169,\n",
       "  0.011595179326832294,\n",
       "  0.011851093731820583,\n",
       "  0.011995241045951843,\n",
       "  0.012171351350843906,\n",
       "  0.010747486725449562,\n",
       "  0.011342947371304035,\n",
       "  0.011734869331121445,\n",
       "  0.011762697249650955,\n",
       "  0.011718576774001122,\n",
       "  0.01225100178271532,\n",
       "  0.01114210020750761,\n",
       "  0.010318105109035969,\n",
       "  0.011088299565017223,\n",
       "  0.012454227544367313,\n",
       "  0.011349422857165337,\n",
       "  0.011817492544651031,\n",
       "  0.012250605039298534,\n",
       "  0.01221225131303072,\n",
       "  0.012341801077127457,\n",
       "  0.010954863391816616,\n",
       "  0.012006002478301525,\n",
       "  0.013509736396372318,\n",
       "  0.011287987232208252,\n",
       "  0.010833614505827427,\n",
       "  0.01109700184315443,\n",
       "  0.011775882914662361,\n",
       "  0.01135217398405075,\n",
       "  0.01115709450095892,\n",
       "  0.012303534895181656,\n",
       "  0.013062460348010063,\n",
       "  0.011687065474689007,\n",
       "  0.012596271000802517,\n",
       "  0.011396721936762333,\n",
       "  0.011381832882761955,\n",
       "  0.011842087842524052,\n",
       "  0.013987325131893158,\n",
       "  0.010978383012115955,\n",
       "  0.011398346163332462,\n",
       "  0.010576508939266205,\n",
       "  0.011576958931982517,\n",
       "  0.011626510880887508,\n",
       "  0.011623996309936047,\n",
       "  0.012014366686344147,\n",
       "  0.011187917552888393,\n",
       "  0.01158322487026453,\n",
       "  0.011278717778623104,\n",
       "  0.011294576339423656,\n",
       "  0.012034422717988491,\n",
       "  0.011884554289281368,\n",
       "  0.010483874939382076,\n",
       "  0.009711517952382565,\n",
       "  0.011191461235284805,\n",
       "  0.01159647200256586,\n",
       "  0.010582402348518372,\n",
       "  0.010894309729337692,\n",
       "  0.0115626510232687,\n",
       "  0.010538565926253796,\n",
       "  0.0104203000664711,\n",
       "  0.01088699046522379,\n",
       "  0.010405688546597958,\n",
       "  0.010998756624758244,\n",
       "  0.010885436087846756,\n",
       "  0.009817322716116905,\n",
       "  0.009732709266245365,\n",
       "  0.010217329487204552,\n",
       "  0.010376863181591034,\n",
       "  0.01036741305142641,\n",
       "  0.010868427343666553,\n",
       "  0.010863330215215683,\n",
       "  0.010170101188123226,\n",
       "  0.010113991796970367,\n",
       "  0.01037236675620079,\n",
       "  0.009611486457288265,\n",
       "  0.011315437965095043,\n",
       "  0.010057870298624039,\n",
       "  0.009278152137994766,\n",
       "  0.009086293168365955,\n",
       "  0.010579099878668785,\n",
       "  0.010134633630514145,\n",
       "  0.00973862037062645,\n",
       "  0.010166723281145096,\n",
       "  0.010657048784196377,\n",
       "  0.009664570912718773,\n",
       "  0.010233409702777863,\n",
       "  0.009855302982032299,\n",
       "  0.00944383442401886,\n",
       "  0.01114549208432436,\n",
       "  0.00986496452242136,\n",
       "  0.009401446208357811,\n",
       "  0.00914351548999548,\n",
       "  0.00989554449915886,\n",
       "  0.009931357577443123,\n",
       "  0.010192178189754486,\n",
       "  0.010314594954252243,\n",
       "  0.010886231437325478,\n",
       "  0.010153502225875854,\n",
       "  0.009894761256873608,\n",
       "  0.010710437782108784,\n",
       "  0.010883078910410404,\n",
       "  0.01085314154624939,\n",
       "  0.010840007103979588,\n",
       "  0.009568367153406143,\n",
       "  0.009344126097857952,\n",
       "  0.009788441471755505,\n",
       "  0.010143604129552841,\n",
       "  0.009837891906499863,\n",
       "  0.010822844691574574,\n",
       "  0.010832712054252625,\n",
       "  0.00951953325420618,\n",
       "  0.009873801842331886,\n",
       "  0.009912326000630856,\n",
       "  0.009630898013710976,\n",
       "  0.010549155063927174,\n",
       "  0.010490053333342075,\n",
       "  0.009162460453808308,\n",
       "  0.008981668390333652,\n",
       "  0.009945949539542198,\n",
       "  0.010206432081758976,\n",
       "  0.009634510613977909,\n",
       "  0.010219551622867584,\n",
       "  0.01036321185529232,\n",
       "  0.00965895690023899,\n",
       "  0.009736877866089344,\n",
       "  0.009698457084596157,\n",
       "  0.009186001494526863,\n",
       "  0.010281144641339779,\n",
       "  0.010469164699316025,\n",
       "  0.009321619756519794,\n",
       "  0.008981166407465935,\n",
       "  0.00968980509787798,\n",
       "  0.010455437004566193,\n",
       "  0.010053339414298534,\n",
       "  0.009924688376486301,\n",
       "  0.010804510675370693,\n",
       "  0.010266847908496857,\n",
       "  0.010612421669065952,\n",
       "  0.010450117290019989,\n",
       "  0.009607985615730286,\n",
       "  0.011646158061921597,\n",
       "  0.011730542406439781,\n",
       "  0.00919437687844038,\n",
       "  0.010006750002503395,\n",
       "  0.010655196383595467,\n",
       "  0.010253345593810081,\n",
       "  0.010074583813548088,\n",
       "  0.010815262794494629,\n",
       "  0.010620679706335068,\n",
       "  0.009434747509658337,\n",
       "  0.010896919295191765,\n",
       "  0.01062818430364132,\n",
       "  0.009587305597960949,\n",
       "  0.01190355233848095,\n",
       "  0.011456803418695927,\n",
       "  0.009812244214117527,\n",
       "  0.010201763361692429,\n",
       "  0.011225489899516106,\n",
       "  0.012120495550334454,\n",
       "  0.010566460900008678,\n",
       "  0.011014056392014027,\n",
       "  0.01260788831859827,\n",
       "  0.011723114177584648,\n",
       "  0.011367525905370712,\n",
       "  0.010177708230912685,\n",
       "  0.011141792871057987,\n",
       "  0.012245822697877884,\n",
       "  0.00984468124806881,\n",
       "  0.009545323438942432,\n",
       "  0.009350905194878578,\n",
       "  0.010712308809161186,\n",
       "  0.009925933554768562,\n",
       "  0.01020137034356594,\n",
       "  0.010235455818474293,\n",
       "  0.010383671149611473,\n",
       "  0.010302677750587463,\n",
       "  0.010344033129513264,\n",
       "  0.009792455472052097,\n",
       "  0.009389274753630161,\n",
       "  0.01207964401692152,\n",
       "  0.009649703279137611,\n",
       "  0.009744247421622276,\n",
       "  0.008837416768074036,\n",
       "  0.009792831726372242,\n",
       "  0.01087157242000103,\n",
       "  0.00900945533066988,\n",
       "  0.010301056317985058,\n",
       "  0.010282270610332489,\n",
       "  0.01005413569509983,\n",
       "  0.009407506324350834,\n",
       "  0.009527411311864853,\n",
       "  0.0096769193187356,\n",
       "  0.010739780031144619,\n",
       "  0.009714499115943909,\n",
       "  0.009445039555430412,\n",
       "  0.008512042462825775,\n",
       "  0.009571724571287632,\n",
       "  0.009306037798523903,\n",
       "  0.009988552890717983,\n",
       "  0.009596803225576878,\n",
       "  0.009708947502076626,\n",
       "  0.009527226909995079,\n",
       "  0.009817209094762802,\n",
       "  0.008942472748458385,\n",
       "  0.008745376951992512,\n",
       "  0.009701427072286606,\n",
       "  0.008542708121240139,\n",
       "  0.00820392370223999,\n",
       "  0.008073709905147552,\n",
       "  0.00865242164582014,\n",
       "  0.008525252342224121,\n",
       "  0.008519420400261879,\n",
       "  0.00862684566527605,\n",
       "  0.009056095965206623,\n",
       "  0.0091292355209589,\n",
       "  0.008451838977634907,\n",
       "  0.008775047957897186,\n",
       "  0.007982809096574783,\n",
       "  0.009737747721374035,\n",
       "  0.008511274121701717,\n",
       "  0.00802473071962595,\n",
       "  0.008037026040256023,\n",
       "  0.009100530296564102,\n",
       "  0.008944425731897354,\n",
       "  0.008280228823423386,\n",
       "  0.009018591605126858,\n",
       "  0.009284619241952896,\n",
       "  0.00881483219563961,\n",
       "  0.009143729694187641,\n",
       "  0.009243503212928772,\n",
       "  0.008114669471979141,\n",
       "  0.01062758732587099,\n",
       "  0.009065583348274231,\n",
       "  0.008647646754980087,\n",
       "  0.008681809529662132,\n",
       "  0.009862537495791912,\n",
       "  0.00941045768558979,\n",
       "  0.010217958129942417,\n",
       "  0.009042498655617237,\n",
       "  0.010468835942447186,\n",
       "  0.008938579820096493,\n",
       "  0.00925350096076727,\n",
       "  0.009482799097895622,\n",
       "  0.009581513702869415,\n",
       "  0.010988160967826843,\n",
       "  0.009601479396224022,\n",
       "  0.00904967449605465,\n",
       "  0.008478031493723392,\n",
       "  0.009868182241916656,\n",
       "  0.009861591272056103,\n",
       "  0.009112793020904064,\n",
       "  0.010657356120646,\n",
       "  0.009610502049326897,\n",
       "  0.01149783469736576,\n",
       "  0.009593273513019085,\n",
       "  0.009616650640964508,\n",
       "  0.009413076564669609,\n",
       "  0.010778679512441158,\n",
       "  0.012062803842127323,\n",
       "  0.009703933261334896,\n",
       "  0.008606952615082264,\n",
       "  0.010259107686579227,\n",
       "  0.011028159409761429,\n",
       "  0.010462304577231407,\n",
       "  0.010660196654498577,\n",
       "  0.01056526880711317,\n",
       "  0.010598847642540932,\n",
       "  0.010802607983350754,\n",
       "  0.01087155845016241,\n",
       "  0.009288971312344074,\n",
       "  0.0121707022190094,\n",
       "  0.009149996563792229,\n",
       "  0.00972038134932518,\n",
       "  0.010440288111567497,\n",
       "  0.01106736809015274,\n",
       "  0.01176426187157631,\n",
       "  0.010589822195470333,\n",
       "  0.010584408417344093,\n",
       "  0.010569513775408268,\n",
       "  0.011640678159892559,\n",
       "  0.00967465154826641,\n",
       "  0.011346637271344662,\n",
       "  0.011028413660824299,\n",
       "  0.009999308735132217,\n",
       "  0.011461418122053146,\n",
       "  0.010412188246846199,\n",
       "  0.010186655446887016,\n",
       "  0.010195725597441196,\n",
       "  0.010646836832165718,\n",
       "  0.011036437936127186,\n",
       "  0.010369759984314442,\n",
       "  0.01038461085408926,\n",
       "  0.010194205678999424,\n",
       "  0.011324855498969555,\n",
       "  0.009898769669234753,\n",
       "  0.009433876723051071,\n",
       "  0.011196769773960114,\n",
       "  0.009618600830435753,\n",
       "  0.008725789375603199,\n",
       "  0.009374366141855717,\n",
       "  0.009777584113180637,\n",
       "  0.009028835222125053,\n",
       "  0.009563003666698933,\n",
       "  0.009591317735612392,\n",
       "  0.010341079905629158,\n",
       "  0.009901964105665684,\n",
       "  0.010475779883563519,\n",
       "  0.009266398847103119,\n",
       "  0.008715813048183918,\n",
       "  0.011738289147615433,\n",
       "  0.00973958894610405,\n",
       "  0.009054858237504959,\n",
       "  0.008397229015827179,\n",
       "  0.009698419831693172,\n",
       "  0.010099297389388084,\n",
       "  0.009157214313745499,\n",
       "  0.009828776121139526,\n",
       "  0.009233543649315834,\n",
       "  0.010615106672048569,\n",
       "  0.009023499675095081,\n",
       "  0.010575204156339169,\n",
       "  0.00872754119336605,\n",
       "  0.011049596592783928,\n",
       "  0.009443283081054688,\n",
       "  0.009297150187194347,\n",
       "  0.008644110523164272,\n",
       "  0.010010435245931149,\n",
       "  0.009114312939345837,\n",
       "  0.009577974677085876,\n",
       "  0.009765716269612312,\n",
       "  0.009816337376832962,\n",
       "  0.009913084097206593,\n",
       "  0.010847142897546291,\n",
       "  0.009287849999964237,\n",
       "  0.008910947479307652,\n",
       "  0.010062032379209995,\n",
       "  0.009231324307620525,\n",
       "  0.008580776862800121,\n",
       "  0.008115570992231369,\n",
       "  0.00895348098129034,\n",
       "  0.009161517024040222,\n",
       "  0.008442370221018791,\n",
       "  0.008680799975991249,\n",
       "  0.00969843752682209,\n",
       "  0.00913403369486332,\n",
       "  0.00878649577498436,\n",
       "  0.009142952039837837,\n",
       "  ...],\n",
       " 'testerrors1': [],\n",
       " 'testerrors2': [],\n",
       " 'testerrorsinf': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models\n",
    "lconfig = load_config(\"../autoencoder/configs/experiments/donnormal.yaml\")\n",
    "experiment = models.TimeInputHelper(lconfig)\n",
    "\n",
    "FFNet = models.FFNet\n",
    "test = experiment.create_timeinput(dset)\n",
    "\n",
    "test.train_model(400, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8a3005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 WeldNet AEs and props together\n",
      "Number of NN trainable parameters 1135132\n",
      "Starting training WeldNet AE + Prop 1/1 (0->300) at Fri Jul  4 22:50:38 2025...\n",
      "train torch.Size([400, 301, 128]) test (100, 301, 128)\n",
      "1: Train Loss 5.020e-02 + 1.825e-04, LR 1.000e-03, Relative AE Error (1, 2, inf): 0.486830, 0.554858, 0.906445\n",
      "11: Train Loss 9.709e-03 + 9.007e-06, LR 1.000e-03, Relative AE Error (1, 2, inf): 0.181283, 0.255140, 0.626496\n",
      "21: Train Loss 4.984e-03 + 9.080e-06, LR 1.000e-03, Relative AE Error (1, 2, inf): 0.141325, 0.193488, 0.611253\n",
      "31: Train Loss 2.565e-03 + 7.542e-06, LR 1.000e-03, Relative AE Error (1, 2, inf): 0.088458, 0.132817, 0.466957\n",
      "41: Train Loss 1.489e-03 + 5.852e-06, LR 1.000e-03, Relative AE Error (1, 2, inf): 0.068213, 0.103190, 0.398821\n",
      "51: Train Loss 1.795e-03 + 1.547e-05, LR 1.000e-03, Relative AE Error (1, 2, inf): 0.095790, 0.132489, 0.498541\n",
      "61: Train Loss 1.360e-03 + 9.637e-06, LR 1.000e-03, Relative AE Error (1, 2, inf): 0.074751, 0.102068, 0.398367\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m FFNet \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mFFNet\n\u001b[0;32m      6\u001b[0m test \u001b[38;5;241m=\u001b[39m experimentg\u001b[38;5;241m.\u001b[39mcreate_weld(dset, windows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_aes_plus_props\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\operatorlearning\\src\\notebooks\\..\\models.py:1544\u001b[0m, in \u001b[0;36mWeldNet.train_aes_plus_props\u001b[1;34m(self, epochs, lamb, save, optim, lr, plottb, gridbatch, printinterval, batch, ridge, loss, encoding_param, best, verbose)\u001b[0m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maestep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m-> 1544\u001b[0m     lossesN, testerrors1N, testerrors2N, testerrorsinfN \u001b[38;5;241m=\u001b[39m \u001b[43mboth_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintinterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprintinterval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestarr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1545\u001b[0m     losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lossesN; testerrors1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m testerrors1N; testerrors2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m testerrors2N; testerrorsinf \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m testerrorsinfN\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best \u001b[38;5;129;01mand\u001b[39;00m ep \u001b[38;5;241m>\u001b[39m epochs \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\operatorlearning\\src\\notebooks\\..\\models.py:1469\u001b[0m, in \u001b[0;36mWeldNet.train_aes_plus_props.<locals>.both_epoch\u001b[1;34m(model, modelprop, dataloader, writer, optimizer, scheduler, ep, printinterval, loss, testarr)\u001b[0m\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m   1468\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1469\u001b[0m   lossout \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1470\u001b[0m   losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(lossout\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()))\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Anaconda3\\envs\\das\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Anaconda3\\envs\\das\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Anaconda3\\envs\\das\\lib\\site-packages\\torch\\optim\\adamw.py:197\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 197\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    200\u001b[0m     params_with_grad: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\operatorlearning\\src\\notebooks\\..\\models.py:1469\u001b[0m, in \u001b[0;36mWeldNet.train_aes_plus_props.<locals>.both_epoch.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m   1468\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maestep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1469\u001b[0m   lossout \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1470\u001b[0m   losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(lossout\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()))\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\operatorlearning\\src\\notebooks\\..\\models.py:1459\u001b[0m, in \u001b[0;36mWeldNet.train_aes_plus_props.<locals>.both_epoch.<locals>.closure\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m   1455\u001b[0m totalloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres \u001b[38;5;241m+\u001b[39m lamb \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror\n\u001b[0;32m   1457\u001b[0m totalloss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m-> 1459\u001b[0m num \u001b[38;5;241m=\u001b[39m \u001b[43mtotalloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maestep \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[0;32m   1462\u001b[0m   writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain/loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(num), global_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maestep)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Anaconda3\\envs\\das\\lib\\site-packages\\torch\\utils\\_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import models\n",
    "lconfig = load_config(\"../autoencoder/configs/experiments/weldnormal.yaml\")\n",
    "experimentg = models.WeldHelper(lconfig)\n",
    "\n",
    "FFNet = models.FFNet\n",
    "test = experimentg.create_weld(dset, windows=1)\n",
    "\n",
    "test.train_aes_plus_props(400, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dd218a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NN trainable parameters 1386528\n",
      "Starting training LTINet model at Fri Jul  4 22:29:04 2025...\n",
      "train torch.Size([400, 301, 128]) test (100, 301, 128)\n",
      "1: Train Loss 9.602e-02, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.708935, 0.792212, 0.964838\n",
      "11: Train Loss 2.010e-02, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.267529, 0.354214, 0.729388\n",
      "21: Train Loss 1.268e-02, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.205877, 0.287795, 0.694042\n",
      "31: Train Loss 8.363e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.158464, 0.236354, 0.650605\n",
      "41: Train Loss 6.621e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.143211, 0.212019, 0.626961\n",
      "51: Train Loss 5.305e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.129349, 0.194317, 0.606483\n",
      "61: Train Loss 4.532e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.119608, 0.179407, 0.584500\n",
      "71: Train Loss 3.918e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.111702, 0.167281, 0.561656\n",
      "81: Train Loss 3.839e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.112542, 0.165706, 0.568269\n",
      "91: Train Loss 3.248e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.101765, 0.155096, 0.555875\n",
      "101: Train Loss 2.776e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.088753, 0.137911, 0.498856\n",
      "111: Train Loss 2.365e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.088387, 0.131867, 0.482643\n",
      "121: Train Loss 3.505e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.122726, 0.166000, 0.565692\n",
      "131: Train Loss 2.016e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.085875, 0.129263, 0.490241\n",
      "141: Train Loss 3.317e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.101560, 0.144798, 0.536172\n",
      "151: Train Loss 1.938e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.074560, 0.116172, 0.467766\n",
      "161: Train Loss 2.148e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.087034, 0.123372, 0.468702\n",
      "171: Train Loss 1.902e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.081512, 0.116369, 0.444060\n",
      "181: Train Loss 1.622e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.082220, 0.119234, 0.471399\n",
      "191: Train Loss 2.034e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.084877, 0.117293, 0.455974\n",
      "201: Train Loss 1.209e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.060684, 0.095898, 0.396601\n",
      "211: Train Loss 1.487e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.075445, 0.105478, 0.409894\n",
      "221: Train Loss 1.732e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.079101, 0.112204, 0.445748\n",
      "231: Train Loss 1.537e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.082820, 0.111080, 0.430481\n",
      "241: Train Loss 1.154e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.062136, 0.093645, 0.403480\n",
      "251: Train Loss 9.585e-04, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.056304, 0.085817, 0.363947\n",
      "261: Train Loss 1.457e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.094404, 0.123287, 0.459235\n",
      "271: Train Loss 1.518e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.074285, 0.100982, 0.425038\n",
      "281: Train Loss 1.571e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.075881, 0.102351, 0.414407\n",
      "291: Train Loss 1.019e-03, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.061180, 0.087675, 0.371776\n",
      "301: Train Loss 9.805e-04, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.055558, 0.082831, 0.368709\n",
      "311: Train Loss 8.339e-04, LR 1.000e-03, Relative LTINet Error (1, 2, inf): 0.053118, 0.080201, 0.355962\n",
      "321: Train Loss 5.995e-04, LR 1.000e-04, Relative LTINet Error (1, 2, inf): 0.033544, 0.066416, 0.327438\n",
      "331: Train Loss 5.805e-04, LR 1.000e-04, Relative LTINet Error (1, 2, inf): 0.032018, 0.065196, 0.319920\n",
      "341: Train Loss 5.746e-04, LR 1.000e-04, Relative LTINet Error (1, 2, inf): 0.031883, 0.064868, 0.318295\n",
      "351: Train Loss 5.696e-04, LR 1.000e-04, Relative LTINet Error (1, 2, inf): 0.031772, 0.064586, 0.317040\n",
      "361: Train Loss 5.648e-04, LR 1.000e-04, Relative LTINet Error (1, 2, inf): 0.031681, 0.064325, 0.315990\n",
      "371: Train Loss 5.601e-04, LR 1.000e-04, Relative LTINet Error (1, 2, inf): 0.031609, 0.064074, 0.315037\n",
      "381: Train Loss 5.554e-04, LR 1.000e-04, Relative LTINet Error (1, 2, inf): 0.031540, 0.063822, 0.314101\n",
      "391: Train Loss 5.507e-04, LR 1.000e-04, Relative LTINet Error (1, 2, inf): 0.031458, 0.063559, 0.313094\n",
      "Finished training LTINet model at Fri Jul  4 22:30:48 2025...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LTINet' object has no attribute 'dynclass'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m FFNet \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mFFNet\n\u001b[0;32m      6\u001b[0m test \u001b[38;5;241m=\u001b[39m experiment\u001b[38;5;241m.\u001b[39mcreate_ltinet(dset)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 420\u001b[0m, in \u001b[0;36mLTINet.train_model\u001b[1;34m(self, epochs, save, optim, lr, printinterval, batch, ridge, loss, best, verbose)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Compute total training epochs\u001b[39;00m\n\u001b[0;32m    416\u001b[0m total_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    418\u001b[0m filename \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynclass\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecclass\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mep_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m )\n\u001b[0;32m    429\u001b[0m dire \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msavedmodels/ltinet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m addr \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dire, filename)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LTINet' object has no attribute 'dynclass'"
     ]
    }
   ],
   "source": [
    "import models\n",
    "lconfig = load_config(\"../autoencoder/configs/experiments/ltinet.yaml\")\n",
    "experiment = LTINetHelper(lconfig)\n",
    "\n",
    "FFNet = models.FFNet\n",
    "test = experiment.create_ltinet(dset)\n",
    "\n",
    "test.train_model(400, lr=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "das",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
