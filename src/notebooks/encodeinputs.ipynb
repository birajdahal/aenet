{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd297a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "basedir = \"../..\"\n",
    "\n",
    "from common.config import create_object, load_config\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.disable()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# dconfig = load_config(\"../autoencoder/configs/data/burgersshift.yaml\")\n",
    "# dconfig.datasize.spacedim = 1\n",
    "# dset = create_object(dconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import itertools\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from copy import deepcopy\n",
    "\n",
    "import utils\n",
    "\n",
    "class ETINetHelper():\n",
    "  def __init__(self, config):\n",
    "    self.update_config(config)\n",
    "\n",
    "  def update_config(self, config):\n",
    "    self.config = deepcopy(config)\n",
    "\n",
    "  def create_etinet(self, dataset, k, config=None, **args):\n",
    "    if config is None:\n",
    "      config = self.config\n",
    "\n",
    "    assert(len(dataset.data.shape) < 4)\n",
    "    if len(dataset.data.shape) == 3:\n",
    "      din = dataset.params.shape[-1]\n",
    "      dout = dataset.data.shape[-1]\n",
    "\n",
    "    td = args.get(\"td\", None)\n",
    "    seed = args.get(\"seed\", 0)\n",
    "    device = args.get(\"device\", 0)\n",
    "\n",
    "    recclass = globals()[args.get(\"recclass\", config.recclass)]\n",
    "    recparams = copy.deepcopy(dict(args.get(\"recparams\", config.recparams)))\n",
    "\n",
    "    aeclass = globals()[args.get(\"aeclass\", config.aeclass)]\n",
    "    aeparams = copy.deepcopy(dict(args.get(\"aeparams\", config.aeparams)))\n",
    "\n",
    "    recparams[\"seq\"][0] = k + 1\n",
    "    recparams[\"seq\"][-1] = dout\n",
    "\n",
    "    return ETINet(dataset, k, aeclass, aeparams, recclass, recparams, td=td, seed=seed, device=device)\n",
    "\n",
    "  @staticmethod\n",
    "  def get_operrs(etinet, times=None, testonly=False):\n",
    "    if testonly:\n",
    "      data = etinet.dataset.data[etinet.numtrain:,]\n",
    "    else:\n",
    "      data = etinet.dataset.data\n",
    "\n",
    "    errors = etinet.get_errors(data, times=times, aggregate=False)\n",
    "\n",
    "    return errors\n",
    "  \n",
    "  @staticmethod\n",
    "  def plot_op_predicts(etinet, testonly=False, xs=None, cmap=\"viridis\"):\n",
    "    if testonly:\n",
    "      data = etinet.dataset.data[etinet.numtrain:,]\n",
    "      params = etinet.dataset.params[etinet.numtrain:,]\n",
    "    else:\n",
    "      data = etinet.dataset.data\n",
    "      params = etinet.dataset.params\n",
    "\n",
    "    if xs == None:\n",
    "      xs = np.linspace(0, 1, len(data[0, 0]))\n",
    "\n",
    "    params = torch.tensor(np.float32(params)).to(etinet.device)\n",
    "\n",
    "    predicts = etinet.propagate(params).cpu().detach()\n",
    "\n",
    "    errors = []\n",
    "    n = predicts.shape[0]\n",
    "    for s in range(data.shape[1]):\n",
    "      currpredict = predicts[:, s-1].reshape((n, -1))\n",
    "      currreference = data[:, s].reshape((n, -1))\n",
    "      errors.append(np.mean(np.linalg.norm(currpredict - currreference, axis=1) / np.linalg.norm(currreference, axis=1)))\n",
    "        \n",
    "    print(f\"Average Relative L2 Error over all times: {np.mean(errors):.4f}\")\n",
    "\n",
    "    if len(data.shape) == 3:\n",
    "      fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "    @widgets.interact(i=(0, n-1), s=(1, etinet.T-1))\n",
    "    def plot_interact(i=0, s=1):\n",
    "      print(f\"Avg Relative L2 Error for t0 to t{s}: {errors[s-1]:.4f}\")\n",
    "\n",
    "      if len(data.shape) == 3:\n",
    "        ax.clear()\n",
    "        ax.set_title(f\"RelL2 {np.linalg.norm(predicts[i, s-1] - data[i, s]) / np.linalg.norm(data[i, s])}\")\n",
    "        ax.plot(xs, data[i, 0], label=\"Input\", linewidth=1)\n",
    "        ax.plot(xs, predicts[i, s-1], label=\"Predicted\", linewidth=1)\n",
    "        ax.plot(xs, data[i, s], label=\"Exact\", linewidth=1)\n",
    "        ax.legend()\n",
    "        \n",
    "  @staticmethod\n",
    "  def plot_errorparams(etinet, param=-1):\n",
    "    if param == -1:\n",
    "        # Auto-detect one varying parameter\n",
    "        param = 0\n",
    "        P = etinet.dataset.params.shape[1]\n",
    "        for p in range(P):\n",
    "            if np.abs(etinet.dataset.params[0, p] - etinet.dataset.params[1, p]) > 0:\n",
    "                param = p\n",
    "                break\n",
    "\n",
    "    l2error = np.asarray(ETINetHelper.get_operrs(etinet, times=[etinet.T - 1]))\n",
    "    params = etinet.dataset.params\n",
    "\n",
    "    print(params.shape, l2error.shape)\n",
    "\n",
    "    if isinstance(param, (list, tuple, np.ndarray)) and len(param) == 2:\n",
    "        # 3D scatter plot for 2 varying parameters\n",
    "        x = params[:, param[0]]\n",
    "        y = params[:, param[1]]\n",
    "        z = l2error\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        sc = ax.scatter(x, y, z, c=z, cmap='viridis', s=10)\n",
    "\n",
    "        ax.set_xlabel(f\"Param {param[0]}\")\n",
    "        ax.set_ylabel(f\"Param {param[1]}\")\n",
    "        ax.set_zlabel(\"Operator Error\")\n",
    "        fig.colorbar(sc, ax=ax, label=\"Operator Error\")\n",
    "\n",
    "    else:\n",
    "        # Fallback to 2D scatter if param is 1D\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(params[:, param], l2error, s=2)\n",
    "        ax.set_xlabel(f\"Parameter {param}\")\n",
    "        ax.set_ylabel(\"Operator Error\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "class ETINet():\n",
    "  def __init__(self, dataset, k, aeclass, aeparams, recclass, recparams, td, seed, device):\n",
    "    self.dataset = dataset\n",
    "    self.device = device\n",
    "    self.td = td\n",
    "    self.k = k\n",
    "  \n",
    "    if self.td is None:\n",
    "      self.prefix = f\"{self.dataset.name}{str(recclass.__name__)}ETINet\"\n",
    "    else:\n",
    "      self.prefix = self.td\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    self.seed = seed\n",
    "\n",
    "    datacopy = self.dataset.data.copy()\n",
    "    self.numtrain = int(datacopy.shape[0] * 0.8)\n",
    "    \n",
    "    self.T = self.dataset.data.shape[1]\n",
    "    self.trainarr = datacopy[:self.numtrain]\n",
    "    self.testarr = datacopy[self.numtrain:]\n",
    "    self.optparams = None\n",
    "\n",
    "    self.datadim = len(self.dataset.data.shape) - 2\n",
    "    self.aestep = 0\n",
    "    self.recstep = 0\n",
    "\n",
    "    aeparams[\"encodeSeq\"][0] = self.dataset.data.shape[-1]\n",
    "    aeparams[\"encodeSeq\"][-1] = self.k\n",
    "    aeparams[\"decodeSeq\"][0] = self.k\n",
    "    aeparams[\"decodeSeq\"][-1] = self.dataset.data.shape[-1]\n",
    "\n",
    "    recparams[\"seq\"][0] = self.k + 1\n",
    "    recparams[\"seq\"][-1] = self.dataset.data.shape[-1]\n",
    "\n",
    "    self.aeclass = aeclass\n",
    "    self.aeparams = copy.deepcopy(aeparams)\n",
    "    self.recclass = recclass\n",
    "    self.recparams = copy.deepcopy(recparams)\n",
    "\n",
    "    self.aenet = aeclass(**aeparams).float().to(device)\n",
    "    self.recnet = recclass(**recparams).float().to(device)\n",
    "\n",
    "    self.metadata = {\n",
    "      \"aeclass\": aeclass.__name__,\n",
    "      \"aeparams\": aeparams,\n",
    "      \"recclass\": recclass.__name__,\n",
    "      \"recparams\": recparams,\n",
    "      \"dataset_name\": dataset.name,\n",
    "      \"data_shape\": list(dataset.data.shape),\n",
    "      \"data_checksum\": float(np.sum(dataset.data)),\n",
    "      \"seed\": seed,\n",
    "      \"epochs\": []\n",
    "    }\n",
    "\n",
    "  def reconstruct(self, z, ts):\n",
    "    z_shape = z.shape\n",
    "    *leading_dims, N = z_shape\n",
    "    T = ts.shape[0]\n",
    "\n",
    "    z_expanded = z.unsqueeze(-2).expand(*leading_dims, T, N)\n",
    "\n",
    "    t_shape = [1] * len(leading_dims) + [T, 1]\n",
    "    t_expanded = ts.view(*t_shape).expand(*leading_dims, T, 1)\n",
    "\n",
    "    result = torch.cat([z_expanded, t_expanded], dim=-1)\n",
    "\n",
    "    recon = self.recnet(result)\n",
    "    return recon\n",
    "\n",
    "  def propagate(self, code, start=1, end=-1):\n",
    "    fullts = torch.linspace(0, 1, self.T).float().to(self.device)\n",
    "    \n",
    "    if end > 0:\n",
    "      ts = fullts[start:end+1]\n",
    "    else:\n",
    "      ts = fullts[start:]\n",
    "\n",
    "    out = self.reconstruct(code, ts)\n",
    "    return out\n",
    "\n",
    "  def get_errors(self, testarr, testrest, ords=(2,), times=None, aggregate=True):\n",
    "    assert(aggregate or len(ords) == 1)\n",
    "    \n",
    "    if isinstance(testarr, np.ndarray):\n",
    "      testarr = torch.tensor(testarr, dtype=torch.float32)\n",
    "\n",
    "    if isinstance(testrest, np.ndarray):\n",
    "      testrest = torch.tensor(testrest, dtype=torch.float32)\n",
    "\n",
    "    if times is None:\n",
    "      times = range(self.T-1)\n",
    "  \n",
    "    out = self.propagate(testarr)\n",
    "\n",
    "    n = testarr.shape[0]\n",
    "    orig = testrest.cpu().detach().numpy()\n",
    "    out = out.cpu().detach().numpy()\n",
    "\n",
    "    if aggregate:\n",
    "      orig = orig.reshape([n, -1])\n",
    "      out = out.reshape([n, -1])\n",
    "      testerrs = []\n",
    "      for o in ords:\n",
    "        testerrs.append(np.mean(np.linalg.norm(orig - out, axis=1, ord=o) / np.linalg.norm(orig, axis=1, ord=o)))\n",
    "\n",
    "      return tuple(testerrs)\n",
    "    \n",
    "    else:\n",
    "      o = ords[0]\n",
    "      testerrs = []\n",
    "\n",
    "      if len(times) == 1:\n",
    "        t = times[0]\n",
    "        origslice = orig[:, t].reshape([n, -1])\n",
    "        outslice = out[:, t].reshape([n, -1])\n",
    "        return np.linalg.norm(origslice - outslice, axis=1, ord=o) / np.linalg.norm(origslice, axis=1, ord=o)\n",
    "      else:\n",
    "        for t in range(orig.shape[1]):\n",
    "          origslice = orig[:, t].reshape([n, -1])\n",
    "          outslice = out[:, t].reshape([n, -1])\n",
    "          testerrs.append(np.mean(np.linalg.norm(origslice - outslice, axis=1, ord=o) / np.linalg.norm(origslice, axis=1, ord=o)))\n",
    "\n",
    "        return testerrs\n",
    "      \n",
    "  def get_ae_errors(self, testarr, ords=(2,)):\n",
    "    if isinstance(testarr, np.ndarray):\n",
    "      testarr = torch.tensor(testarr, dtype=torch.float32)\n",
    "  \n",
    "    out = self.aenet(testarr).cpu().detach().numpy()\n",
    "    orig = testarr.cpu().detach().numpy()\n",
    "\n",
    "    testerrs = []\n",
    "    for o in ords:\n",
    "      testerrs.append(np.mean(np.linalg.norm(orig - out, axis=1, ord=o) / np.linalg.norm(orig, axis=1, ord=o)))\n",
    "\n",
    "    return tuple(testerrs)\n",
    "\n",
    "  def load_models(self, filename_prefix, verbose=False, min_epochs=0):\n",
    "    search_path = f\"savedmodels/etinet/{filename_prefix}*.pickle\"\n",
    "    matching_files = glob.glob(search_path)\n",
    "\n",
    "    print(\"Searching for model files matching prefix:\", filename_prefix)\n",
    "    if not hasattr(self, \"metadata\"):\n",
    "        raise ValueError(\"Missing self.metadata. Cannot match models without metadata. Ensure model has been initialized with same config.\")\n",
    "\n",
    "    for addr in matching_files:\n",
    "      try:\n",
    "          with open(addr, \"rb\") as handle:\n",
    "              dic = pickle.load(handle)\n",
    "      except Exception as e:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to read error: {e}\")\n",
    "          continue\n",
    "\n",
    "      meta = dic.get(\"metadata\", {})\n",
    "      is_match = all(\n",
    "          meta.get(k) == self.metadata.get(k)\n",
    "          for k in self.metadata.keys()\n",
    "      )\n",
    "\n",
    "      # Check if model meets the minimum epoch requirement\n",
    "      model_epochs = meta.get(\"epochs\")\n",
    "      if model_epochs is None:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to missing epoch metadata.\")\n",
    "          continue\n",
    "      elif isinstance(model_epochs, list):  # handle legacy or list format\n",
    "          if sum(model_epochs) < min_epochs:\n",
    "              if verbose:\n",
    "                  print(f\"Skipping {addr} due to insufficient epochs ({sum(model_epochs)} < {min_epochs})\")\n",
    "              continue\n",
    "      elif model_epochs < min_epochs:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to insufficient epochs ({model_epochs} < {min_epochs})\")\n",
    "          continue\n",
    "\n",
    "      if is_match:\n",
    "          print(\"Model match found. Loading from:\", addr)\n",
    "          self.recnet.load_state_dict(dic[\"recnet\"])\n",
    "          self.aenet.load_state_dict(dic[\"aenet\"])\n",
    "          self.metadata[\"epochs\"] = meta.get(\"epochs\")\n",
    "          if \"opt\" in dic:     \n",
    "            self.optparams = dic[\"opt\"]\n",
    "\n",
    "          return True\n",
    "      elif verbose:\n",
    "          print(\"Metadata mismatch in file:\", addr)\n",
    "          for k in self.metadata:\n",
    "              print(f\"{k}: saved={meta.get(k)} vs current={self.metadata.get(k)}\")\n",
    "\n",
    "    print(\"Load failed. No matching models found.\")\n",
    "    print(\"Searched:\", matching_files)\n",
    "    return False\n",
    "\n",
    "  def train_recnet(self, epochs, save=True, optim=torch.optim.AdamW, lr=1e-4, printinterval=10, batch=32, ridge=0, loss=None, best=True, verbose=False):\n",
    "    def recnet_epoch(dataloader, writer=None, optimizer=None, scheduler=None, ep=0, printinterval=10, loss=None, testarr=None, testrest=None):\n",
    "      losses = []\n",
    "      testerrors1 = []\n",
    "      testerrors2 = []\n",
    "      testerrorsinf = []\n",
    "\n",
    "      def closure(codes, rest):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = self.propagate(codes)\n",
    "        target = rest\n",
    "        \n",
    "        res = loss(out, target)\n",
    "        res.backward()\n",
    "        \n",
    "        if writer is not None and self.recstep % 5 == 0:\n",
    "          writer.add_scalar(\"main/loss\", res, global_step=self.recstep)\n",
    "\n",
    "        return res\n",
    "\n",
    "      for codes, rest in dataloader:\n",
    "        self.recstep += 1\n",
    "        error = optimizer.step(lambda: closure(codes, rest))\n",
    "        losses.append(float(error.cpu().detach()))\n",
    "\n",
    "      if scheduler is not None:\n",
    "        scheduler.step(np.mean(losses))\n",
    "\n",
    "      # print test\n",
    "      if printinterval > 0 and (ep % printinterval == 0):\n",
    "        testerr1, testerr2, testerrinf = self.get_errors(testarr, testrest, ords=(1, 2, np.inf))\n",
    "        if scheduler is not None:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, LR {scheduler.get_last_lr()[-1]:.3e}, Relative ETINet Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "        else:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, Relative ETINet Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"misc/relativeL1error\", testerr1, global_step=ep)\n",
    "            writer.add_scalar(\"main/relativeL2error\", testerr2, global_step=ep)\n",
    "            writer.add_scalar(\"misc/relativeLInferror\", testerrinf, global_step=ep)\n",
    "\n",
    "      return losses, testerrors1, testerrors2, testerrorsinf\n",
    "  \n",
    "    assert(self.aestep > 0)\n",
    "\n",
    "    loss = nn.MSELoss() if loss is None else loss()\n",
    "\n",
    "    losses, testerrors1, testerrors2, testerrorsinf = [], [], [], []\n",
    "\n",
    "    initial = torch.tensor(self.trainarr[:, 0], dtype=torch.float32).to(self.device)\n",
    "    rest = torch.tensor(self.trainarr[:, 1:], dtype=torch.float32).to(self.device)\n",
    "    train = self.aenet.encode(initial).detach()\n",
    "\n",
    "    testinitial = torch.tensor(self.testarr[:, 0], dtype=torch.float32).to(self.device)\n",
    "    testrest = torch.tensor(self.testarr[:, 1:], dtype=torch.float32).to(self.device)\n",
    "    test = self.aenet.encode(testinitial).detach()\n",
    "\n",
    "    opt = optim(self.recnet.parameters(), lr=lr, weight_decay=ridge)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(opt, patience=30)\n",
    "    dataloader = DataLoader(torch.utils.data.TensorDataset(train, rest), shuffle=False, batch_size=batch)\n",
    "\n",
    "    writer = None\n",
    "    if self.td is not None:\n",
    "      name = f\"./tensorboard/{datetime.datetime.now().strftime('%d-%B-%Y')}/{self.td}/{datetime.datetime.now().strftime('%H.%M.%S')}/\"\n",
    "      writer = torch.utils.tensorboard.SummaryWriter(name)\n",
    "      print(\"Tensorboard writer location is \" + name)\n",
    "\n",
    "    print(\"Number of NN trainable parameters\", utils.num_params(self.recnet))\n",
    "    print(f\"Starting ETINet rec model at {time.asctime()}...\")\n",
    "    print(\"train\", train.shape, \"test\", test.shape)\n",
    "      \n",
    "    bestdict = { \"loss\": float(np.inf), \"ep\": 0 }\n",
    "    for ep in range(epochs):\n",
    "      lossesN, testerrors1N, testerrors2N, testerrorsinfN = recnet_epoch(dataloader, optimizer=opt, scheduler=scheduler, writer=writer, ep=ep, printinterval=printinterval, loss=loss, testarr=test, testrest=testrest)\n",
    "      losses += lossesN; testerrors1 += testerrors1N; testerrors2 += testerrors2N; testerrorsinf += testerrorsinfN\n",
    "\n",
    "      if best and ep > epochs // 2:\n",
    "        avgloss = np.mean(lossesN)\n",
    "        if avgloss < bestdict[\"loss\"]:\n",
    "          bestdict[\"recnet\"] = self.recnet.state_dict()\n",
    "          bestdict[\"opt\"] = opt.state_dict()\n",
    "          bestdict[\"loss\"] = avgloss\n",
    "          bestdict[\"ep\"] = ep\n",
    "        elif verbose:\n",
    "          print(f\"Loss not improved at epoch {ep} (Ratio: {avgloss/bestdict['loss']:.2f}) from {bestdict['ep']} (Loss: {bestdict['loss']:.2e})\")\n",
    "      \n",
    "    print(f\"Finished training ETINet rec model at {time.asctime()}...\")\n",
    "\n",
    "    if best:\n",
    "      self.recnet.load_state_dict(bestdict[\"recnet\"])\n",
    "      opt.load_state_dict(bestdict[\"opt\"])\n",
    "\n",
    "    self.aeoptparams = opt.state_dict()\n",
    "    self.metadata[\"aeepochs\"].append(epochs)\n",
    "\n",
    "    if save:\n",
    "      now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "      # Compute total training epochs\n",
    "      total_epochs = sum(self.metadata[\"epochs\"]) if isinstance(self.metadata[\"epochs\"], list) else self.metadata[\"epochs\"]\n",
    "\n",
    "      filename = (\n",
    "          f\"{self.dataset.name}_\"\n",
    "          f\"{self.aeclass.__name__}_\"\n",
    "          f\"{self.aeparams['seq']}_\"\n",
    "          f\"{self.recclass.__name__}_\"\n",
    "          f\"{self.recparams['seq']}_\"\n",
    "          f\"{self.seed}_\"\n",
    "          f\"{total_epochs}ep_\"\n",
    "          f\"{now}.pickle\"\n",
    "      )\n",
    "\n",
    "      dire = \"savedmodels/etinet\"\n",
    "      addr = os.path.join(dire, filename)\n",
    "\n",
    "      if not os.path.exists(dire):\n",
    "          os.makedirs(dire)\n",
    "\n",
    "      with open(addr, \"wb\") as handle:\n",
    "          pickle.dump({\n",
    "              \"aenet\": self.aenet.state_dict(),\n",
    "              \"recnet\": self.recnet.state_dict(),\n",
    "              \"metadata\": self.metadata,\n",
    "              \"opt\": self.optparams\n",
    "          }, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "      print(\"Model saved at\", addr)\n",
    "\n",
    "    return { \"losses\": losses, \"testerrors1\": testerrors1, \"testerrors2\": testerrors2, \"testerrorsinf\": testerrorsinf }\n",
    "\n",
    "  def train_aenet(self, epochs, save=True, optim=torch.optim.AdamW, lr=1e-4, printinterval=10, batch=32, ridge=0, loss=None, best=True, verbose=False):\n",
    "    def aenet_epoch(dataloader, writer=None, optimizer=None, scheduler=None, ep=0, printinterval=10, loss=None, testarr=None):\n",
    "      losses = []\n",
    "      testerrors1 = []\n",
    "      testerrors2 = []\n",
    "      testerrorsinf = []\n",
    "\n",
    "      def closure(codes):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = self.aenet(codes)\n",
    "        target = codes\n",
    "        \n",
    "        res = loss(out, target)\n",
    "        res.backward()\n",
    "        \n",
    "        if writer is not None and self.recstep % 5 == 0:\n",
    "          writer.add_scalar(\"main/aeloss\", res, global_step=self.recstep)\n",
    "\n",
    "        return res\n",
    "\n",
    "      for codes in dataloader:\n",
    "        self.aestep += 1\n",
    "        error = optimizer.step(lambda: closure(codes))\n",
    "        losses.append(float(error.cpu().detach()))\n",
    "\n",
    "      if scheduler is not None:\n",
    "        scheduler.step(np.mean(losses))\n",
    "\n",
    "      # print test\n",
    "      if printinterval > 0 and (ep % printinterval == 0):\n",
    "        testerr1, testerr2, testerrinf = self.get_ae_errors(testarr, ords=(1, 2, np.inf))\n",
    "        if scheduler is not None:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, LR {scheduler.get_last_lr()[-1]:.3e}, Relative ETINet AE Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "        else:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, Relative ETINet AE Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"misc/relativeL1error\", testerr1, global_step=ep)\n",
    "            writer.add_scalar(\"main/relativeL2error\", testerr2, global_step=ep)\n",
    "            writer.add_scalar(\"misc/relativeLInferror\", testerrinf, global_step=ep)\n",
    "\n",
    "      return losses, testerrors1, testerrors2, testerrorsinf\n",
    "  \n",
    "    loss = nn.MSELoss() if loss is None else loss()\n",
    "\n",
    "    losses, testerrors1, testerrors2, testerrorsinf = [], [], [], []\n",
    "\n",
    "    initial = torch.tensor(self.trainarr[:, 0], dtype=torch.float32).to(self.device)\n",
    "    test = torch.tensor(self.testarr[:, 0], dtype=torch.float32).to(self.device)\n",
    "\n",
    "    opt = optim(self.aenet.parameters(), lr=lr, weight_decay=ridge)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(opt, patience=30)\n",
    "    dataloader = DataLoader(initial, shuffle=False, batch_size=batch)\n",
    "\n",
    "    if self.optparams is not None:\n",
    "      opt.load_state_dict(self.optparams)\n",
    "\n",
    "    writer = None\n",
    "    if self.td is not None:\n",
    "      name = f\"./tensorboard/{datetime.datetime.now().strftime('%d-%B-%Y')}/{self.td}/{datetime.datetime.now().strftime('%H.%M.%S')}/\"\n",
    "      writer = torch.utils.tensorboard.SummaryWriter(name)\n",
    "      print(\"Tensorboard writer location is \" + name)\n",
    "\n",
    "    print(\"Number of NN trainable parameters\", utils.num_params(self.recnet))\n",
    "    print(f\"Starting training ETINet AE model at {time.asctime()}...\")\n",
    "    print(\"train\", initial.shape, \"test\", test.shape)\n",
    "      \n",
    "    bestdict = { \"loss\": float(np.inf), \"ep\": 0 }\n",
    "    for ep in range(epochs):\n",
    "      lossesN, testerrors1N, testerrors2N, testerrorsinfN = aenet_epoch(dataloader, optimizer=opt, scheduler=scheduler, writer=writer, ep=ep, printinterval=printinterval, loss=loss, testarr=test)\n",
    "      losses += lossesN; testerrors1 += testerrors1N; testerrors2 += testerrors2N; testerrorsinf += testerrorsinfN\n",
    "\n",
    "      if best and ep > epochs // 2:\n",
    "        avgloss = np.mean(lossesN)\n",
    "        if avgloss < bestdict[\"loss\"]:\n",
    "          bestdict[\"aenet\"] = self.aenet.state_dict()\n",
    "          bestdict[\"opt\"] = opt.state_dict()\n",
    "          bestdict[\"loss\"] = avgloss\n",
    "          bestdict[\"ep\"] = ep\n",
    "        elif verbose:\n",
    "          print(f\"Loss not improved at epoch {ep} (Ratio: {avgloss/bestdict['loss']:.2f}) from {bestdict['ep']} (Loss: {bestdict['loss']:.2e})\")\n",
    "      \n",
    "    print(f\"Finished training ETINet AE model at {time.asctime()}...\")\n",
    "\n",
    "    if best:\n",
    "      self.aenet.load_state_dict(bestdict[\"aenet\"])\n",
    "      opt.load_state_dict(bestdict[\"opt\"])\n",
    "\n",
    "    return { \"losses\": losses, \"testerrors1\": testerrors1, \"testerrors2\": testerrors2, \"testerrorsinf\": testerrorsinf }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd070be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dconfig = load_config(\"../autoencoder/configs/data/burgersshift.yaml\")\n",
    "dconfig.datasize.spacedim = 1\n",
    "dset = create_object(dconfig)\n",
    "\n",
    "#dset.downsample_time(10)\n",
    "dset.downsample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd218a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NN trainable parameters 800528\n",
      "Starting training ETINet AE model at Sat Jul  5 10:34:57 2025...\n",
      "train torch.Size([400, 128]) test torch.Size([100, 128])\n",
      "1: Train Loss 4.922e-02, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.424543, 0.429133, 0.601456\n",
      "11: Train Loss 2.471e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.034005, 0.034449, 0.046659\n",
      "21: Train Loss 3.528e-03, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.111951, 0.108367, 0.126833\n",
      "31: Train Loss 6.569e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.054653, 0.054799, 0.069752\n",
      "41: Train Loss 2.728e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.043904, 0.044011, 0.056270\n",
      "51: Train Loss 6.194e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.016327, 0.016507, 0.022629\n",
      "61: Train Loss 5.602e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.015526, 0.015650, 0.021682\n",
      "71: Train Loss 5.224e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.014990, 0.015098, 0.020943\n",
      "81: Train Loss 4.918e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.014505, 0.014609, 0.020311\n",
      "91: Train Loss 4.632e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.014045, 0.014149, 0.019730\n",
      "101: Train Loss 4.363e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.013612, 0.013719, 0.019157\n",
      "111: Train Loss 4.106e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.013193, 0.013303, 0.018596\n",
      "121: Train Loss 3.852e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.012780, 0.012894, 0.017990\n",
      "131: Train Loss 3.606e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.012373, 0.012490, 0.017417\n",
      "141: Train Loss 3.371e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.011971, 0.012090, 0.016843\n",
      "151: Train Loss 3.143e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.011570, 0.011689, 0.016264\n",
      "161: Train Loss 2.924e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.011170, 0.011291, 0.015693\n",
      "171: Train Loss 2.718e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.010767, 0.010895, 0.015166\n",
      "181: Train Loss 2.523e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.010368, 0.010500, 0.014644\n",
      "191: Train Loss 2.339e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.009981, 0.010115, 0.014140\n",
      "201: Train Loss 2.169e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.009599, 0.009738, 0.013647\n",
      "211: Train Loss 2.014e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.009232, 0.009373, 0.013202\n",
      "221: Train Loss 1.873e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.008880, 0.009023, 0.012773\n",
      "231: Train Loss 1.743e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.008541, 0.008690, 0.012357\n",
      "241: Train Loss 1.624e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.008206, 0.008363, 0.011943\n",
      "251: Train Loss 1.508e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.007875, 0.008046, 0.011565\n",
      "261: Train Loss 1.398e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.007569, 0.007748, 0.011171\n",
      "271: Train Loss 1.296e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.007278, 0.007460, 0.010797\n",
      "281: Train Loss 1.202e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.007011, 0.007190, 0.010434\n",
      "291: Train Loss 1.113e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.006748, 0.006925, 0.010098\n",
      "301: Train Loss 1.028e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.006496, 0.006673, 0.009741\n",
      "311: Train Loss 9.545e-06, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.006389, 0.006545, 0.009394\n",
      "321: Train Loss 9.365e-06, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.006813, 0.006920, 0.009658\n",
      "331: Train Loss 9.580e-06, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.007230, 0.007316, 0.010032\n",
      "341: Train Loss 1.189e-05, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.008439, 0.008520, 0.011513\n",
      "351: Train Loss 6.827e-06, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.005471, 0.005626, 0.008338\n",
      "361: Train Loss 6.376e-06, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.005339, 0.005496, 0.008181\n",
      "371: Train Loss 6.207e-06, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.005286, 0.005445, 0.008120\n",
      "381: Train Loss 6.095e-06, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.005249, 0.005408, 0.008079\n",
      "391: Train Loss 6.007e-06, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.005218, 0.005377, 0.008047\n",
      "Finished training ETINet AE model at Sat Jul  5 10:35:21 2025...\n",
      "Number of NN trainable parameters 800528\n",
      "Starting ETINet rec model at Sat Jul  5 10:35:21 2025...\n",
      "train torch.Size([400, 2]) test torch.Size([100, 2])\n",
      "1: Train Loss 3.569e-02, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.432724, 0.473387, 0.792500\n",
      "11: Train Loss 6.183e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.136324, 0.205734, 0.582156\n",
      "21: Train Loss 3.073e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.103898, 0.150454, 0.509490\n",
      "31: Train Loss 1.987e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.081972, 0.122911, 0.466929\n",
      "41: Train Loss 2.047e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.100632, 0.128329, 0.459438\n",
      "51: Train Loss 1.257e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.064697, 0.095883, 0.396412\n",
      "61: Train Loss 8.997e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.053757, 0.083381, 0.365580\n",
      "71: Train Loss 8.028e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.057199, 0.081684, 0.348806\n",
      "81: Train Loss 8.337e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.060325, 0.082392, 0.354964\n",
      "91: Train Loss 7.314e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.054859, 0.075391, 0.332361\n",
      "101: Train Loss 5.009e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.041527, 0.063281, 0.303148\n",
      "111: Train Loss 5.916e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.056741, 0.074541, 0.316116\n",
      "121: Train Loss 4.913e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.045988, 0.063377, 0.295586\n",
      "131: Train Loss 7.568e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.057279, 0.072785, 0.305324\n",
      "141: Train Loss 5.717e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.048479, 0.065509, 0.297459\n",
      "151: Train Loss 3.917e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.043452, 0.059217, 0.273875\n",
      "161: Train Loss 2.833e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.034838, 0.050002, 0.253203\n",
      "171: Train Loss 3.517e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.043284, 0.056246, 0.254967\n",
      "181: Train Loss 3.370e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.041854, 0.054599, 0.256116\n",
      "191: Train Loss 3.975e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.047171, 0.060118, 0.273549\n",
      "201: Train Loss 1.787e-04, LR 1.000e-04, Relative ETINet Error (1, 2, inf): 0.024015, 0.039990, 0.229696\n",
      "211: Train Loss 1.745e-04, LR 1.000e-04, Relative ETINet Error (1, 2, inf): 0.023780, 0.039576, 0.227279\n",
      "221: Train Loss 1.717e-04, LR 1.000e-04, Relative ETINet Error (1, 2, inf): 0.023638, 0.039272, 0.225522\n",
      "231: Train Loss 1.692e-04, LR 1.000e-04, Relative ETINet Error (1, 2, inf): 0.023519, 0.039001, 0.224045\n",
      "241: Train Loss 1.670e-04, LR 1.000e-04, Relative ETINet Error (1, 2, inf): 0.023408, 0.038743, 0.222641\n",
      "251: Train Loss 1.648e-04, LR 1.000e-04, Relative ETINet Error (1, 2, inf): 0.023303, 0.038492, 0.221270\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "lconfig = load_config(\"../autoencoder/configs/experiments/etinet.yaml\")\n",
    "experiment = ETINetHelper(lconfig)\n",
    "\n",
    "FFNet = models.FFNet\n",
    "FFAutoencoder = models.FFAutoencoder\n",
    "test = experiment.create_etinet(dset, 2)\n",
    "\n",
    "test.train_aenet(400, lr=1e-3)\n",
    "test.train_recnet(400, lr=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "das",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
