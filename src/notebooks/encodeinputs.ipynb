{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd297a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "basedir = \"../..\"\n",
    "\n",
    "from common.config import create_object, load_config\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.disable()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# dconfig = load_config(\"../autoencoder/configs/data/burgersshift.yaml\")\n",
    "# dconfig.datasize.spacedim = 1\n",
    "# dset = create_object(dconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd070be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dconfig = load_config(\"../autoencoder/configs/data/droplettrigpoly1.yaml\")\n",
    "dconfig.datasize.spacedim = 1\n",
    "dset = create_object(dconfig)\n",
    "\n",
    "#dset.downsample_time(10)\n",
    "#dset.downsample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa9bea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import itertools\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from copy import deepcopy\n",
    "\n",
    "import utils\n",
    "\n",
    "class ETINetHelper():\n",
    "  def __init__(self, config):\n",
    "    self.update_config(config)\n",
    "\n",
    "  def update_config(self, config):\n",
    "    self.config = deepcopy(config)\n",
    "\n",
    "  def create_etinet(self, dataset, k, config=None, **args):\n",
    "    if config is None:\n",
    "      config = self.config\n",
    "\n",
    "    assert(len(dataset.data.shape) < 4)\n",
    "    if len(dataset.data.shape) == 3:\n",
    "      din = dataset.params.shape[-1]\n",
    "      dout = dataset.data.shape[-1]\n",
    "\n",
    "    td = args.get(\"td\", None)\n",
    "    seed = args.get(\"seed\", 0)\n",
    "    device = args.get(\"device\", 0)\n",
    "\n",
    "    recclass = globals()[args.get(\"recclass\", config.recclass)]\n",
    "    recparams = copy.deepcopy(dict(args.get(\"recparams\", config.recparams)))\n",
    "\n",
    "    aeclass = globals()[args.get(\"aeclass\", config.aeclass)]\n",
    "    aeparams = copy.deepcopy(dict(args.get(\"aeparams\", config.aeparams)))\n",
    "\n",
    "    recparams[\"seq\"][0] = k + 1\n",
    "    recparams[\"seq\"][-1] = dout\n",
    "\n",
    "    return ETINet(dataset, k, aeclass, aeparams, recclass, recparams, td=td, seed=seed, device=device)\n",
    "\n",
    "  @staticmethod\n",
    "  def get_operrs(etinet, times=None, testonly=False):\n",
    "    if testonly:\n",
    "      data = etinet.dataset.data[etinet.numtrain:,]\n",
    "    else:\n",
    "      data = etinet.dataset.data\n",
    "\n",
    "    inputs = torch.tensor(data[:, 0]).to(etinet.device)\n",
    "\n",
    "    encode = etinet.aenet.encode(inputs)\n",
    "    errors = etinet.get_errors(encode, data[:, 1:], times=times, aggregate=False)\n",
    "\n",
    "    return errors\n",
    "  \n",
    "  @staticmethod\n",
    "  def plot_op_predicts(etinet, testonly=False, xs=None, cmap=\"viridis\", topdown=True):\n",
    "    if testonly:\n",
    "      data = etinet.dataset.data[etinet.numtrain:,]\n",
    "    else:\n",
    "      data = etinet.dataset.data\n",
    "\n",
    "    if xs == None:\n",
    "      xs = np.linspace(0, 1, len(data[0, 0]))\n",
    "\n",
    "    inputs = etinet.aenet.encode(torch.tensor(data[:, 0]).to(etinet.device))\n",
    "    predicts = etinet.propagate(inputs).cpu().detach().numpy()\n",
    "\n",
    "    errors = []\n",
    "    n = predicts.shape[0]\n",
    "    for s in range(data[:, 1:].shape[1]):\n",
    "      currpredict = predicts[:, s].reshape((n, -1))\n",
    "      currreference = data[:, s].reshape((n, -1))\n",
    "      errors.append(np.mean(np.linalg.norm(currpredict - currreference, axis=1) / np.linalg.norm(currreference, axis=1)))\n",
    "        \n",
    "    print(f\"Average Relative L2 Error over all times: {np.mean(errors):.4f}\")\n",
    "\n",
    "    if len(data.shape) == 3:\n",
    "      if topdown:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "         \n",
    "      else:\n",
    "        fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "    if topdown:\n",
    "      @widgets.interact(i=(0, n-1))\n",
    "      def plot_interact(i=0):\n",
    "        if len(data.shape) == 3:\n",
    "          axes[0].clear()\n",
    "          axes[1].clear()\n",
    "          axes[2].clear()\n",
    "\n",
    "          fig.suptitle(f\"RelL2 {np.linalg.norm(predicts[i, s-1] - data[i, s]) / np.linalg.norm(data[i, s])}\")\n",
    "\n",
    "          axes[0].set_title(\"Exact\")\n",
    "          axes[0].imshow(data[i, 1:], cmap=cmap, origin=\"lower\", aspect='auto')\n",
    "\n",
    "          axes[1].set_title(\"Predict\")\n",
    "          axes[1].imshow(predicts[i, 1:], cmap=cmap, origin=\"lower\", aspect='auto')\n",
    "\n",
    "          axes[2].set_title(\"|Exact - Predict|\")\n",
    "          axes[2].imshow(np.abs(data[i, 1:] - predicts[i]), cmap=cmap, origin=\"lower\", aspect='auto')\n",
    "       \n",
    "    else:\n",
    "      @widgets.interact(i=(0, n-1), s=(1, etinet.T-1))\n",
    "      def plot_interact(i=0, s=1):\n",
    "        print(f\"Avg Relative L2 Error for t0 to t{s}: {errors[s-1]:.4f}\")\n",
    "\n",
    "        if len(data.shape) == 3:\n",
    "          ax.clear()\n",
    "          ax.set_title(f\"RelL2 {np.linalg.norm(predicts[i, s-1] - data[i, s]) / np.linalg.norm(data[i, s])}\")\n",
    "          ax.plot(xs, data[i, 0], label=\"Input\", linewidth=1)\n",
    "          ax.plot(xs, predicts[i, s-1], label=\"Predicted\", linewidth=1)\n",
    "          ax.plot(xs, data[i, s], label=\"Exact\", linewidth=1)\n",
    "          ax.legend()\n",
    "\n",
    "  @staticmethod\n",
    "  def plot_errorparams(etinet, param=-1):\n",
    "    if param == -1:\n",
    "        # Auto-detect one varying parameter\n",
    "        param = 0\n",
    "        P = etinet.dataset.params.shape[1]\n",
    "        for p in range(P):\n",
    "            if np.abs(etinet.dataset.params[0, p] - etinet.dataset.params[1, p]) > 0:\n",
    "                param = p\n",
    "                break\n",
    "\n",
    "    l2error = np.asarray(ETINetHelper.get_operrs(etinet, times=[etinet.T - 1]))\n",
    "    params = etinet.dataset.params\n",
    "\n",
    "    print(params.shape, l2error.shape)\n",
    "\n",
    "    if isinstance(param, (list, tuple, np.ndarray)) and len(param) == 2:\n",
    "        # 3D scatter plot for 2 varying parameters\n",
    "        x = params[:, param[0]]\n",
    "        y = params[:, param[1]]\n",
    "        z = l2error\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        sc = ax.scatter(x, y, z, c=z, cmap='viridis', s=10)\n",
    "\n",
    "        ax.set_xlabel(f\"Param {param[0]}\")\n",
    "        ax.set_ylabel(f\"Param {param[1]}\")\n",
    "        ax.set_zlabel(\"Operator Error\")\n",
    "        fig.colorbar(sc, ax=ax, label=\"Operator Error\")\n",
    "\n",
    "    else:\n",
    "      # Fallback to 2D scatter if param is 1D\n",
    "      fig, ax = plt.subplots()\n",
    "      ax.scatter(params[:, param], l2error, s=2)\n",
    "      ax.set_xlabel(f\"Parameter {param}\")\n",
    "      ax.set_ylabel(\"Operator Error\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "class ETINet():\n",
    "  def __init__(self, dataset, k, aeclass, aeparams, recclass, recparams, td, seed, device):\n",
    "    self.dataset = dataset\n",
    "    self.device = device\n",
    "    self.td = td\n",
    "    self.k = k\n",
    "  \n",
    "    if self.td is None:\n",
    "      self.prefix = f\"{self.dataset.name}{str(recclass.__name__)}ETINet\"\n",
    "    else:\n",
    "      self.prefix = self.td\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    self.seed = seed\n",
    "\n",
    "    datacopy = self.dataset.data.copy()\n",
    "    self.numtrain = int(datacopy.shape[0] * 0.8)\n",
    "    \n",
    "    self.T = self.dataset.data.shape[1]\n",
    "    self.trainarr = datacopy[:self.numtrain]\n",
    "    self.testarr = datacopy[self.numtrain:]\n",
    "    self.optparams = None\n",
    "\n",
    "    self.datadim = len(self.dataset.data.shape) - 2\n",
    "    self.aestep = 0\n",
    "    self.recstep = 0\n",
    "\n",
    "    aeparams[\"encodeSeq\"][0] = self.dataset.data.shape[-1]\n",
    "    aeparams[\"encodeSeq\"][-1] = self.k\n",
    "    aeparams[\"decodeSeq\"][0] = self.k\n",
    "    aeparams[\"decodeSeq\"][-1] = self.dataset.data.shape[-1]\n",
    "\n",
    "    recparams[\"seq\"][0] = self.k + 1\n",
    "    recparams[\"seq\"][-1] = self.dataset.data.shape[-1]\n",
    "\n",
    "    self.aeclass = aeclass\n",
    "    self.aeparams = copy.deepcopy(aeparams)\n",
    "    self.recclass = recclass\n",
    "    self.recparams = copy.deepcopy(recparams)\n",
    "\n",
    "    self.aenet = aeclass(**aeparams).float().to(device)\n",
    "    self.recnet = recclass(**recparams).float().to(device)\n",
    "\n",
    "    self.metadata = {\n",
    "      \"aeclass\": aeclass.__name__,\n",
    "      \"aeparams\": aeparams,\n",
    "      \"recclass\": recclass.__name__,\n",
    "      \"recparams\": recparams,\n",
    "      \"dataset_name\": dataset.name,\n",
    "      \"data_shape\": list(dataset.data.shape),\n",
    "      \"data_checksum\": float(np.sum(dataset.data)),\n",
    "      \"seed\": seed,\n",
    "      \"epochs\": []\n",
    "    }\n",
    "\n",
    "  def reconstruct(self, z, ts):\n",
    "    z_shape = z.shape\n",
    "    *leading_dims, N = z_shape\n",
    "    T = ts.shape[0]\n",
    "\n",
    "    z_expanded = z.unsqueeze(-2).expand(*leading_dims, T, N)\n",
    "\n",
    "    t_shape = [1] * len(leading_dims) + [T, 1]\n",
    "    t_expanded = ts.view(*t_shape).expand(*leading_dims, T, 1)\n",
    "\n",
    "    result = torch.cat([z_expanded, t_expanded], dim=-1)\n",
    "\n",
    "    recon = self.recnet(result)\n",
    "    return recon\n",
    "\n",
    "  def propagate(self, code, start=1, end=-1):\n",
    "    fullts = torch.linspace(0, 1, self.T).float().to(self.device)\n",
    "    \n",
    "    if end > 0:\n",
    "      ts = fullts[start:end+1]\n",
    "    else:\n",
    "      ts = fullts[start:]\n",
    "\n",
    "    out = self.reconstruct(code, ts)\n",
    "    return out\n",
    "\n",
    "  def get_errors(self, testarr, testrest, ords=(2,), times=None, aggregate=True):\n",
    "    assert(aggregate or len(ords) == 1)\n",
    "    \n",
    "    if isinstance(testarr, np.ndarray):\n",
    "      testarr = torch.tensor(testarr, dtype=torch.float32)\n",
    "\n",
    "    if isinstance(testrest, np.ndarray):\n",
    "      testrest = torch.tensor(testrest, dtype=torch.float32)\n",
    "\n",
    "    if times is None:\n",
    "      times = range(self.T-1)\n",
    "  \n",
    "    out = self.propagate(testarr)\n",
    "\n",
    "    n = testarr.shape[0]\n",
    "    orig = testrest.cpu().detach().numpy()\n",
    "    out = out.cpu().detach().numpy()\n",
    "\n",
    "    print(orig.shape, out.shape)\n",
    "\n",
    "    if aggregate:\n",
    "      orig = orig.reshape([n, -1])\n",
    "      out = out.reshape([n, -1])\n",
    "      testerrs = []\n",
    "      for o in ords:\n",
    "        testerrs.append(np.mean(np.linalg.norm(orig - out, axis=1, ord=o) / np.linalg.norm(orig, axis=1, ord=o)))\n",
    "\n",
    "      return tuple(testerrs)\n",
    "    \n",
    "    else:\n",
    "      o = ords[0]\n",
    "      testerrs = []\n",
    "\n",
    "      if len(times) == 1:\n",
    "        t = times[0]\n",
    "        origslice = orig[:, t-1].reshape([n, -1])\n",
    "        outslice = out[:, t-1].reshape([n, -1])\n",
    "        return np.linalg.norm(origslice - outslice, axis=1, ord=o) / np.linalg.norm(origslice, axis=1, ord=o)\n",
    "      else:\n",
    "        for t in range(orig.shape[1]):\n",
    "          origslice = orig[:, t].reshape([n, -1])\n",
    "          outslice = out[:, t].reshape([n, -1])\n",
    "          testerrs.append(np.mean(np.linalg.norm(origslice - outslice, axis=1, ord=o) / np.linalg.norm(origslice, axis=1, ord=o)))\n",
    "\n",
    "        return testerrs\n",
    "      \n",
    "  def get_ae_errors(self, testarr, ords=(2,)):\n",
    "    if isinstance(testarr, np.ndarray):\n",
    "      testarr = torch.tensor(testarr, dtype=torch.float32)\n",
    "  \n",
    "    out = self.aenet(testarr).cpu().detach().numpy()\n",
    "    orig = testarr.cpu().detach().numpy()\n",
    "\n",
    "    testerrs = []\n",
    "    for o in ords:\n",
    "      testerrs.append(np.mean(np.linalg.norm(orig - out, axis=1, ord=o) / np.linalg.norm(orig, axis=1, ord=o)))\n",
    "\n",
    "    return tuple(testerrs)\n",
    "\n",
    "  def load_models(self, filename_prefix, verbose=False, min_epochs=0):\n",
    "    search_path = f\"savedmodels/etinet/{filename_prefix}*.pickle\"\n",
    "    matching_files = glob.glob(search_path)\n",
    "\n",
    "    print(\"Searching for model files matching prefix:\", filename_prefix)\n",
    "    if not hasattr(self, \"metadata\"):\n",
    "        raise ValueError(\"Missing self.metadata. Cannot match models without metadata. Ensure model has been initialized with same config.\")\n",
    "\n",
    "    for addr in matching_files:\n",
    "      try:\n",
    "          with open(addr, \"rb\") as handle:\n",
    "              dic = pickle.load(handle)\n",
    "      except Exception as e:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to read error: {e}\")\n",
    "          continue\n",
    "\n",
    "      meta = dic.get(\"metadata\", {})\n",
    "      is_match = all(\n",
    "          meta.get(k) == self.metadata.get(k)\n",
    "          for k in self.metadata.keys()\n",
    "      )\n",
    "\n",
    "      # Check if model meets the minimum epoch requirement\n",
    "      model_epochs = meta.get(\"epochs\")\n",
    "      if model_epochs is None:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to missing epoch metadata.\")\n",
    "          continue\n",
    "      elif isinstance(model_epochs, list):  # handle legacy or list format\n",
    "          if sum(model_epochs) < min_epochs:\n",
    "              if verbose:\n",
    "                  print(f\"Skipping {addr} due to insufficient epochs ({sum(model_epochs)} < {min_epochs})\")\n",
    "              continue\n",
    "      elif model_epochs < min_epochs:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to insufficient epochs ({model_epochs} < {min_epochs})\")\n",
    "          continue\n",
    "\n",
    "      if is_match:\n",
    "          print(\"Model match found. Loading from:\", addr)\n",
    "          self.recnet.load_state_dict(dic[\"recnet\"])\n",
    "          self.aenet.load_state_dict(dic[\"aenet\"])\n",
    "          self.metadata[\"epochs\"] = meta.get(\"epochs\")\n",
    "          if \"opt\" in dic:     \n",
    "            self.optparams = dic[\"opt\"]\n",
    "\n",
    "          return True\n",
    "      elif verbose:\n",
    "          print(\"Metadata mismatch in file:\", addr)\n",
    "          for k in self.metadata:\n",
    "              print(f\"{k}: saved={meta.get(k)} vs current={self.metadata.get(k)}\")\n",
    "\n",
    "    print(\"Load failed. No matching models found.\")\n",
    "    print(\"Searched:\", matching_files)\n",
    "    return False\n",
    "\n",
    "  def train_recnet(self, epochs, save=True, optim=torch.optim.AdamW, lr=1e-4, printinterval=10, batch=32, ridge=0, loss=None, best=True, verbose=False):\n",
    "    def recnet_epoch(dataloader, writer=None, optimizer=None, scheduler=None, ep=0, printinterval=10, loss=None, testarr=None, testrest=None):\n",
    "      losses = []\n",
    "      testerrors1 = []\n",
    "      testerrors2 = []\n",
    "      testerrorsinf = []\n",
    "\n",
    "      def closure(codes, rest):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = self.propagate(codes)\n",
    "        target = rest\n",
    "        \n",
    "        res = loss(out, target)\n",
    "        res.backward()\n",
    "        \n",
    "        if writer is not None and self.recstep % 5 == 0:\n",
    "          writer.add_scalar(\"main/loss\", res, global_step=self.recstep)\n",
    "\n",
    "        return res\n",
    "\n",
    "      for codes, rest in dataloader:\n",
    "        self.recstep += 1\n",
    "        error = optimizer.step(lambda: closure(codes, rest))\n",
    "        losses.append(float(error.cpu().detach()))\n",
    "\n",
    "      if scheduler is not None:\n",
    "        scheduler.step(np.mean(losses))\n",
    "\n",
    "      # print test\n",
    "      if printinterval > 0 and (ep % printinterval == 0):\n",
    "        testerr1, testerr2, testerrinf = self.get_errors(testarr, testrest, ords=(1, 2, np.inf))\n",
    "        if scheduler is not None:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, LR {scheduler.get_last_lr()[-1]:.3e}, Relative ETINet Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "        else:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, Relative ETINet Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"misc/relativeL1error\", testerr1, global_step=ep)\n",
    "            writer.add_scalar(\"main/relativeL2error\", testerr2, global_step=ep)\n",
    "            writer.add_scalar(\"misc/relativeLInferror\", testerrinf, global_step=ep)\n",
    "\n",
    "      return losses, testerrors1, testerrors2, testerrorsinf\n",
    "  \n",
    "    assert(self.aestep > 0)\n",
    "\n",
    "    loss = nn.MSELoss() if loss is None else loss()\n",
    "\n",
    "    losses, testerrors1, testerrors2, testerrorsinf = [], [], [], []\n",
    "\n",
    "    initial = torch.tensor(self.trainarr[:, 0], dtype=torch.float32).to(self.device)\n",
    "    rest = torch.tensor(self.trainarr[:, 1:], dtype=torch.float32).to(self.device)\n",
    "    train = self.aenet.encode(initial).detach()\n",
    "\n",
    "    testinitial = torch.tensor(self.testarr[:, 0], dtype=torch.float32).to(self.device)\n",
    "    testrest = torch.tensor(self.testarr[:, 1:], dtype=torch.float32).to(self.device)\n",
    "    test = self.aenet.encode(testinitial).detach()\n",
    "\n",
    "    opt = optim(self.recnet.parameters(), lr=lr, weight_decay=ridge)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(opt, patience=30)\n",
    "    dataloader = DataLoader(torch.utils.data.TensorDataset(train, rest), shuffle=False, batch_size=batch)\n",
    "\n",
    "    writer = None\n",
    "    if self.td is not None:\n",
    "      name = f\"./tensorboard/{datetime.datetime.now().strftime('%d-%B-%Y')}/{self.td}/{datetime.datetime.now().strftime('%H.%M.%S')}/\"\n",
    "      writer = torch.utils.tensorboard.SummaryWriter(name)\n",
    "      print(\"Tensorboard writer location is \" + name)\n",
    "\n",
    "    print(\"Number of NN trainable parameters\", utils.num_params(self.recnet))\n",
    "    print(f\"Starting ETINet rec model at {time.asctime()}...\")\n",
    "    print(\"train\", train.shape, \"test\", test.shape)\n",
    "      \n",
    "    bestdict = { \"loss\": float(np.inf), \"ep\": 0 }\n",
    "    for ep in range(epochs):\n",
    "      lossesN, testerrors1N, testerrors2N, testerrorsinfN = recnet_epoch(dataloader, optimizer=opt, scheduler=scheduler, writer=writer, ep=ep, printinterval=printinterval, loss=loss, testarr=test, testrest=testrest)\n",
    "      losses += lossesN; testerrors1 += testerrors1N; testerrors2 += testerrors2N; testerrorsinf += testerrorsinfN\n",
    "\n",
    "      if best and ep > epochs // 2:\n",
    "        avgloss = np.mean(lossesN)\n",
    "        if avgloss < bestdict[\"loss\"]:\n",
    "          bestdict[\"recnet\"] = self.recnet.state_dict()\n",
    "          bestdict[\"opt\"] = opt.state_dict()\n",
    "          bestdict[\"loss\"] = avgloss\n",
    "          bestdict[\"ep\"] = ep\n",
    "        elif verbose:\n",
    "          print(f\"Loss not improved at epoch {ep} (Ratio: {avgloss/bestdict['loss']:.2f}) from {bestdict['ep']} (Loss: {bestdict['loss']:.2e})\")\n",
    "      \n",
    "    print(f\"Finished training ETINet rec model at {time.asctime()}...\")\n",
    "\n",
    "    if best:\n",
    "      self.recnet.load_state_dict(bestdict[\"recnet\"])\n",
    "      opt.load_state_dict(bestdict[\"opt\"])\n",
    "\n",
    "    self.aeoptparams = opt.state_dict()\n",
    "    self.metadata[\"epochs\"].append(epochs)\n",
    "\n",
    "    if save:\n",
    "      now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "      # Compute total training epochs\n",
    "      total_epochs = sum(self.metadata[\"epochs\"]) if isinstance(self.metadata[\"epochs\"], list) else self.metadata[\"epochs\"]\n",
    "\n",
    "      filename = (\n",
    "          f\"{self.dataset.name}_\"\n",
    "          f\"{self.aeclass.__name__}_\"\n",
    "          f\"{self.aeparams['encodeSeq']}_\"\n",
    "          f\"{self.recclass.__name__}_\"\n",
    "          f\"{self.recparams['seq']}_\"\n",
    "          f\"{self.seed}_\"\n",
    "          f\"{total_epochs}ep_\"\n",
    "          f\"{now}.pickle\"\n",
    "      )\n",
    "\n",
    "      dire = \"savedmodels/etinet\"\n",
    "      addr = os.path.join(dire, filename)\n",
    "\n",
    "      if not os.path.exists(dire):\n",
    "          os.makedirs(dire)\n",
    "\n",
    "      with open(addr, \"wb\") as handle:\n",
    "          pickle.dump({\n",
    "              \"aenet\": self.aenet.state_dict(),\n",
    "              \"recnet\": self.recnet.state_dict(),\n",
    "              \"metadata\": self.metadata,\n",
    "              \"opt\": self.optparams\n",
    "          }, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "      print(\"Model saved at\", addr)\n",
    "\n",
    "    return { \"losses\": losses, \"testerrors1\": testerrors1, \"testerrors2\": testerrors2, \"testerrorsinf\": testerrorsinf }\n",
    "\n",
    "  def train_aenet(self, epochs, save=True, optim=torch.optim.AdamW, lr=1e-4, printinterval=10, batch=32, ridge=0, loss=None, best=True, verbose=False):\n",
    "    def aenet_epoch(dataloader, writer=None, optimizer=None, scheduler=None, ep=0, printinterval=10, loss=None, testarr=None):\n",
    "      losses = []\n",
    "      testerrors1 = []\n",
    "      testerrors2 = []\n",
    "      testerrorsinf = []\n",
    "\n",
    "      def closure(codes):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = self.aenet(codes)\n",
    "        target = codes\n",
    "        \n",
    "        res = loss(out, target)\n",
    "        res.backward()\n",
    "        \n",
    "        if writer is not None and self.recstep % 5 == 0:\n",
    "          writer.add_scalar(\"main/aeloss\", res, global_step=self.recstep)\n",
    "\n",
    "        return res\n",
    "\n",
    "      for codes in dataloader:\n",
    "        self.aestep += 1\n",
    "        error = optimizer.step(lambda: closure(codes))\n",
    "        losses.append(float(error.cpu().detach()))\n",
    "\n",
    "      if scheduler is not None:\n",
    "        scheduler.step(np.mean(losses))\n",
    "\n",
    "      # print test\n",
    "      if printinterval > 0 and (ep % printinterval == 0):\n",
    "        testerr1, testerr2, testerrinf = self.get_ae_errors(testarr, ords=(1, 2, np.inf))\n",
    "        if scheduler is not None:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, LR {scheduler.get_last_lr()[-1]:.3e}, Relative ETINet AE Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "        else:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, Relative ETINet AE Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"misc/relativeL1error\", testerr1, global_step=ep)\n",
    "            writer.add_scalar(\"main/relativeL2error\", testerr2, global_step=ep)\n",
    "            writer.add_scalar(\"misc/relativeLInferror\", testerrinf, global_step=ep)\n",
    "\n",
    "      return losses, testerrors1, testerrors2, testerrorsinf\n",
    "  \n",
    "    loss = nn.MSELoss() if loss is None else loss()\n",
    "\n",
    "    losses, testerrors1, testerrors2, testerrorsinf = [], [], [], []\n",
    "\n",
    "    initial = torch.tensor(self.trainarr[:, 0], dtype=torch.float32).to(self.device)\n",
    "    test = torch.tensor(self.testarr[:, 0], dtype=torch.float32).to(self.device)\n",
    "\n",
    "    opt = optim(self.aenet.parameters(), lr=lr, weight_decay=ridge)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(opt, patience=30)\n",
    "    dataloader = DataLoader(initial, shuffle=False, batch_size=batch)\n",
    "\n",
    "    if self.optparams is not None:\n",
    "      opt.load_state_dict(self.optparams)\n",
    "\n",
    "    writer = None\n",
    "    if self.td is not None:\n",
    "      name = f\"./tensorboard/{datetime.datetime.now().strftime('%d-%B-%Y')}/{self.td}/{datetime.datetime.now().strftime('%H.%M.%S')}/\"\n",
    "      writer = torch.utils.tensorboard.SummaryWriter(name)\n",
    "      print(\"Tensorboard writer location is \" + name)\n",
    "\n",
    "    print(\"Number of NN trainable parameters\", utils.num_params(self.recnet))\n",
    "    print(f\"Starting training ETINet AE model at {time.asctime()}...\")\n",
    "    print(\"train\", initial.shape, \"test\", test.shape)\n",
    "      \n",
    "    bestdict = { \"loss\": float(np.inf), \"ep\": 0 }\n",
    "    for ep in range(epochs):\n",
    "      lossesN, testerrors1N, testerrors2N, testerrorsinfN = aenet_epoch(dataloader, optimizer=opt, scheduler=scheduler, writer=writer, ep=ep, printinterval=printinterval, loss=loss, testarr=test)\n",
    "      losses += lossesN; testerrors1 += testerrors1N; testerrors2 += testerrors2N; testerrorsinf += testerrorsinfN\n",
    "\n",
    "      if best and ep > epochs // 2:\n",
    "        avgloss = np.mean(lossesN)\n",
    "        if avgloss < bestdict[\"loss\"]:\n",
    "          bestdict[\"aenet\"] = self.aenet.state_dict()\n",
    "          bestdict[\"opt\"] = opt.state_dict()\n",
    "          bestdict[\"loss\"] = avgloss\n",
    "          bestdict[\"ep\"] = ep\n",
    "        elif verbose:\n",
    "          print(f\"Loss not improved at epoch {ep} (Ratio: {avgloss/bestdict['loss']:.2f}) from {bestdict['ep']} (Loss: {bestdict['loss']:.2e})\")\n",
    "      \n",
    "    print(f\"Finished training ETINet AE model at {time.asctime()}...\")\n",
    "\n",
    "    if best:\n",
    "      self.aenet.load_state_dict(bestdict[\"aenet\"])\n",
    "      opt.load_state_dict(bestdict[\"opt\"])\n",
    "\n",
    "    return { \"losses\": losses, \"testerrors1\": testerrors1, \"testerrors2\": testerrors2, \"testerrorsinf\": testerrorsinf }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e5d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\"etinet\", \"ltinet\", \"ldnet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2dd218a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NN trainable parameters 909301\n",
      "Starting training ETINet AE model at Mon Jul 21 10:41:51 2025...\n",
      "train torch.Size([200, 301]) test torch.Size([50, 301])\n",
      "1: Train Loss 2.187e-03, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.112253, 0.136846, 0.386307\n",
      "11: Train Loss 6.491e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.053248, 0.064079, 0.118436\n",
      "21: Train Loss 6.646e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.052937, 0.063736, 0.113986\n",
      "31: Train Loss 6.775e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.052982, 0.063756, 0.116930\n",
      "41: Train Loss 8.687e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.048085, 0.058123, 0.117756\n",
      "51: Train Loss 2.017e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.027273, 0.034397, 0.073421\n",
      "61: Train Loss 1.796e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.026920, 0.034455, 0.071044\n",
      "71: Train Loss 1.899e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.029123, 0.036854, 0.072781\n",
      "81: Train Loss 1.831e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.027167, 0.034371, 0.072105\n",
      "91: Train Loss 1.546e-04, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.025036, 0.030979, 0.072428\n",
      "101: Train Loss 4.624e-05, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.029663, 0.037947, 0.085912\n",
      "111: Train Loss 3.146e-06, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.003345, 0.004088, 0.009977\n",
      "121: Train Loss 7.686e-07, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.002004, 0.002461, 0.006232\n",
      "131: Train Loss 7.264e-07, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.001593, 0.001955, 0.005189\n",
      "141: Train Loss 4.790e-07, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.001274, 0.001572, 0.004246\n",
      "151: Train Loss 3.062e-07, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.001144, 0.001412, 0.003806\n",
      "161: Train Loss 1.996e-07, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.001074, 0.001327, 0.003559\n",
      "171: Train Loss 1.529e-07, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.001041, 0.001286, 0.003414\n",
      "181: Train Loss 1.610e-07, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.001377, 0.001701, 0.004274\n",
      "191: Train Loss 5.302e-06, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.003622, 0.004342, 0.009195\n",
      "201: Train Loss 4.427e-05, LR 1.000e-03, Relative ETINet AE Error (1, 2, inf): 0.021653, 0.023050, 0.033053\n",
      "211: Train Loss 2.983e-07, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.001359, 0.001664, 0.004123\n",
      "221: Train Loss 1.252e-07, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.001039, 0.001287, 0.003393\n",
      "231: Train Loss 9.822e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000974, 0.001208, 0.003214\n",
      "241: Train Loss 8.684e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000935, 0.001161, 0.003114\n",
      "251: Train Loss 8.156e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000908, 0.001129, 0.003049\n",
      "261: Train Loss 7.889e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000888, 0.001104, 0.002998\n",
      "271: Train Loss 7.726e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000872, 0.001085, 0.002955\n",
      "281: Train Loss 7.624e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000859, 0.001069, 0.002915\n",
      "291: Train Loss 7.533e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000846, 0.001054, 0.002876\n",
      "301: Train Loss 7.443e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000835, 0.001041, 0.002844\n",
      "311: Train Loss 7.354e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000826, 0.001029, 0.002815\n",
      "321: Train Loss 7.259e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000818, 0.001019, 0.002790\n",
      "331: Train Loss 7.169e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000809, 0.001009, 0.002766\n",
      "341: Train Loss 7.075e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000802, 0.001000, 0.002744\n",
      "351: Train Loss 6.965e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000796, 0.000992, 0.002725\n",
      "361: Train Loss 6.848e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000790, 0.000985, 0.002712\n",
      "371: Train Loss 6.725e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000787, 0.000982, 0.002707\n",
      "381: Train Loss 6.588e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000788, 0.000982, 0.002715\n",
      "391: Train Loss 6.414e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000795, 0.000992, 0.002744\n",
      "401: Train Loss 6.197e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000814, 0.001015, 0.002809\n",
      "411: Train Loss 5.978e-08, LR 1.000e-04, Relative ETINet AE Error (1, 2, inf): 0.000849, 0.001060, 0.002925\n",
      "421: Train Loss 5.856e-08, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.000740, 0.000922, 0.002522\n",
      "431: Train Loss 5.849e-08, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.000738, 0.000920, 0.002516\n",
      "441: Train Loss 5.844e-08, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.000737, 0.000919, 0.002513\n",
      "451: Train Loss 5.838e-08, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.000736, 0.000918, 0.002510\n",
      "461: Train Loss 5.830e-08, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.000735, 0.000917, 0.002508\n",
      "471: Train Loss 5.821e-08, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.000734, 0.000915, 0.002505\n",
      "481: Train Loss 5.812e-08, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.000733, 0.000914, 0.002502\n",
      "491: Train Loss 5.801e-08, LR 1.000e-05, Relative ETINet AE Error (1, 2, inf): 0.000732, 0.000913, 0.002499\n",
      "Finished training ETINet AE model at Mon Jul 21 10:42:07 2025...\n",
      "Number of NN trainable parameters 909301\n",
      "Starting ETINet rec model at Mon Jul 21 10:42:07 2025...\n",
      "train torch.Size([200, 10]) test torch.Size([50, 10])\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "1: Train Loss 1.591e-02, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.221903, 0.300075, 0.562100\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "11: Train Loss 9.462e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.193645, 0.281723, 0.583804\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "21: Train Loss 8.451e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.174607, 0.256795, 0.537281\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "31: Train Loss 6.617e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.152840, 0.220764, 0.505608\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "41: Train Loss 5.356e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.130732, 0.190277, 0.468793\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "51: Train Loss 4.224e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.120577, 0.175559, 0.456077\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "61: Train Loss 3.486e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.112469, 0.164230, 0.441512\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "71: Train Loss 3.032e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.106601, 0.154901, 0.426165\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "81: Train Loss 2.642e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.099523, 0.145745, 0.413599\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "91: Train Loss 2.349e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.096203, 0.139209, 0.398584\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "101: Train Loss 2.013e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.093998, 0.134410, 0.390038\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "111: Train Loss 1.769e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.086121, 0.124949, 0.379652\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "121: Train Loss 1.672e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.083608, 0.119877, 0.365634\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "131: Train Loss 1.398e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.077732, 0.113437, 0.354929\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "141: Train Loss 1.250e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.074577, 0.109295, 0.348512\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "151: Train Loss 1.127e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.070982, 0.104596, 0.340369\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "161: Train Loss 9.960e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.069265, 0.101018, 0.327716\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "171: Train Loss 1.026e-03, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.070180, 0.099556, 0.316240\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "181: Train Loss 8.093e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.065125, 0.094388, 0.306681\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "191: Train Loss 7.907e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.063356, 0.092552, 0.305906\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "201: Train Loss 6.657e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.062930, 0.090240, 0.296150\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "211: Train Loss 6.235e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.058421, 0.084039, 0.282728\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "221: Train Loss 6.124e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.061886, 0.086733, 0.279969\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "231: Train Loss 5.340e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.059947, 0.083446, 0.272192\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "241: Train Loss 5.292e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.062036, 0.084624, 0.267697\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "251: Train Loss 4.858e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.057967, 0.081254, 0.266056\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "261: Train Loss 4.234e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.057172, 0.078683, 0.256525\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "271: Train Loss 3.867e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.056557, 0.077305, 0.257918\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "281: Train Loss 5.173e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.056280, 0.077529, 0.245732\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "291: Train Loss 3.526e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.046287, 0.068807, 0.242470\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "301: Train Loss 3.413e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.048342, 0.069344, 0.239622\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "311: Train Loss 3.428e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.050026, 0.071001, 0.243365\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "321: Train Loss 3.033e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.046606, 0.068630, 0.240784\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "331: Train Loss 2.914e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.051707, 0.072071, 0.238132\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "341: Train Loss 2.885e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.051983, 0.072936, 0.243914\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "351: Train Loss 2.894e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.045895, 0.065796, 0.233600\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "361: Train Loss 2.932e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.048020, 0.067955, 0.233565\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "371: Train Loss 2.859e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.047974, 0.065751, 0.222264\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "381: Train Loss 5.104e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.058181, 0.078692, 0.248683\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "391: Train Loss 2.727e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.046684, 0.066290, 0.220762\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "401: Train Loss 2.452e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.048713, 0.069197, 0.226606\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "411: Train Loss 2.432e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.045247, 0.064538, 0.224332\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "421: Train Loss 2.266e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.042633, 0.061012, 0.212811\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "431: Train Loss 2.048e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.042673, 0.060901, 0.212601\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "441: Train Loss 2.068e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.043325, 0.060891, 0.206445\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "451: Train Loss 1.947e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.042855, 0.061167, 0.213264\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "461: Train Loss 2.103e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.039926, 0.057157, 0.205756\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "471: Train Loss 1.944e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.043859, 0.062776, 0.221619\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "481: Train Loss 1.623e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.036067, 0.054020, 0.210540\n",
      "(50, 100, 301) (50, 100, 301)\n",
      "491: Train Loss 1.597e-04, LR 1.000e-03, Relative ETINet Error (1, 2, inf): 0.037007, 0.055061, 0.210168\n",
      "Finished training ETINet rec model at Mon Jul 21 10:42:32 2025...\n",
      "Model saved at savedmodels/etinet\\dtrigpoly1_FFAutoencoder_[301, 600, 600, 600, 10]_FFNet_[11, 600, 600, 600, 301]_0_500ep_2025-07-21_10-42-32.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'losses': [0.10148518532514572,\n",
       "  0.08618798106908798,\n",
       "  0.06175689399242401,\n",
       "  0.030077064409852028,\n",
       "  0.02598160319030285,\n",
       "  0.030582120642066002,\n",
       "  0.01590675488114357,\n",
       "  0.010357859544456005,\n",
       "  0.013937491923570633,\n",
       "  0.0176104549318552,\n",
       "  0.018090389668941498,\n",
       "  0.017687326297163963,\n",
       "  0.014555457979440689,\n",
       "  0.013835892081260681,\n",
       "  0.014232377521693707,\n",
       "  0.01554053369909525,\n",
       "  0.014859921298921108,\n",
       "  0.011170512065291405,\n",
       "  0.011355932801961899,\n",
       "  0.010686914436519146,\n",
       "  0.011907569132745266,\n",
       "  0.011434457264840603,\n",
       "  0.011543471366167068,\n",
       "  0.011633199639618397,\n",
       "  0.010943735018372536,\n",
       "  0.012295125983655453,\n",
       "  0.010849087499082088,\n",
       "  0.010548949241638184,\n",
       "  0.010069601237773895,\n",
       "  0.010815968737006187,\n",
       "  0.011266159825026989,\n",
       "  0.010436946526169777,\n",
       "  0.011100124567747116,\n",
       "  0.009605739265680313,\n",
       "  0.009911504574120045,\n",
       "  0.009968812577426434,\n",
       "  0.010775329545140266,\n",
       "  0.010921524837613106,\n",
       "  0.009877667762339115,\n",
       "  0.010793427936732769,\n",
       "  0.009546171873807907,\n",
       "  0.009828509762883186,\n",
       "  0.009653775952756405,\n",
       "  0.010355974547564983,\n",
       "  0.010675468482077122,\n",
       "  0.009795193560421467,\n",
       "  0.010754602029919624,\n",
       "  0.009381699375808239,\n",
       "  0.009631229564547539,\n",
       "  0.009525968693196774,\n",
       "  0.010333569720387459,\n",
       "  0.010623211041092873,\n",
       "  0.009699283167719841,\n",
       "  0.01059680338948965,\n",
       "  0.009292364120483398,\n",
       "  0.009561501443386078,\n",
       "  0.009471660479903221,\n",
       "  0.010248337872326374,\n",
       "  0.010539278388023376,\n",
       "  0.009653708897531033,\n",
       "  0.010563676245510578,\n",
       "  0.009251058101654053,\n",
       "  0.009514953009784222,\n",
       "  0.009416886605322361,\n",
       "  0.010203137062489986,\n",
       "  0.010509960353374481,\n",
       "  0.009588319808244705,\n",
       "  0.010506837628781796,\n",
       "  0.009216048754751682,\n",
       "  0.009513401426374912,\n",
       "  0.009364820085465908,\n",
       "  0.010138288140296936,\n",
       "  0.010442504659295082,\n",
       "  0.009544831700623035,\n",
       "  0.01045070681720972,\n",
       "  0.009143877774477005,\n",
       "  0.009462296031415462,\n",
       "  0.009314273484051228,\n",
       "  0.010076943784952164,\n",
       "  0.010344529524445534,\n",
       "  0.009467191062867641,\n",
       "  0.010381362400949001,\n",
       "  0.009073306806385517,\n",
       "  0.009421336464583874,\n",
       "  0.00923437625169754,\n",
       "  0.009991363622248173,\n",
       "  0.01023365743458271,\n",
       "  0.009364396333694458,\n",
       "  0.010273417457938194,\n",
       "  0.008986777625977993,\n",
       "  0.009389342740178108,\n",
       "  0.009140683338046074,\n",
       "  0.00988849624991417,\n",
       "  0.010097060352563858,\n",
       "  0.00925679225474596,\n",
       "  0.01016081590205431,\n",
       "  0.008894947357475758,\n",
       "  0.00935059692710638,\n",
       "  0.009050549007952213,\n",
       "  0.009793042205274105,\n",
       "  0.009954018518328667,\n",
       "  0.009146083146333694,\n",
       "  0.010052447207272053,\n",
       "  0.008797447197139263,\n",
       "  0.009289625100791454,\n",
       "  0.008946099318563938,\n",
       "  0.009679814800620079,\n",
       "  0.009817605838179588,\n",
       "  0.009017878212034702,\n",
       "  0.009926684200763702,\n",
       "  0.00869300402700901,\n",
       "  0.00921413954347372,\n",
       "  0.00881013460457325,\n",
       "  0.009521594271063805,\n",
       "  0.009646588005125523,\n",
       "  0.00886588916182518,\n",
       "  0.0097592081874609,\n",
       "  0.008532412350177765,\n",
       "  0.009106107987463474,\n",
       "  0.008632262237370014,\n",
       "  0.00931019987910986,\n",
       "  0.009392890147864819,\n",
       "  0.008659478276968002,\n",
       "  0.009534673765301704,\n",
       "  0.00833611749112606,\n",
       "  0.008946238085627556,\n",
       "  0.008380426093935966,\n",
       "  0.009007899090647697,\n",
       "  0.009074083529412746,\n",
       "  0.008384265005588531,\n",
       "  0.009271244518458843,\n",
       "  0.008113949559628963,\n",
       "  0.008768102154135704,\n",
       "  0.008110635913908482,\n",
       "  0.008671790361404419,\n",
       "  0.00876760296523571,\n",
       "  0.008102470077574253,\n",
       "  0.009001226164400578,\n",
       "  0.00792439840734005,\n",
       "  0.008619340136647224,\n",
       "  0.007866386324167252,\n",
       "  0.008366699330508709,\n",
       "  0.008484864607453346,\n",
       "  0.007831369526684284,\n",
       "  0.008741337805986404,\n",
       "  0.007728151511400938,\n",
       "  0.008450772613286972,\n",
       "  0.00765694584697485,\n",
       "  0.008063794113695621,\n",
       "  0.008199386298656464,\n",
       "  0.007564504165202379,\n",
       "  0.008468441665172577,\n",
       "  0.007513015065342188,\n",
       "  0.008252840489149094,\n",
       "  0.007441980764269829,\n",
       "  0.007785255089402199,\n",
       "  0.00792054645717144,\n",
       "  0.007296049501746893,\n",
       "  0.00817873328924179,\n",
       "  0.007300811354070902,\n",
       "  0.00804194901138544,\n",
       "  0.00721294991672039,\n",
       "  0.007510128431022167,\n",
       "  0.0076228417456150055,\n",
       "  0.007022815756499767,\n",
       "  0.007884455844759941,\n",
       "  0.007083399221301079,\n",
       "  0.007841936312615871,\n",
       "  0.006988127715885639,\n",
       "  0.007246107328683138,\n",
       "  0.007342569530010223,\n",
       "  0.0067820982076227665,\n",
       "  0.007619782350957394,\n",
       "  0.00690110307186842,\n",
       "  0.007653942331671715,\n",
       "  0.006813134998083115,\n",
       "  0.0069998931139707565,\n",
       "  0.007093703839927912,\n",
       "  0.006559947971254587,\n",
       "  0.007389333564788103,\n",
       "  0.006720996927469969,\n",
       "  0.007459825370460749,\n",
       "  0.006653213407844305,\n",
       "  0.00675363652408123,\n",
       "  0.006858231499791145,\n",
       "  0.006341333035379648,\n",
       "  0.007150331046432257,\n",
       "  0.00652265315875411,\n",
       "  0.007283552084118128,\n",
       "  0.006473004352301359,\n",
       "  0.006499762646853924,\n",
       "  0.006602485664188862,\n",
       "  0.006111831869930029,\n",
       "  0.006902278866618872,\n",
       "  0.006323872599750757,\n",
       "  0.0071029057726264,\n",
       "  0.006295797415077686,\n",
       "  0.006266143638640642,\n",
       "  0.006345904897898436,\n",
       "  0.00587140629068017,\n",
       "  0.006665936205536127,\n",
       "  0.006130728404968977,\n",
       "  0.006933145225048065,\n",
       "  0.0061243209056556225,\n",
       "  0.006055799312889576,\n",
       "  0.00609199283644557,\n",
       "  0.005631363019347191,\n",
       "  0.006442368496209383,\n",
       "  0.005945221520960331,\n",
       "  0.0067778355441987514,\n",
       "  0.0059437318705022335,\n",
       "  0.005872573237866163,\n",
       "  0.005864108446985483,\n",
       "  0.005386007484048605,\n",
       "  0.006232422310858965,\n",
       "  0.005799213889986277,\n",
       "  0.006617147475481033,\n",
       "  0.005751387216150761,\n",
       "  0.005688105244189501,\n",
       "  0.005661533679813147,\n",
       "  0.00515136169269681,\n",
       "  0.00601013470441103,\n",
       "  0.005641703959554434,\n",
       "  0.006482359953224659,\n",
       "  0.005556901916861534,\n",
       "  0.005484739784151316,\n",
       "  0.00547074805945158,\n",
       "  0.004937704186886549,\n",
       "  0.0057964320294559,\n",
       "  0.005486123729497194,\n",
       "  0.006346103269606829,\n",
       "  0.0053782076574862,\n",
       "  0.0052713328041136265,\n",
       "  0.005285235121846199,\n",
       "  0.004749244544655085,\n",
       "  0.0055854059755802155,\n",
       "  0.005314900539815426,\n",
       "  0.006222069729119539,\n",
       "  0.005198096390813589,\n",
       "  0.005057061091065407,\n",
       "  0.005089232698082924,\n",
       "  0.004584528040140867,\n",
       "  0.005365761462599039,\n",
       "  0.005146898794919252,\n",
       "  0.006088217254728079,\n",
       "  0.005025010090321302,\n",
       "  0.004847320262342691,\n",
       "  0.004897749051451683,\n",
       "  0.0044447206892073154,\n",
       "  0.005155405029654503,\n",
       "  0.004984016064554453,\n",
       "  0.00594322057440877,\n",
       "  0.004887847695499659,\n",
       "  0.004659833386540413,\n",
       "  0.004726017359644175,\n",
       "  0.004301012028008699,\n",
       "  0.004988898988813162,\n",
       "  0.004841881338506937,\n",
       "  0.0058165984228253365,\n",
       "  0.004771077539771795,\n",
       "  0.004504117649048567,\n",
       "  0.004585805349051952,\n",
       "  0.004177043680101633,\n",
       "  0.004854103084653616,\n",
       "  0.00471788365393877,\n",
       "  0.005704766605049372,\n",
       "  0.004671536851674318,\n",
       "  0.004366036504507065,\n",
       "  0.004457196686416864,\n",
       "  0.004068793263286352,\n",
       "  0.004737593699246645,\n",
       "  0.004597682040184736,\n",
       "  0.00559299997985363,\n",
       "  0.004570414312183857,\n",
       "  0.00424042996019125,\n",
       "  0.004332303535193205,\n",
       "  0.003976576495915651,\n",
       "  0.004624557215720415,\n",
       "  0.004483906552195549,\n",
       "  0.005475326906889677,\n",
       "  0.004473027307540178,\n",
       "  0.004124406725168228,\n",
       "  0.004215695429593325,\n",
       "  0.003891432425007224,\n",
       "  0.00451721390709281,\n",
       "  0.004378718324005604,\n",
       "  0.005355506204068661,\n",
       "  0.004386202897876501,\n",
       "  0.004015880636870861,\n",
       "  0.00411234050989151,\n",
       "  0.0038146316073834896,\n",
       "  0.004419995471835136,\n",
       "  0.004280281253159046,\n",
       "  0.005242049694061279,\n",
       "  0.00430748937651515,\n",
       "  0.003917299676686525,\n",
       "  0.004021208733320236,\n",
       "  0.003745621768757701,\n",
       "  0.004334711004048586,\n",
       "  0.004188781604170799,\n",
       "  0.005131438374519348,\n",
       "  0.0042328075505793095,\n",
       "  0.003826128551736474,\n",
       "  0.003937677014619112,\n",
       "  0.003681859001517296,\n",
       "  0.004255225881934166,\n",
       "  0.0041037616319954395,\n",
       "  0.005022082943469286,\n",
       "  0.004163627978414297,\n",
       "  0.003742820117622614,\n",
       "  0.003863099729642272,\n",
       "  0.0036236911546438932,\n",
       "  0.004183528013527393,\n",
       "  0.004026965703815222,\n",
       "  0.004912667907774448,\n",
       "  0.00409116642549634,\n",
       "  0.0036646416410803795,\n",
       "  0.003793755080550909,\n",
       "  0.00356478663161397,\n",
       "  0.004111016634851694,\n",
       "  0.00394838023930788,\n",
       "  0.004800434224307537,\n",
       "  0.004013765137642622,\n",
       "  0.003589209634810686,\n",
       "  0.0037261820398271084,\n",
       "  0.003506277920678258,\n",
       "  0.004038479179143906,\n",
       "  0.0038725952617824078,\n",
       "  0.004692595452070236,\n",
       "  0.003929295111447573,\n",
       "  0.0035183089785277843,\n",
       "  0.0036698421463370323,\n",
       "  0.0034516658633947372,\n",
       "  0.003981702495366335,\n",
       "  0.0038019586354494095,\n",
       "  0.00455846032127738,\n",
       "  0.003856680355966091,\n",
       "  0.003454191144555807,\n",
       "  0.0035998946987092495,\n",
       "  0.0033985490445047617,\n",
       "  0.003902828088030219,\n",
       "  0.0037278947420418262,\n",
       "  0.004482987802475691,\n",
       "  0.0037613140884786844,\n",
       "  0.003388254437595606,\n",
       "  0.003554277354851365,\n",
       "  0.0033482785802334547,\n",
       "  0.003862077370285988,\n",
       "  0.003663046285510063,\n",
       "  0.004307566676288843,\n",
       "  0.0037106529343873262,\n",
       "  0.0033342596143484116,\n",
       "  0.0034883751068264246,\n",
       "  0.0032968276645988226,\n",
       "  0.003798830322921276,\n",
       "  0.003576525254175067,\n",
       "  0.004224403761327267,\n",
       "  0.0036214771680533886,\n",
       "  0.0032694232650101185,\n",
       "  0.0034396578557789326,\n",
       "  0.0032448142301291227,\n",
       "  0.003749521216377616,\n",
       "  0.003519346471875906,\n",
       "  0.004110120702534914,\n",
       "  0.0035577479284256697,\n",
       "  0.0032047720160335302,\n",
       "  0.003385429736226797,\n",
       "  0.0032021119259297848,\n",
       "  0.0037007364444434643,\n",
       "  0.003454189980402589,\n",
       "  0.00401637377217412,\n",
       "  0.0034899970050901175,\n",
       "  0.0031440304592251778,\n",
       "  0.0033388256561011076,\n",
       "  0.0031581774819642305,\n",
       "  0.003655747277662158,\n",
       "  0.003395361825823784,\n",
       "  0.003938991576433182,\n",
       "  0.003434690646827221,\n",
       "  0.0030866253655403852,\n",
       "  0.0032957233488559723,\n",
       "  0.0031194803304970264,\n",
       "  0.0036126812919974327,\n",
       "  0.003343186341226101,\n",
       "  0.003854883136227727,\n",
       "  0.0033866404555737972,\n",
       "  0.0030296745244413614,\n",
       "  0.0032496657222509384,\n",
       "  0.0030758236534893513,\n",
       "  0.0035780896432697773,\n",
       "  0.0032833789009600878,\n",
       "  0.003802528139203787,\n",
       "  0.0033247540704905987,\n",
       "  0.002974624279886484,\n",
       "  0.0032062220852822065,\n",
       "  0.0030358033254742622,\n",
       "  0.0035435338504612446,\n",
       "  0.0032320062164217234,\n",
       "  0.0037276328075677156,\n",
       "  0.003274539252743125,\n",
       "  0.0029276711866259575,\n",
       "  0.0031552596483379602,\n",
       "  0.002996874274685979,\n",
       "  0.003511160844936967,\n",
       "  0.003165303496643901,\n",
       "  0.0036906644236296415,\n",
       "  0.003202222753316164,\n",
       "  0.0028815544210374355,\n",
       "  0.003102357964962721,\n",
       "  0.0029681308660656214,\n",
       "  0.003470003604888916,\n",
       "  0.003120449371635914,\n",
       "  0.003609663574025035,\n",
       "  0.0031464295461773872,\n",
       "  0.002838879358023405,\n",
       "  0.0030441535636782646,\n",
       "  0.0029348935931921005,\n",
       "  0.0034208339639008045,\n",
       "  0.003058966714888811,\n",
       "  0.0035677109844982624,\n",
       "  0.0030817303340882063,\n",
       "  0.002796422690153122,\n",
       "  0.0029971832409501076,\n",
       "  0.0029108659364283085,\n",
       "  0.0033749123103916645,\n",
       "  0.0030316151678562164,\n",
       "  0.003486115485429764,\n",
       "  0.003042190335690975,\n",
       "  0.002754021668806672,\n",
       "  0.002950995694845915,\n",
       "  0.00287642446346581,\n",
       "  0.003328789724037051,\n",
       "  0.0029812853317707777,\n",
       "  0.0034511457197368145,\n",
       "  0.002989307278767228,\n",
       "  0.002710812957957387,\n",
       "  0.0029166501481086016,\n",
       "  0.0028539064805954695,\n",
       "  0.003288009436801076,\n",
       "  0.0029697667341679335,\n",
       "  0.0033768960274755955,\n",
       "  0.002967643551528454,\n",
       "  0.0026719141751527786,\n",
       "  0.002881550695747137,\n",
       "  0.002815619809553027,\n",
       "  0.0032497388310730457,\n",
       "  0.0029195223469287157,\n",
       "  0.0033434226643294096,\n",
       "  0.002920671598985791,\n",
       "  0.002628608141094446,\n",
       "  0.0028535653837025166,\n",
       "  0.0027927090413868427,\n",
       "  0.003214855445548892,\n",
       "  0.0029185740277171135,\n",
       "  0.003277204465121031,\n",
       "  0.002911610994488001,\n",
       "  0.0025895372964441776,\n",
       "  0.002827340504154563,\n",
       "  0.0027503857854753733,\n",
       "  0.003184833098202944,\n",
       "  0.002866394817829132,\n",
       "  0.003243840066716075,\n",
       "  0.002866157563403249,\n",
       "  0.0025469611864537,\n",
       "  0.0028030015528202057,\n",
       "  0.0027212752029299736,\n",
       "  0.0031605421099811792,\n",
       "  0.0028608012944459915,\n",
       "  0.003193582408130169,\n",
       "  0.002852844772860408,\n",
       "  0.0025074200239032507,\n",
       "  0.0027767496649175882,\n",
       "  0.002672740491107106,\n",
       "  0.0031393796671181917,\n",
       "  0.0028014841955155134,\n",
       "  0.0031584480311721563,\n",
       "  0.0027900603599846363,\n",
       "  0.0024742952082306147,\n",
       "  0.0027396429795771837,\n",
       "  0.002641533501446247,\n",
       "  0.003112364560365677,\n",
       "  0.002780613023787737,\n",
       "  0.003119234461337328,\n",
       "  0.0027527741622179747,\n",
       "  0.0024390933103859425,\n",
       "  0.002700981218367815,\n",
       "  0.00260214414447546,\n",
       "  0.0030765864066779613,\n",
       "  0.002731872722506523,\n",
       "  0.003073229221627116,\n",
       "  0.00269082048907876,\n",
       "  0.0024115729611366987,\n",
       "  0.0026580500416457653,\n",
       "  0.0025739523116499186,\n",
       "  0.0030390035826712847,\n",
       "  0.002706324914470315,\n",
       "  0.0030320759397000074,\n",
       "  0.0026488546282052994,\n",
       "  0.0023766416124999523,\n",
       "  0.002619639039039612,\n",
       "  0.0025414028204977512,\n",
       "  0.0029969639144837856,\n",
       "  0.0026713854167610407,\n",
       "  0.002981752622872591,\n",
       "  0.0026021592784672976,\n",
       "  0.002347346628084779,\n",
       "  0.002583635738119483,\n",
       "  0.002512221224606037,\n",
       "  0.0029591266065835953,\n",
       "  0.0026420934591442347,\n",
       "  0.002944427076727152,\n",
       "  0.002560326363891363,\n",
       "  0.0023146180901676416,\n",
       "  0.002551295794546604,\n",
       "  0.002483206335455179,\n",
       "  0.002921548904851079,\n",
       "  0.0026122659910470247,\n",
       "  0.002902749227359891,\n",
       "  0.0025227658916264772,\n",
       "  0.002285056747496128,\n",
       "  0.002518886234611273,\n",
       "  0.0024525399785488844,\n",
       "  0.0028870475944131613,\n",
       "  0.002581893466413021,\n",
       "  0.0028652269393205643,\n",
       "  0.0024870240595191717,\n",
       "  0.002256248611956835,\n",
       "  0.002488770755007863,\n",
       "  0.0024281172081828117,\n",
       "  0.002856655977666378,\n",
       "  0.0025558769702911377,\n",
       "  0.002827918389812112,\n",
       "  0.0024556389544159174,\n",
       "  0.002228568075224757,\n",
       "  0.0024581330362707376,\n",
       "  0.002400788012892008,\n",
       "  0.0028254485223442316,\n",
       "  0.0025285526644438505,\n",
       "  0.002785777673125267,\n",
       "  0.0024222834035754204,\n",
       "  0.002201266121119261,\n",
       "  0.0024291793815791607,\n",
       "  0.0023752483539283276,\n",
       "  0.0027980541344732046,\n",
       "  0.0025013810954988003,\n",
       "  0.002750321989879012,\n",
       "  0.0023901406675577164,\n",
       "  0.0021757620852440596,\n",
       "  0.002399998717010021,\n",
       "  0.002349999500438571,\n",
       "  0.0027706963010132313,\n",
       "  0.0024769005831331015,\n",
       "  0.002711812499910593,\n",
       "  0.0023618717677891254,\n",
       "  0.0021507844794541597,\n",
       "  0.002372274175286293,\n",
       "  0.0023260321468114853,\n",
       "  0.002743725897744298,\n",
       "  0.0024510130751878023,\n",
       "  0.0026764050126075745,\n",
       "  0.00233441055752337,\n",
       "  0.002126402920112014,\n",
       "  0.0023467428982257843,\n",
       "  0.0023059602826833725,\n",
       "  0.0027193163987249136,\n",
       "  0.0024284308310598135,\n",
       "  0.0026420089416205883,\n",
       "  0.002312004566192627,\n",
       "  0.0021023040171712637,\n",
       "  0.0023220195434987545,\n",
       "  0.0022850658278912306,\n",
       "  0.002694222377613187,\n",
       "  0.00240590563043952,\n",
       "  0.002605608431622386,\n",
       "  0.0022895208094269037,\n",
       "  0.0020767960231751204,\n",
       "  0.0022986356634646654,\n",
       "  0.002263666596263647,\n",
       "  0.0026704727206379175,\n",
       "  0.00238309265114367,\n",
       "  0.0025730347260832787,\n",
       "  0.002269560704007745,\n",
       "  0.002054163720458746,\n",
       "  0.002277252497151494,\n",
       "  0.0022414010018110275,\n",
       "  0.002647487446665764,\n",
       "  0.0023663011379539967,\n",
       "  0.0025393422693014145,\n",
       "  0.0022467582020908594,\n",
       "  0.002027961192652583,\n",
       "  0.0022563061211258173,\n",
       "  0.00221843714825809,\n",
       "  0.002620526123791933,\n",
       "  0.002341141225770116,\n",
       "  0.002507411874830723,\n",
       "  0.0022269717883318663,\n",
       "  0.002003024099394679,\n",
       "  0.0022382207680493593,\n",
       "  0.0021967196371406317,\n",
       "  0.0026028661523014307,\n",
       "  0.0023238249123096466,\n",
       "  0.002474038628861308,\n",
       "  0.0022147884592413902,\n",
       "  0.001981765031814575,\n",
       "  0.0022244160063564777,\n",
       "  0.0021754757035523653,\n",
       "  0.0025851784739643335,\n",
       "  0.0023091787006706,\n",
       "  0.0024406742304563522,\n",
       "  0.002199901035055518,\n",
       "  0.0019539862405508757,\n",
       "  0.0022119793575257063,\n",
       "  0.002144827973097563,\n",
       "  0.0025715683586895466,\n",
       "  0.0022899142932146788,\n",
       "  0.002408064203336835,\n",
       "  0.0021828666795045137,\n",
       "  0.0019292712677270174,\n",
       "  0.002200684743002057,\n",
       "  0.0021165385842323303,\n",
       "  0.0025612362660467625,\n",
       "  0.0022696806117892265,\n",
       "  0.0023835301399230957,\n",
       "  0.002162171760573983,\n",
       "  0.001903509721159935,\n",
       "  0.002189303282648325,\n",
       "  0.0020833639428019524,\n",
       "  0.0025524769444018602,\n",
       "  0.0022446708753705025,\n",
       "  0.0023625988978892565,\n",
       "  0.0021314083132892847,\n",
       "  0.001880335039459169,\n",
       "  0.0021719755604863167,\n",
       "  0.002051155548542738,\n",
       "  0.0025441565085202456,\n",
       "  0.002208279212936759,\n",
       "  0.0023485138081014156,\n",
       "  0.002085090149194002,\n",
       "  0.0018657577456906438,\n",
       "  0.002146894810721278,\n",
       "  0.0020304869394749403,\n",
       "  0.0025252311024814844,\n",
       "  0.00217019603587687,\n",
       "  0.0023290752433240414,\n",
       "  0.002033345401287079,\n",
       "  0.0018548869993537664,\n",
       "  0.002105419524013996,\n",
       "  0.0020184461027383804,\n",
       "  0.002491447376087308,\n",
       "  0.0021420340053737164,\n",
       "  0.0023008522111922503,\n",
       "  0.0019886703230440617,\n",
       "  0.0018451494397595525,\n",
       "  0.002068081870675087,\n",
       "  0.002016091486439109,\n",
       "  0.002448020037263632,\n",
       "  0.0021224261727184057,\n",
       "  0.002258653286844492,\n",
       "  0.001961523201316595,\n",
       "  0.0018273433670401573,\n",
       "  0.0020352029241621494,\n",
       "  0.002014334313571453,\n",
       "  0.0024090951774269342,\n",
       "  0.0021133001428097486,\n",
       "  0.0022104813251644373,\n",
       "  0.0019511980935931206,\n",
       "  0.0018043357413262129,\n",
       "  0.0020145655144006014,\n",
       "  0.002007521456107497,\n",
       "  0.002377772470936179,\n",
       "  0.0021046854089945555,\n",
       "  0.0021639573387801647,\n",
       "  0.0019524270901456475,\n",
       "  0.0017814348684623837,\n",
       "  0.0020033058244735003,\n",
       "  0.0019947043620049953,\n",
       "  0.0023579054977744818,\n",
       "  0.0020940331742167473,\n",
       "  0.0021235516760498285,\n",
       "  0.001953767379745841,\n",
       "  0.0017582620494067669,\n",
       "  0.0019943993538618088,\n",
       "  0.001973144244402647,\n",
       "  0.002345423912629485,\n",
       "  0.002079431666061282,\n",
       "  0.002088862471282482,\n",
       "  0.0019516701577231288,\n",
       "  0.0017412489978596568,\n",
       "  0.0019846183713525534,\n",
       "  0.0019488532561808825,\n",
       "  0.0023369661066681147,\n",
       "  0.002061679260805249,\n",
       "  0.0020580764394253492,\n",
       "  0.0019470241386443377,\n",
       "  0.0017287889495491982,\n",
       "  0.0019749291241168976,\n",
       "  0.0019221497932448983,\n",
       "  0.0023315176367759705,\n",
       "  0.0020427515264600515,\n",
       "  0.00203407509252429,\n",
       "  0.0019363927422091365,\n",
       "  0.00171852414496243,\n",
       "  0.001962947426363826,\n",
       "  0.001898252870887518,\n",
       "  0.002326592104509473,\n",
       "  0.002024261746555567,\n",
       "  0.0020131829660385847,\n",
       "  0.0019207182340323925,\n",
       "  0.0017116981325671077,\n",
       "  0.001953663071617484,\n",
       "  0.0018738850485533476,\n",
       "  0.0023199538700282574,\n",
       "  0.0020050243474543095,\n",
       "  0.001996359322220087,\n",
       "  0.0018984600901603699,\n",
       "  0.0017057800432667136,\n",
       "  0.0019437465816736221,\n",
       "  0.001850687200203538,\n",
       "  0.002313389675691724,\n",
       "  0.0019873944111168385,\n",
       "  0.0019812360405921936,\n",
       "  0.0018693703459575772,\n",
       "  0.0016986217815428972,\n",
       "  0.0019306347239762545,\n",
       "  0.0018299993826076388,\n",
       "  0.0023006668779999018,\n",
       "  0.001968726748600602,\n",
       "  0.0019664596766233444,\n",
       "  0.0018342180410400033,\n",
       "  0.0016948450356721878,\n",
       "  0.0019139046780765057,\n",
       "  0.0018124512862414122,\n",
       "  0.002278345637023449,\n",
       "  0.0019528870470821857,\n",
       "  0.0019462438067421317,\n",
       "  0.0017975671216845512,\n",
       "  0.0016847116639837623,\n",
       "  0.0018933187238872051,\n",
       "  0.0017972647910937667,\n",
       "  0.00224888208322227,\n",
       "  0.0019374790135771036,\n",
       "  0.0019227754091843963,\n",
       "  0.001760558458045125,\n",
       "  0.00166759779676795,\n",
       "  0.001870744046755135,\n",
       "  0.0017847816925495863,\n",
       "  0.002214658772572875,\n",
       "  0.001925500575453043,\n",
       "  0.0018921211594715714,\n",
       "  0.0017297129379585385,\n",
       "  0.0016436332371085882,\n",
       "  0.0018501300364732742,\n",
       "  0.0017716943984851241,\n",
       "  0.0021752729080617428,\n",
       "  0.0019134776666760445,\n",
       "  0.0018606075318530202,\n",
       "  0.001706431619822979,\n",
       "  0.0016157375648617744,\n",
       "  0.0018338060472160578,\n",
       "  0.0017571562202647328,\n",
       "  0.0021414482034742832,\n",
       "  0.0018991843098774552,\n",
       "  0.0018292993772774935,\n",
       "  0.0016891158884391189,\n",
       "  0.001591130392625928,\n",
       "  0.0018206139793619514,\n",
       "  0.0017435472691431642,\n",
       "  0.0021126156207174063,\n",
       "  0.0018848925828933716,\n",
       "  0.0017978715477511287,\n",
       "  0.0016741851577535272,\n",
       "  0.0015691568842157722,\n",
       "  0.0018084693001583219,\n",
       "  0.0017279920866712928,\n",
       "  0.002086776541545987,\n",
       "  0.0018693431047722697,\n",
       "  0.001769070397131145,\n",
       "  0.0016608304576948285,\n",
       "  0.001547516672872007,\n",
       "  0.0017947673331946135,\n",
       "  0.0017113469075411558,\n",
       "  0.002064409665763378,\n",
       "  0.0018546454375609756,\n",
       "  0.0017434170003980398,\n",
       "  0.0016470116097480059,\n",
       "  0.0015270533040165901,\n",
       "  0.0017796638421714306,\n",
       "  0.0016962584340944886,\n",
       "  0.0020458155777305365,\n",
       "  0.001840686541981995,\n",
       "  0.0017182850278913975,\n",
       "  0.0016319410642609,\n",
       "  0.0015059637371450663,\n",
       "  0.0017647452186793089,\n",
       "  0.0016810037195682526,\n",
       "  0.0020272545516490936,\n",
       "  0.001829582266509533,\n",
       "  0.0016965034883469343,\n",
       "  0.0016191861359402537,\n",
       "  0.001488259993493557,\n",
       "  0.001751336851157248,\n",
       "  0.0016693300567567348,\n",
       "  0.002010939409956336,\n",
       "  0.0018198153702542186,\n",
       "  0.0016766341868788004,\n",
       "  0.0016076206229627132,\n",
       "  0.0014735913136973977,\n",
       "  0.0017443335382267833,\n",
       "  0.0016593503532931209,\n",
       "  0.0020010238513350487,\n",
       "  0.0018111689714714885,\n",
       "  0.0016568502178415656,\n",
       "  0.0015946186613291502,\n",
       "  0.0014622188173234463,\n",
       "  0.0017467326251789927,\n",
       "  0.0016555004986003041,\n",
       "  0.0019939246121793985,\n",
       "  0.0017976317321881652,\n",
       "  0.0016336147673428059,\n",
       "  0.0015928501961752772,\n",
       "  0.0014861159725114703,\n",
       "  0.001800726167857647,\n",
       "  0.0016788265202194452,\n",
       "  0.0019925630185753107,\n",
       "  0.0017742662457749248,\n",
       "  0.0016245078295469284,\n",
       "  0.0016607723664492369,\n",
       "  0.0016080165514722466,\n",
       "  0.001891882624477148,\n",
       "  0.0016355466796085238,\n",
       "  0.001957806060090661,\n",
       "  0.0018237801268696785,\n",
       "  0.0017503394046798348,\n",
       "  0.001778139267116785,\n",
       "  0.0015605647349730134,\n",
       "  0.0017469620797783136,\n",
       "  0.0016259652329608798,\n",
       "  0.0021477865520864725,\n",
       "  0.0018595472211018205,\n",
       "  0.0016013429267331958,\n",
       "  0.001623262302018702,\n",
       "  0.0015323865227401257,\n",
       "  0.0017820196226239204,\n",
       "  0.0015570579562336206,\n",
       "  0.0020488195586949587,\n",
       "  0.0018807487795129418,\n",
       "  0.0016720439307391644,\n",
       "  0.0015329529996961355,\n",
       "  0.0014288934180513024,\n",
       "  0.0017384699312970042,\n",
       "  0.0015711368760094047,\n",
       "  0.0019252330530434847,\n",
       "  0.0017267805524170399,\n",
       "  0.0015900194412097335,\n",
       "  0.0015090605011209846,\n",
       "  0.0014097100356593728,\n",
       "  0.001699464744888246,\n",
       "  0.0015787979355081916,\n",
       "  0.0019269675249233842,\n",
       "  0.001714173355139792,\n",
       "  0.0015796322841197252,\n",
       "  0.0014797506155446172,\n",
       "  0.0013868007808923721,\n",
       "  0.001661514164879918,\n",
       "  0.0015355556970462203,\n",
       "  0.0018668734701350331,\n",
       "  0.0016972365556284785,\n",
       "  0.0015367163578048348,\n",
       "  0.0014545237645506859,\n",
       "  0.0013501530047506094,\n",
       "  0.0016427543014287949,\n",
       "  0.0015066517516970634,\n",
       "  0.0018406332237645984,\n",
       "  0.001682994537986815,\n",
       "  0.0015078417491167784,\n",
       "  0.0014381275977939367,\n",
       "  0.0013266041642054915,\n",
       "  0.0016265660524368286,\n",
       "  0.0014747469685971737,\n",
       "  0.0018105868948623538,\n",
       "  0.0016501678619533777,\n",
       "  0.0014787267427891493,\n",
       "  0.00140477460809052,\n",
       "  0.0013175109634175897,\n",
       "  0.0015982594341039658,\n",
       "  0.0014464855194091797,\n",
       "  0.0018019706476479769,\n",
       "  0.0016340762376785278,\n",
       "  0.0014514047652482986,\n",
       "  0.0013971258886158466,\n",
       "  0.0012858463451266289,\n",
       "  0.0015920334262773395,\n",
       "  0.0014431234449148178,\n",
       "  0.00178497564047575,\n",
       "  0.0016143943648785353,\n",
       "  0.0014511644840240479,\n",
       "  0.001381664420478046,\n",
       "  0.001279890420846641,\n",
       "  0.0015773207414895296,\n",
       "  0.0014209082582965493,\n",
       "  0.0017813827143982053,\n",
       "  0.0016004755161702633,\n",
       "  0.0014156963443383574,\n",
       "  0.0013738669222220778,\n",
       "  0.0012864950112998486,\n",
       "  0.0015652502188459039,\n",
       "  0.001414823462255299,\n",
       "  0.0017656412674114108,\n",
       "  0.0015828987816348672,\n",
       "  0.001418696716427803,\n",
       "  0.0013709438499063253,\n",
       "  0.0012575576547533274,\n",
       "  0.0015637647593393922,\n",
       "  0.0013995926128700376,\n",
       "  0.001770740607753396,\n",
       "  0.0015729081351310015,\n",
       "  0.0013976823538541794,\n",
       "  0.0013563247630372643,\n",
       "  0.0012565621873363853,\n",
       "  0.0015372459311038256,\n",
       "  0.0013924124650657177,\n",
       "  0.0017568905605003238,\n",
       "  0.0015554832061752677,\n",
       "  0.001385906944051385,\n",
       "  0.0013489119010046124,\n",
       "  0.0012734279735013843,\n",
       "  0.001538193435408175,\n",
       "  0.0013832973781973124,\n",
       "  0.0017421513330191374,\n",
       "  0.0015494247199967504,\n",
       "  0.0013688403414562345,\n",
       "  0.0013435130240395665,\n",
       "  0.001238431897945702,\n",
       "  0.0015245027607306838,\n",
       "  0.0013723396696150303,\n",
       "  0.0017423960380256176,\n",
       "  0.0015311379684135318,\n",
       "  0.0013596031349152327,\n",
       "  0.0013282035943120718,\n",
       "  0.0012378788087517023,\n",
       "  0.0014997102553024888,\n",
       "  0.0013670470798388124,\n",
       "  0.0017278391169384122,\n",
       "  0.0015245529357343912,\n",
       "  0.001338821486569941,\n",
       "  0.0013150270096957684,\n",
       "  0.0012415567180141807,\n",
       "  0.0015006496105343103,\n",
       "  0.0013620792888104916,\n",
       "  0.001710975542664528,\n",
       "  0.0015157376183196902,\n",
       "  0.001321019371971488,\n",
       "  0.0013120071962475777,\n",
       "  0.0012242244556546211,\n",
       "  0.001491131610237062,\n",
       "  0.0013514156453311443,\n",
       "  0.0017002777894958854,\n",
       "  0.0014991392381489277,\n",
       "  0.0013120005605742335,\n",
       "  0.0013029455440118909,\n",
       "  0.0012263444950804114,\n",
       "  0.0014721954939886928,\n",
       "  0.0013426367659121752,\n",
       "  0.001695691724307835,\n",
       "  0.0014960337430238724,\n",
       "  0.001292322645895183,\n",
       "  0.0012893647653982043,\n",
       "  0.0012118036393076181,\n",
       "  0.0014590035425499082,\n",
       "  0.0013316357508301735,\n",
       "  0.001667733653448522,\n",
       "  0.0014851801097393036,\n",
       "  0.001276885042898357,\n",
       "  0.0012754617491737008,\n",
       "  0.001189537812024355,\n",
       "  0.0014485148712992668,\n",
       "  0.0013210795586928725,\n",
       "  0.0016597292851656675,\n",
       "  0.00148001650813967,\n",
       "  0.0012746247230097651,\n",
       "  0.0012796293012797832,\n",
       "  0.00118507188744843,\n",
       "  0.001431419630534947,\n",
       "  0.0013150875456631184,\n",
       "  0.0016411100514233112,\n",
       "  0.0014681509928777814,\n",
       "  0.0012501028832048178,\n",
       "  0.0012534341076388955,\n",
       "  0.0011771308491006494,\n",
       "  0.0014421653468161821,\n",
       "  0.00132264057174325,\n",
       "  0.0016356694977730513,\n",
       "  0.0014525523874908686,\n",
       "  0.0012311983155086637,\n",
       "  0.0012642430374398828,\n",
       "  0.001219849567860365,\n",
       "  0.0014846633421257138,\n",
       "  0.0013173194602131844,\n",
       "  0.001606750418432057,\n",
       "  0.0014402749948203564,\n",
       "  ...],\n",
       " 'testerrors1': [],\n",
       " 'testerrors2': [],\n",
       " 'testerrorsinf': []}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models\n",
    "lconfig = load_config(\"../autoencoder/configs/experiments/etinet.yaml\")\n",
    "experiment = ETINetHelper(lconfig)\n",
    "\n",
    "FFNet = models.FFNet\n",
    "FFAutoencoder = models.FFAutoencoder\n",
    "test = experiment.create_etinet(dset, 10)\n",
    "\n",
    "test.train_aenet(500, lr=1e-3)\n",
    "test.train_recnet(500, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afdb05f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ETINet.get_errors() missing 1 required positional argument: 'testrest'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mETINetHelper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_errorparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 120\u001b[0m, in \u001b[0;36mETINetHelper.plot_errorparams\u001b[1;34m(etinet, param)\u001b[0m\n\u001b[0;32m    117\u001b[0m             param \u001b[38;5;241m=\u001b[39m p\n\u001b[0;32m    118\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m l2error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mETINetHelper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_operrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43metinet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43metinet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    121\u001b[0m params \u001b[38;5;241m=\u001b[39m etinet\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mparams\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(params\u001b[38;5;241m.\u001b[39mshape, l2error\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[2], line 65\u001b[0m, in \u001b[0;36mETINetHelper.get_operrs\u001b[1;34m(etinet, times, testonly)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m   data \u001b[38;5;241m=\u001b[39m etinet\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m---> 65\u001b[0m errors \u001b[38;5;241m=\u001b[39m \u001b[43metinet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m errors\n",
      "\u001b[1;31mTypeError\u001b[0m: ETINet.get_errors() missing 1 required positional argument: 'testrest'"
     ]
    }
   ],
   "source": [
    "ETINetHelper.plot_errorparams(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "das",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
