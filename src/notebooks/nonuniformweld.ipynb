{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2930701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup code\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "basedir = \"../..\"\n",
    "\n",
    "import datetime\n",
    "import skdim\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "import models\n",
    "import utils\n",
    "\n",
    "def determine_params(paramarr):\n",
    "  encoding_param = []\n",
    "  P = paramarr.shape[1]\n",
    "\n",
    "  for p in range(P):\n",
    "    if np.abs(paramarr[0, p] - paramarr[1, p]) > 0:\n",
    "      encoding_param.append(p)\n",
    "\n",
    "  return encoding_param\n",
    "\n",
    "%matplotlib widget\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "dsets = [(f\"{basedir}/datasets/burgers/grfarc2visc0p001-shift.mat\", \"grfarc2visc0p001\"),\n",
    "          (f\"{basedir}/datasets/burgers/grfarc2visc0p001-scale.mat\", \"grfarc2visc0p001\"),\n",
    "          (f\"{basedir}/datasets/transport/hats2_2500_shift.mat\", \"alldata\"),\n",
    "          (f\"{basedir}/datasets/transport/hats2_2500_scale.mat\", \"alldata\"),\n",
    "          (f\"{basedir}/datasets/kdv/kdv2-shift.mat\", \"kdv2wide\"),\n",
    "          (f\"{basedir}/datasets/kdv/kdv2-scale.mat\", \"kdv2wide\")]\n",
    "\n",
    "names = [\"bshift\", \"bscale\", \"tshift\", \"tscale\", \"kshift\", \"kscale\"]\n",
    "\n",
    "# dsets2d = [(f\"{basedir}/datasets/transport/hats2d_shift.mat\", \"alldata\"), \n",
    "#            (f\"{basedir}/datasets/transport/hats2d_scale.mat\", \"alldata\")]\n",
    "\n",
    "# names2d = [\"t2shift\", \"t2scale\"]\n",
    "\n",
    "# dsets = dsets2d\n",
    "# names = names2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218f5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeldNetNonUniform(models.WeldNet):\n",
    "  def __init__(self, dataset, maxwindows, aeclass, aeparams, propclass, propparams, transclass=None, transparams=None, alpha=0.05, residualprop=True, straightness=0, warmstart=True, tensorboard_directory=None, seed=0, device=0, kinetic=0, autonomous=True):\n",
    "    self.dataset = dataset\n",
    "    self.device = device\n",
    "    self.td = tensorboard_directory\n",
    "    self.straightness = straightness\n",
    "    self.kinetic = kinetic\n",
    "    self.autonomous = autonomous\n",
    "    self.residualprop = residualprop\n",
    "\n",
    "    assert(autonomous)\n",
    "\n",
    "    assert(self.straightness == 0 or self.kinetic == 0)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    datacopy = self.dataset.data.copy()\n",
    "    self.numtrain = int(datacopy.shape[0] * 0.8)\n",
    "    \n",
    "    self.T = self.dataset.data.shape[1]\n",
    "    self.Wmax = maxwindows\n",
    "    self.alpha = alpha\n",
    "    \n",
    "    self.aes = []\n",
    "    self.props = []\n",
    "\n",
    "    self.alltrain = datacopy[:self.numtrain]\n",
    "    self.alltest = datacopy[self.numtrain:]\n",
    "\n",
    "    self.aedata = [aeclass, aeparams, maxwindows, alpha, warmstart, self.straightness, self.kinetic]\n",
    "    self.propdata = [propclass, propparams, maxwindows, alpha, warmstart, self.autonomous, self.residualprop]\n",
    "    self.datadata = [np.sum(self.dataset.data), self.dataset.data.shape]\n",
    "\n",
    "    if transclass is not None:\n",
    "      self.transcoderdata = self.aedata + self.propdata + [transclass, transparams]\n",
    "\n",
    "    self.windowvals = False\n",
    "    self.transcoders = []\n",
    "    self.aeclass = aeclass\n",
    "    self.aeparams = aeparams\n",
    "    self.propclass = propclass\n",
    "    self.propparams = propparams\n",
    "    self.transclass = propclass\n",
    "    self.transparams = transparams\n",
    "    self.warmstart = warmstart\n",
    "\n",
    "  def train_aes(self, epochs_first, warmstart_epochs=-1, save=True, optim=torch.optim.AdamW, lr=1e-4, plottb=True, backwardstime=False, printinterval=10, batch=32, ridge=0, loss=None, encoding_param=-1):\n",
    "    if warmstart_epochs == -1 and self.warmstart:\n",
    "      warmstart_epochs = epochs_first\n",
    "    \n",
    "    def train_one_ae(ae, a, b, epochs):\n",
    "      train = torch.tensor(self.alltrain[:, a:b+1]).float()\n",
    "      test = torch.tensor(self.alltest[:, a:b+1]).float()\n",
    "\n",
    "      writer = None\n",
    "      if self.td is not None:\n",
    "        name = f\"./tensorboard/{datetime.datetime.now().strftime('%d-%B-%Y')}/{self.td}-weld{a}to{b}/{datetime.datetime.now().strftime('%H.%M.%S')}/\"\n",
    "        writer = torch.utils.tensorboard.SummaryWriter(name)\n",
    "        print(\"Tensorboard writer location is \" + name)\n",
    "        \n",
    "      print(f\"Starting training WeldNet AE on [{a}, {b}] at {time.asctime()}...\")\n",
    "      print(\"Number of NN trainable parameters\", utils.num_params(ae))\n",
    "      print(\"train\", train.shape, \"test\", test.shape)\n",
    "\n",
    "      self.aestep = 0\n",
    "      opt = optim(ae.parameters(), lr=lr, weight_decay=ridge)\n",
    "      dataloader = DataLoader(train, batch_size=batch)\n",
    "\n",
    "      for ep in range(epochs):\n",
    "          ae_epoch(ae, dataloader, optimizer=opt, writer=writer, ep=ep, printinterval=printinterval, loss=loss, testarr=test)\n",
    "\n",
    "          if ep % 5 == 0 and plottb:\n",
    "            pass#self.plot_encoding(w, encoding_param, step=self.aestep, writer=writer, tensorboard=True)\n",
    "      \n",
    "      print(f\"Finish training AE [{a}, {b}] at {time.asctime()}.\")\n",
    "\n",
    "      return ae\n",
    "\n",
    "    def ae_epoch(model, dataloader, writer=None, optimizer=None, ep=0, printinterval=10, loss=None, testarr=None):\n",
    "      def closure(batch):\n",
    "        optimizer.zero_grad()\n",
    "        total = 0\n",
    "        penalties = 0\n",
    "\n",
    "        for N in range(batch.shape[0]):\n",
    "          traj = batch[N, :, :]\n",
    "          enc = model.encode(traj)\n",
    "          proj = model.decode(enc)\n",
    "\n",
    "          # compute regularization here\n",
    "          # add penalization to enc here\n",
    "          # (enc[:-1] - enc[1:]) ** 2 is proportional to velocity\n",
    "\n",
    "          res = loss(traj, proj)\n",
    "\n",
    "          if self.straightness > 0:\n",
    "            T = traj.shape[0]\n",
    "            i_values = torch.arange(1, T)\n",
    "            weights = (T - i_values) / T\n",
    "            term1 = torch.outer(weights, enc[0, :])\n",
    "            term2 = torch.outer((i_values / T), enc[-1, :])\n",
    "            term3 = enc[i_values, :]\n",
    "            penalty = loss(term1 + term2, term3)\n",
    "            penalties += self.straightness * penalty\n",
    "          elif self.kinetic > 0:\n",
    "            #acceleration\n",
    "            #starts = enc[:-2]\n",
    "            #mids = enc[1:-1]\n",
    "            #ends = enc[2:]\n",
    "\n",
    "            # maybe scale?\n",
    "\n",
    "            # order 2\n",
    "            starts = enc[:-2]\n",
    "            ends = enc[2:]\n",
    "            penalty = loss(starts - ends, torch.zeros_like(starts))\n",
    "            penalties += self.kinetic * penalty\n",
    "\n",
    "            # order one\n",
    "            # starts = enc[:-1, :]\n",
    "            # ends = enc[1:, :]\n",
    "            # penalty = loss(starts - ends, torch.zeros_like(starts))\n",
    "            # penalties += self.kinetic * penalty\n",
    "\n",
    "          total += res\n",
    "        \n",
    "        total += penalties\n",
    "        total.backward()\n",
    "        \n",
    "        if writer is not None and self.aestep % 5:\n",
    "          writer.add_scalar(\"main/loss\", total, global_step=self.aestep)\n",
    "          writer.add_scalar(\"main/penalty\", penalties, global_step=self.aestep)\n",
    "\n",
    "        return total\n",
    "\n",
    "      for batch in dataloader:\n",
    "        self.aestep += 1\n",
    "        error = optimizer.step(lambda: closure(batch))\n",
    "\n",
    "      # print test\n",
    "      if printinterval > 0 and (ep % printinterval == 0):\n",
    "        proj = model(testarr)\n",
    "        testarr = testarr.cpu().detach().numpy()\n",
    "        proj = proj.cpu().detach().numpy()\n",
    "\n",
    "        testerr1 = np.mean(np.linalg.norm(testarr - proj, axis=1, ord=1) / np.linalg.norm(testarr, axis=1, ord=1))\n",
    "        testerr2 = np.mean(np.linalg.norm(testarr - proj, axis=1, ord=2) / np.linalg.norm(testarr, axis=1, ord=2))\n",
    "        testerrinf = np.mean(np.linalg.norm(testarr - proj, axis=1, ord=np.inf) / np.linalg.norm(testarr, axis=1, ord=np.inf))\n",
    "        \n",
    "        print(f\"{ep+1}: Train Loss {error:.3e}, Relative Projection Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"misc/relativeL1proj\", testerr1, global_step=ep)\n",
    "            writer.add_scalar(\"main/relativeL2proj\", testerr2, global_step=ep)\n",
    "            writer.add_scalar(\"misc/relativeLInfproj\", testerrinf, global_step=ep)\n",
    "\n",
    "    if loss is None:\n",
    "       loss = nn.MSELoss()\n",
    "    else:\n",
    "       loss = loss()\n",
    "\n",
    "    if encoding_param == -1:\n",
    "      encoding_param = []\n",
    "      P = self.dataset.params.shape[1]\n",
    "\n",
    "      for p in range(P):\n",
    "        if np.abs(self.dataset.params[0, p] - self.dataset.params[1, p]) > 0:\n",
    "          encoding_param.append(p)\n",
    "\n",
    "    print(f\"Training until we have up to {self.Wmax} WeldNet AEs. Starting with initial [0, {self.T-1}].\")\n",
    "\n",
    "    aefirst = self.aeclass(**self.aeparams)\n",
    "    aefirst = train_one_ae(aefirst, 0, self.T-1, epochs_first)\n",
    "\n",
    "    windowsleft = [[(0, self.T-1), aefirst]]\n",
    "    windowsdone = []\n",
    "\n",
    "    while len(windowsleft) > 0 and len(windowsleft) + len(windowsdone) < self.Wmax:\n",
    "      out = windowsleft[0]\n",
    "      a, b = out[0]\n",
    "      ae = out[1]\n",
    "      windowsleft = windowsleft[1:]\n",
    "\n",
    "      c = a + (b - a) // 2\n",
    "      state = ae.state_dict()\n",
    "\n",
    "      ae_a = self.aeclass(**self.aeparams)\n",
    "      if warmstart_epochs > 0:\n",
    "        ae_a.load_state_dict(state)\n",
    "      ae_a = train_one_ae(ae_a, a, c, warmstart_epochs)\n",
    "\n",
    "      ae_b = self.aeclass(**self.aeparams)\n",
    "      if warmstart_epochs > 0:\n",
    "        ae_b.load_state_dict(state)\n",
    "      ae_b = train_one_ae(ae_b, c, b, warmstart_epochs)\n",
    "\n",
    "      # lets evaluate error at c\n",
    "      testslice = torch.tensor(self.alltest[:, c, :]).float().to(self.device)\n",
    "      error = torch.norm(testslice - ae.forward(testslice))\n",
    "      errora = torch.norm(testslice - ae_a.forward(testslice))\n",
    "      errorb = torch.norm(testslice - ae_b.forward(testslice))\n",
    "\n",
    "      print(float(errora), float(errorb), \"versus\", float(error))\n",
    "\n",
    "      if 0.5 * (errora + errorb) < (1 - self.alpha) * error:\n",
    "        if c - a < 2 or b - c < 2:\n",
    "          if backwardstime:\n",
    "            windowsleft.append([(c, b), ae_b])\n",
    "            windowsleft.append([(a, c), ae_a])\n",
    "          else:\n",
    "            windowsleft.append([(a, c), ae_a])\n",
    "            windowsleft.append([(c, b), ae_b])    \n",
    "        \n",
    "          print(\"Splitting and finishing\", (a, b), \"at\", c)\n",
    "\n",
    "        else:\n",
    "          if backwardstime:\n",
    "            windowsleft.append([(c, b), ae_b])\n",
    "            windowsleft.append([(a, c), ae_a])\n",
    "          else:\n",
    "            windowsleft.append([(a, c), ae_a])\n",
    "            windowsleft.append([(c, b), ae_b])    \n",
    "       \n",
    "          print(\"Splitting\", (a, b), \"at\", c)\n",
    "      else:\n",
    "        windowsdone.append([(a, b), ae])\n",
    "        print(\"No more improvement for\", (a, b))\n",
    "\n",
    "    windows = sorted(windowsleft + windowsdone, key=lambda x: x[0][0])\n",
    "    self.aes = [x[1] for x in windows]\n",
    "    self.windowvals = [range(x[0][0], x[0][1]+1) for x in windows]\n",
    "\n",
    "    if save:\n",
    "      dire = \"savedmodels/weldnonunif\"\n",
    "      addr = f\"{dire}/{self.td}{self.Wmax}-{self.alpha}-{datetime.datetime.now().strftime('%d-%B-%Y-%H.%M')}.pickle\"\n",
    "\n",
    "      if not os.path.exists(dire):\n",
    "        os.makedirs(dire)\n",
    "\n",
    "      with open(addr, \"wb\") as handle:\n",
    "        pickle.dump({\"aes\": self.aes, \"aedata\": self.aedata, \"datadata\": self.datadata}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"AEs saved at\", addr)\n",
    "\n",
    "    print(\"Finished training all timewindows\")\n",
    "\n",
    "  def load_aes(self, filename, verbose=False):\n",
    "    matching_files = glob.glob(f\"savedmodels/weldnonunif/{filename}*{self.Wmax}-{self.alpha}-*\")\n",
    "    print(\"Searching for\", str(self.aedata), str(self.datadata))\n",
    "\n",
    "    for addr in matching_files:\n",
    "      with open(addr, \"rb\") as handle:\n",
    "        dic = pickle.load(handle)\n",
    "\n",
    "        if str(self.aedata) == str(dic[\"aedata\"]) and str(self.datadata) == str(dic[\"datadata\"]):\n",
    "          print(\"Loading AEs from\", addr)\n",
    "          self.aes = dic[\"aes\"]\n",
    "          return True\n",
    "        elif verbose:\n",
    "          print(\"NO MATCH\", str(dic[\"aedata\"]), str(dic[\"datadata\"]))\n",
    "            \n",
    "    print(f\"Load failed. Could not match with any files\")\n",
    "    print(matching_files)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302ae4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('../../datasets/burgers/grfarc2visc0p001-shift.mat', 'grfarc2visc0p001') bshift\n",
      "Searching for [<class 'models.FFAutoencoder'>, {'encodeSeq': [256, 400, 400, 400, 4], 'decodeSeq': [4, 400, 400, 400, 256], 'activation': ReLU()}, 10, 0.05, True, 0, 0] [-0.00015802056145730603, (500, 26, 256)]\n",
      "Loading AEs from savedmodels/weldnonunif\\bshiftbase-4-auton10-0.05-04-September-2024-18.50.pickle\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19912/2288568161.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mweld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_aes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprintinterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplottb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m           \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m           \u001b[1;31m# loadboth = False#weld.load_all(name)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baseepochs = 200\n",
    "\n",
    "aeclass = models.FFAutoencoder\n",
    "propclass = models.FFNet\n",
    "\n",
    "welds = {}\n",
    "start = 0\n",
    "end = 1\n",
    "for dset, name in zip(dsets[start:end], names[start:end]):\n",
    "  dataset = utils.DynamicData(dset)\n",
    "  print(dset, name)\n",
    "\n",
    "  dataset.shuffle_inplace()\n",
    "  dataset.subset_data(500)\n",
    "  dataset.downsample(int(dataset.data.shape[2] / 256))\n",
    "  dataset.data = dataset.data[:, ::2]\n",
    "  dataset.scaledown()\n",
    "\n",
    "  din = dataset.data.shape[-1]\n",
    "  Lae = 3\n",
    "  Lprop = 3\n",
    "  pae = 400\n",
    "  pprop = 200\n",
    "  rp = True\n",
    "  trans = True\n",
    "  ws = True\n",
    "\n",
    "  welds[name] = []\n",
    "\n",
    "  for k in [4]:\n",
    "    for style in [\"base\"]:\n",
    "      for w in [10]:\n",
    "        for auton in [True]:\n",
    "          aeargs = { \"encodeSeq\": [din] + [pae] * Lae + [k], \"decodeSeq\": [k] + [pae] * Lae + [din], \"activation\": nn.ReLU() }\n",
    "          propargs = { \"seq\": [k if auton else k+1] + [pprop] * Lprop + [k], \"activation\": nn.ReLU() }\n",
    "          transclass = models.FFNet if trans else None\n",
    "          transargs = { \"seq\": [k] + [pprop] * Lprop + [k], \"activation\": nn.ReLU() } if trans else None\n",
    "\n",
    "          if style == \"base\":\n",
    "            weld = WeldNetNonUniform(dataset, w, aeclass, aeargs, propclass, propargs, warmstart=ws, transclass=transclass, transparams=transargs, device=device, tensorboard_directory=f\"{name}base-{k}-{'auton' if auton else 'nonauton'}\", straightness=0, autonomous=auton, residualprop=rp)\n",
    "          elif style == \"straight\":\n",
    "            weld = WeldNetNonUniform(dataset, w, aeclass, aeargs, propclass, propargs, warmstart=ws, transclass=transclass, transparams=transargs, device=device, tensorboard_directory=f\"{name}straight-{k}-{'auton' if auton else 'nonauton'}\", straightness=0.1, autonomous=auton, residualprop=rp)\n",
    "          else:\n",
    "            weld = WeldNetNonUniform(dataset, w, aeclass, aeargs, propclass, propargs, warmstart=ws, transclass=transclass, transparams=transargs, device=device, tensorboard_directory=f\"{name}kinetic-{k}-{'auton' if auton else 'nonauton'}\", kinetic=1, autonomous=auton, residualprop=rp)\n",
    "\n",
    "          loadae = weld.load_aes(name, verbose=True)\n",
    "          if not loadae:\n",
    "            weld.train_aes(baseepochs, printinterval=25, batch=16, save=True, plottb=False, lr=1e-4)\n",
    "          \n",
    "          assert(False)\n",
    "          # loadboth = False#weld.load_all(name)\n",
    "          \n",
    "          # if not loadboth:\n",
    "          #   loadae = weld.load_aes(name)\n",
    "          #   if not loadae:\n",
    "          #     weld.train_aes(baseepochs, warmstart_epochs=baseepochs, printinterval=25, batch=16, save=True, plottb=False, lr=1e-4)\n",
    "\n",
    "          #   weld.train_propagators(baseepochs * 10, batch=32, printinterval=baseepochs, save=True, lr=1e-10)\n",
    "\n",
    "          # loadweld = False#weld.load_transcoders(name)\n",
    "          # if not loadweld:\n",
    "          #   weld.train_transcoders(baseepochs * 5, printinterval=baseepochs//2, lr=1e-4, save=True)\n",
    "          # #assert(False)\n",
    "\n",
    "          # welds[name].append(weld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3508d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weld.windowvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f4ef3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19912/155463251.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWeldAnalyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mWeldAnalyzer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_encoding_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweld\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Admin\\Documents\\aenet\\src\\notebooks\\..\\models.py\u001b[0m in \u001b[0;36mplot_encoding_window\u001b[1;34m(weld, ws, p, writer, step, tensorboard, maxscatter, testonly, threedim)\u001b[0m\n\u001b[0;32m    828\u001b[0m       \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduced\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 830\u001b[1;33m       \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtestonly\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mweld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindowvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    831\u001b[0m       \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_times_dataparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m       \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from models import WeldAnalyzer\n",
    "\n",
    "WeldAnalyzer.plot_encoding_window(weld)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
