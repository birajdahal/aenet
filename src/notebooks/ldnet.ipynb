{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd297a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "basedir = \"../..\"\n",
    "\n",
    "from common.config import create_object, load_config\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.disable()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# dconfig = load_config(\"../autoencoder/configs/data/burgersshift.yaml\")\n",
    "# dconfig.datasize.spacedim = 1\n",
    "# dset = create_object(dconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import itertools\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from copy import deepcopy\n",
    "\n",
    "import utils\n",
    "\n",
    "class LDNetHelper():\n",
    "  def __init__(self, config):\n",
    "    self.update_config(config)\n",
    "\n",
    "  def update_config(self, config):\n",
    "    self.config = deepcopy(config)\n",
    "\n",
    "  def create_ldnet(self, dataset, k, config=None, **args):\n",
    "    if config is None:\n",
    "      config = self.config\n",
    "\n",
    "    assert(len(dataset.data.shape) < 4)\n",
    "    if len(dataset.data.shape) == 3:\n",
    "      din = dataset.params.shape[-1]\n",
    "      dout = dataset.data.shape[-1]\n",
    "\n",
    "    td = args.get(\"td\", None)\n",
    "    seed = args.get(\"seed\", 0)\n",
    "    device = args.get(\"device\", 0)\n",
    "\n",
    "    dynclass = globals()[args.get(\"dynclass\", config.dynclass)]\n",
    "    dynparams = copy.deepcopy(dict(args.get(\"dynparams\", config.dynparams)))\n",
    "    decclass = globals()[args.get(\"decclass\", config.decclass)]\n",
    "    recparams = copy.deepcopy(dict(args.get(\"recparams\", config.recparams)))\n",
    "\n",
    "    dynparams[\"seq\"][0] = k + din\n",
    "    dynparams[\"seq\"][-1] = k\n",
    "    recparams[\"seq\"][0] = k + din\n",
    "    recparams[\"seq\"][-1] = dout\n",
    "\n",
    "    return LDNet(dataset, k, dynclass, dynparams, decclass, recparams, td=td, seed=seed, device=device)\n",
    "\n",
    "  @staticmethod\n",
    "  def get_operrs(ldnet, times=None, testonly=False):\n",
    "    if testonly:\n",
    "      data = ldnet.dataset.data[ldnet.numtrain:,]\n",
    "    else:\n",
    "      data = ldnet.dataset.data\n",
    "\n",
    "    errors = ldnet.get_errors(data, times=times, aggregate=False)\n",
    "\n",
    "    return errors\n",
    "  \n",
    "  @staticmethod\n",
    "  def plot_op_predicts(ldnet, testonly=False, xs=None, cmap=\"viridis\"):\n",
    "    if testonly:\n",
    "      data = ldnet.dataset.data[ldnet.numtrain:,]\n",
    "      params = ldnet.dataset.params[ldnet.numtrain:,]\n",
    "    else:\n",
    "      data = ldnet.dataset.data\n",
    "      params = ldnet.dataset.params\n",
    "\n",
    "    if xs == None:\n",
    "      xs = np.linspace(0, 1, len(data[0, 0]))\n",
    "\n",
    "    params = torch.tensor(np.float32(params)).to(ldnet.device)\n",
    "\n",
    "    predicts = ldnet.propagate(params).cpu().detach()\n",
    "\n",
    "    errors = []\n",
    "    n = predicts.shape[0]\n",
    "    for s in range(data.shape[1]):\n",
    "      currpredict = predicts[:, s-1].reshape((n, -1))\n",
    "      currreference = data[:, s].reshape((n, -1))\n",
    "      errors.append(np.mean(np.linalg.norm(currpredict - currreference, axis=1) / np.linalg.norm(currreference, axis=1)))\n",
    "        \n",
    "    print(f\"Average Relative L2 Error over all times: {np.mean(errors):.4f}\")\n",
    "\n",
    "    if len(data.shape) == 3:\n",
    "      fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "    @widgets.interact(i=(0, n-1), s=(1, ldnet.T-1))\n",
    "    def plot_interact(i=0, s=1):\n",
    "      print(f\"Avg Relative L2 Error for t0 to t{s}: {errors[s-1]:.4f}\")\n",
    "\n",
    "      if len(data.shape) == 3:\n",
    "        ax.clear()\n",
    "        ax.set_title(f\"RelL2 {np.linalg.norm(predicts[i, s-1] - data[i, s]) / np.linalg.norm(data[i, s])}\")\n",
    "        ax.plot(xs, data[i, 0], label=\"Input\", linewidth=1)\n",
    "        ax.plot(xs, predicts[i, s-1], label=\"Predicted\", linewidth=1)\n",
    "        ax.plot(xs, data[i, s], label=\"Exact\", linewidth=1)\n",
    "        ax.legend()\n",
    "        \n",
    "  @staticmethod\n",
    "  def plot_errorparams(ldnet, param=-1):\n",
    "    if param == -1:\n",
    "        # Auto-detect one varying parameter\n",
    "        param = 0\n",
    "        P = ldnet.dataset.params.shape[1]\n",
    "        for p in range(P):\n",
    "            if np.abs(ldnet.dataset.params[0, p] - ldnet.dataset.params[1, p]) > 0:\n",
    "                param = p\n",
    "                break\n",
    "\n",
    "    l2error = np.asarray(LDNetHelper.get_operrs(ldnet, times=[ldnet.T - 1]))\n",
    "    params = ldnet.dataset.params\n",
    "\n",
    "    print(params.shape, l2error.shape)\n",
    "\n",
    "    if isinstance(param, (list, tuple, np.ndarray)) and len(param) == 2:\n",
    "        # 3D scatter plot for 2 varying parameters\n",
    "        x = params[:, param[0]]\n",
    "        y = params[:, param[1]]\n",
    "        z = l2error\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        sc = ax.scatter(x, y, z, c=z, cmap='viridis', s=10)\n",
    "\n",
    "        ax.set_xlabel(f\"Param {param[0]}\")\n",
    "        ax.set_ylabel(f\"Param {param[1]}\")\n",
    "        ax.set_zlabel(\"Operator Error\")\n",
    "        fig.colorbar(sc, ax=ax, label=\"Operator Error\")\n",
    "\n",
    "    else:\n",
    "        # Fallback to 2D scatter if param is 1D\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(params[:, param], l2error, s=2)\n",
    "        ax.set_xlabel(f\"Parameter {param}\")\n",
    "        ax.set_ylabel(\"Operator Error\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "class LDNet():\n",
    "  def __init__(self, dataset, k, dynclass, dynparams, decclass, recparams, td, seed, device):\n",
    "    self.dataset = dataset\n",
    "    self.device = device\n",
    "    self.td = td\n",
    "    self.k = k\n",
    "    self.f = self.dataset.params.shape[1]\n",
    "  \n",
    "    if self.td is None:\n",
    "      self.prefix = f\"{self.dataset.name}{str(dynclass.__name__)}LDNet-{dynparams['seq'][-1]}\"\n",
    "    else:\n",
    "      self.prefix = self.td\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    self.seed = seed\n",
    "\n",
    "    datacopy = self.dataset.data.copy()\n",
    "    self.numtrain = int(datacopy.shape[0] * 0.8)\n",
    "    \n",
    "    self.T = self.dataset.data.shape[1]\n",
    "    self.trainarr = datacopy[:self.numtrain]\n",
    "    self.testarr = datacopy[self.numtrain:]\n",
    "    self.trainparams = self.dataset.params[:self.numtrain]\n",
    "    self.testparams = self.dataset.params[self.numtrain:]\n",
    "    self.optparams = None\n",
    "\n",
    "    self.datadim = len(self.dataset.data.shape) - 2\n",
    "\n",
    "    self.dynclass = dynclass\n",
    "    self.dynparams = copy.deepcopy(dynparams)\n",
    "    self.decclass = decclass\n",
    "    self.recparams = copy.deepcopy(recparams)\n",
    "\n",
    "    dynparams[\"seq\"][0] = self.k + self.f\n",
    "    dynparams[\"seq\"][-1] = self.k\n",
    "    recparams[\"seq\"][0] = self.k + self.f\n",
    "    recparams[\"seq\"][-1] = self.dataset.data.shape[-1]\n",
    "\n",
    "    self.dynnet = dynclass(**dynparams).float().to(device)\n",
    "    self.recnet = decclass(**recparams).float().to(device)\n",
    "\n",
    "    self.metadata = {\n",
    "      \"dynclass\": dynclass.__name__,\n",
    "      \"dynparams\": dynparams,\n",
    "      \"decclass\": decclass.__name__,\n",
    "      \"recparams\": recparams,\n",
    "      \"dataset_name\": dataset.name,\n",
    "      \"data_shape\": list(dataset.data.shape),\n",
    "      \"data_checksum\": float(np.sum(dataset.data)),\n",
    "      \"seed\": seed,\n",
    "      \"epochs\": []\n",
    "    }\n",
    "\n",
    "  def propagate(self, code, start=0, end=-1, returncodes=False):\n",
    "    if end == -1:\n",
    "      end = self.T - 1\n",
    "\n",
    "    z = code\n",
    "\n",
    "    # get first decode\n",
    "    if z.shape[-1] != self.f + self.k:\n",
    "      if z.shape[-1] == self.f:\n",
    "        z_fixed = z\n",
    "        z_dynamic = torch.zeros(list(z_fixed.shape[:-1]) + [self.k])\n",
    "        z = torch.cat([z_fixed, z_dynamic], dim=-1)\n",
    "      else:\n",
    "        print(z.shape)\n",
    "        assert(False)\n",
    "\n",
    "    zpreds = [z]\n",
    "    for t in range(start, end):\n",
    "      z = self.forward(z)\n",
    "      zpreds.append(z)\n",
    "\n",
    "    zpreds = torch.stack(zpreds, dim=1)\n",
    "    upreds = self.recnet(zpreds)\n",
    "\n",
    "    if returncodes:\n",
    "      return upreds, zpreds\n",
    "    else:\n",
    "      return upreds\n",
    "\n",
    "  def get_errors(self, testarr, testparams, ords=(2,), times=None, aggregate=True):\n",
    "    assert(aggregate or len(ords) == 1)\n",
    "    \n",
    "    if isinstance(testarr, np.ndarray):\n",
    "      testarr = torch.tensor(testarr, dtype=torch.float32)\n",
    "\n",
    "    if isinstance(testparams, np.ndarray):\n",
    "      testparams = torch.tensor(testparams, dtype=torch.float32)\n",
    "\n",
    "    if times is None:\n",
    "      times = range(self.T-1)\n",
    "  \n",
    "    out = self.propagate(testparams)\n",
    "\n",
    "    n = testarr.shape[0]\n",
    "    orig = testarr.cpu().detach().numpy()\n",
    "    out = out.cpu().detach().numpy()\n",
    "\n",
    "    if aggregate:\n",
    "      orig = orig.reshape([n, -1])\n",
    "      out = out.reshape([n, -1])\n",
    "      testerrs = []\n",
    "      for o in ords:\n",
    "        testerrs.append(np.mean(np.linalg.norm(orig - out, axis=1, ord=o) / np.linalg.norm(orig, axis=1, ord=o)))\n",
    "\n",
    "      return tuple(testerrs)\n",
    "    \n",
    "    else:\n",
    "      o = ords[0]\n",
    "      testerrs = []\n",
    "\n",
    "      if len(times) == 1:\n",
    "        t = times[0]\n",
    "        origslice = orig[:, t].reshape([n, -1])\n",
    "        outslice = out[:, t].reshape([n, -1])\n",
    "        return np.linalg.norm(origslice - outslice, axis=1, ord=o) / np.linalg.norm(origslice, axis=1, ord=o)\n",
    "      else:\n",
    "        for t in range(orig.shape[1]):\n",
    "          origslice = orig[:, t].reshape([n, -1])\n",
    "          outslice = out[:, t].reshape([n, -1])\n",
    "          testerrs.append(np.mean(np.linalg.norm(origslice - outslice, axis=1, ord=o) / np.linalg.norm(origslice, axis=1, ord=o)))\n",
    "\n",
    "        return testerrs\n",
    "\n",
    "  def forward(self, z_full, decode=False):\n",
    "    if z_full.shape[-1] == self.f + self.k:\n",
    "      z_fixed = z_full[..., :self.f]\n",
    "      z_dynamic = z_full[..., self.f:]\n",
    "\n",
    "    else:\n",
    "      if z_full.shape[-1] == self.f:\n",
    "        z_fixed = z_full\n",
    "        z_dynamic = torch.zeros(list(z_fixed.shape[:-1]) + [self.k])\n",
    "        z_full = torch.cat([z_fixed, z_dynamic], dim=-1)\n",
    "      else:\n",
    "        print(z_full.shape)\n",
    "        assert(False)\n",
    "\n",
    "    deltaz = self.dynnet(z_full)\n",
    "    z_next_dynamic = z_dynamic + deltaz\n",
    "    z_next_full = torch.cat([z_fixed, z_next_dynamic], dim=-1)\n",
    "\n",
    "    if decode:\n",
    "      decoded = self.recnet(z_next_full)\n",
    "      return decoded, z_next_full\n",
    "    \n",
    "    else:\n",
    "      return z_next_full\n",
    "\n",
    "  def load_model(self, filename_prefix, verbose=False, min_epochs=0):\n",
    "    search_path = f\"savedmodels/ldnet/{filename_prefix}*.pickle\"\n",
    "    matching_files = glob.glob(search_path)\n",
    "\n",
    "    print(\"Searching for model files matching prefix:\", filename_prefix)\n",
    "    if not hasattr(self, \"metadata\"):\n",
    "        raise ValueError(\"Missing self.metadata. Cannot match models without metadata. Ensure model has been initialized with same config.\")\n",
    "\n",
    "    for addr in matching_files:\n",
    "      try:\n",
    "          with open(addr, \"rb\") as handle:\n",
    "              dic = pickle.load(handle)\n",
    "      except Exception as e:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to read error: {e}\")\n",
    "          continue\n",
    "\n",
    "      meta = dic.get(\"metadata\", {})\n",
    "      is_match = all(\n",
    "          meta.get(k) == self.metadata.get(k)\n",
    "          for k in self.metadata.keys()\n",
    "      )\n",
    "\n",
    "      # Check if model meets the minimum epoch requirement\n",
    "      model_epochs = meta.get(\"epochs\")\n",
    "      if model_epochs is None:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to missing epoch metadata.\")\n",
    "          continue\n",
    "      elif isinstance(model_epochs, list):  # handle legacy or list format\n",
    "          if sum(model_epochs) < min_epochs:\n",
    "              if verbose:\n",
    "                  print(f\"Skipping {addr} due to insufficient epochs ({sum(model_epochs)} < {min_epochs})\")\n",
    "              continue\n",
    "      elif model_epochs < min_epochs:\n",
    "          if verbose:\n",
    "              print(f\"Skipping {addr} due to insufficient epochs ({model_epochs} < {min_epochs})\")\n",
    "          continue\n",
    "\n",
    "      if is_match:\n",
    "          print(\"Model match found. Loading from:\", addr)\n",
    "          self.dynnet.load_state_dict(dic[\"dynnet\"])\n",
    "          self.recnet.load_state_dict(dic[\"recnet\"])\n",
    "          self.metadata[\"epochs\"] = meta.get(\"epochs\")\n",
    "          if \"opt\" in dic:     \n",
    "            self.optparams = dic[\"opt\"]\n",
    "\n",
    "          return True\n",
    "      elif verbose:\n",
    "          print(\"Metadata mismatch in file:\", addr)\n",
    "          for k in self.metadata:\n",
    "              print(f\"{k}: saved={meta.get(k)} vs current={self.metadata.get(k)}\")\n",
    "\n",
    "    print(\"Load failed. No matching models found.\")\n",
    "    print(\"Searched:\", matching_files)\n",
    "    return False\n",
    "\n",
    "  def train_model(self, epochs, save=True, optim=torch.optim.AdamW, lr=1e-4, printinterval=10, batch=32, ridge=0, loss=None, best=True, verbose=False):\n",
    "    def train_epoch(dataloader, writer=None, optimizer=None, scheduler=None, ep=0, printinterval=10, loss=None, testarr=None, testparams=None):\n",
    "      losses = []\n",
    "      testerrors1 = []\n",
    "      testerrors2 = []\n",
    "      testerrorsinf = []\n",
    "\n",
    "      def closure(values, params):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = self.propagate(params)\n",
    "        target = values\n",
    "        \n",
    "        res = loss(out, target)\n",
    "        res.backward()\n",
    "        \n",
    "        if writer is not None and self.trainstep % 5 == 0:\n",
    "          writer.add_scalar(\"main/loss\", res, global_step=self.trainstep)\n",
    "\n",
    "        return res\n",
    "\n",
    "      for values, params in dataloader:\n",
    "        self.trainstep += 1\n",
    "        error = optimizer.step(lambda: closure(values, params))\n",
    "        losses.append(float(error.cpu().detach()))\n",
    "\n",
    "      if scheduler is not None:\n",
    "        scheduler.step(np.mean(losses))\n",
    "\n",
    "      # print test\n",
    "      if printinterval > 0 and (ep % printinterval == 0):\n",
    "        testerr1, testerr2, testerrinf = self.get_errors(testarr, testparams, ords=(1, 2, np.inf))\n",
    "        if scheduler is not None:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, LR {scheduler.get_last_lr()[-1]:.3e}, Relative LDNet Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "        else:\n",
    "          print(f\"{ep+1}: Train Loss {error:.3e}, Relative LDNet Error (1, 2, inf): {testerr1:3f}, {testerr2:3f}, {testerrinf:3f}\")\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"misc/relativeL1error\", testerr1, global_step=ep)\n",
    "            writer.add_scalar(\"main/relativeL2error\", testerr2, global_step=ep)\n",
    "            writer.add_scalar(\"misc/relativeLInferror\", testerrinf, global_step=ep)\n",
    "\n",
    "      return losses, testerrors1, testerrors2, testerrorsinf\n",
    "  \n",
    "    loss = nn.MSELoss() if loss is None else loss()\n",
    "\n",
    "    losses, testerrors1, testerrors2, testerrorsinf = [], [], [], []\n",
    "    self.trainstep = 0\n",
    "\n",
    "    train = torch.tensor(self.trainarr, dtype=torch.float32).to(self.device)\n",
    "    params = torch.tensor(self.trainparams, dtype=torch.float32).to(self.device)\n",
    "    test = self.testarr\n",
    "\n",
    "    opt = optim(itertools.chain(self.dynnet.parameters(), self.recnet.parameters()), lr=lr, weight_decay=ridge)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(opt, patience=30)\n",
    "    dataloader = DataLoader(torch.utils.data.TensorDataset(train, params), shuffle=False, batch_size=batch)\n",
    "\n",
    "    if self.optparams is not None:\n",
    "      opt.load_state_dict(self.optparams)\n",
    "\n",
    "    writer = None\n",
    "    if self.td is not None:\n",
    "      name = f\"./tensorboard/{datetime.datetime.now().strftime('%d-%B-%Y')}/{self.td}/{datetime.datetime.now().strftime('%H.%M.%S')}/\"\n",
    "      writer = torch.utils.tensorboard.SummaryWriter(name)\n",
    "      print(\"Tensorboard writer location is \" + name)\n",
    "\n",
    "    print(\"Number of NN trainable parameters\", utils.num_params(self.dynnet), \"+\", utils.num_params(self.recnet))\n",
    "    print(f\"Starting training LDNet model at {time.asctime()}...\")\n",
    "    print(\"train\", train.shape, \"test\", test.shape)\n",
    "      \n",
    "    bestdict = { \"loss\": float(np.inf), \"ep\": 0 }\n",
    "    for ep in range(epochs):\n",
    "      lossesN, testerrors1N, testerrors2N, testerrorsinfN = train_epoch(dataloader, optimizer=opt, scheduler=scheduler, writer=writer, ep=ep, printinterval=printinterval, loss=loss, testarr=test, testparams=self.testparams)\n",
    "      losses += lossesN; testerrors1 += testerrors1N; testerrors2 += testerrors2N; testerrorsinf += testerrorsinfN\n",
    "\n",
    "      if best and ep > epochs // 2:\n",
    "        avgloss = np.mean(lossesN)\n",
    "        if avgloss < bestdict[\"loss\"]:\n",
    "          bestdict[\"dynnet\"] = self.dynnet.state_dict()\n",
    "          bestdict[\"recnet\"] = self.recnet.state_dict()\n",
    "          bestdict[\"opt\"] = opt.state_dict()\n",
    "          bestdict[\"loss\"] = avgloss\n",
    "          bestdict[\"ep\"] = ep\n",
    "        elif verbose:\n",
    "          print(f\"Loss not improved at epoch {ep} (Ratio: {avgloss/bestdict['loss']:.2f}) from {bestdict['ep']} (Loss: {bestdict['loss']:.2e})\")\n",
    "      \n",
    "    print(f\"Finished training LDNet model at {time.asctime()}...\")\n",
    "\n",
    "    if best:\n",
    "      self.dynnet.load_state_dict(bestdict[\"dynnet\"])\n",
    "      self.recnet.load_state_dict(bestdict[\"recnet\"])\n",
    "      opt.load_state_dict(bestdict[\"opt\"])\n",
    "\n",
    "    self.optparams = opt.state_dict()\n",
    "    self.metadata[\"epochs\"].append(epochs)\n",
    "\n",
    "    if save:\n",
    "      now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "      # Compute total training epochs\n",
    "      total_epochs = sum(self.metadata[\"epochs\"]) if isinstance(self.metadata[\"epochs\"], list) else self.metadata[\"epochs\"]\n",
    "\n",
    "      filename = (\n",
    "          f\"{self.dataset.name}_\"\n",
    "          f\"{self.dynclass.__name__}_\"\n",
    "          f\"{self.dynparams['seq']}_\"\n",
    "          f\"{self.decclass.__name__}_\"\n",
    "          f\"{self.recparams['seq']}_\"\n",
    "          f\"{self.seed}_\"\n",
    "          f\"{total_epochs}ep_\"\n",
    "          f\"{now}.pickle\"\n",
    "      )\n",
    "\n",
    "      dire = \"savedmodels/ldnet\"\n",
    "      addr = os.path.join(dire, filename)\n",
    "\n",
    "      if not os.path.exists(dire):\n",
    "          os.makedirs(dire)\n",
    "\n",
    "      with open(addr, \"wb\") as handle:\n",
    "          pickle.dump({\n",
    "              \"dynnet\": self.dynnet.state_dict(),\n",
    "              \"recnet\": self.recnet.state_dict(),\n",
    "              \"metadata\": self.metadata,\n",
    "              \"opt\": self.optparams\n",
    "          }, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "      print(\"Model saved at\", addr)\n",
    "\n",
    "    return { \"losses\": losses, \"testerrors1\": testerrors1, \"testerrors2\": testerrors2, \"testerrorsinf\": testerrorsinf }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd070be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dconfig = load_config(\"../autoencoder/configs/data/burgersshift.yaml\")\n",
    "dconfig.datasize.spacedim = 1\n",
    "dset = create_object(dconfig)\n",
    "\n",
    "#dset.downsample_time(10)\n",
    "dset.downsample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de5b64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "das",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
